<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Transformer综述论文阅读</title>
    <url>/2022/02/26/Transformer%E7%BB%BC%E8%BF%B0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>最近想要研读一下关于transformer变体结构在多模态特征融合时的操作的论文，首先需要广泛地了解一下transformer结构及其变体，于是阅读了复旦大学邱锡鹏教授组里的transformer综述论文，本篇博客详细记录了综述论文中的重点以及本文作者对于这篇论文的理解。 <span id="more"></span></p>
<h3 id="介绍">介绍</h3>
<p>Transformer结构在许多人工智能领域都取得了巨大成功，如自然语言处理、计算机视觉和音频处理领域，到目前，已经提出了大量Transformer结构的变体（又称X-formers），综述论文从三个角度介绍了各种X-former：X-former对于传统Transformer架构的修改、基于Transformer变体的预训练模型以及Transformer变体模型在领域上的应用。</p>
<p>Transformer结构最初是作为一个用于机器翻译的序列到序列模型提出的，后来的工作表明，基于Transformer的预训练模型（PTMs）可以在各种任务上达到最先进的性能。由于Transformer取得的成功，在过去几年内，又有人陆续提出Transformer的变体，主要是在以下几个角度作出改进：</p>
<ol type="1">
<li>模型效率。应用Transformer的一个关键挑战是它在处理长序列时效率低下，这主要是自注意模块中的计算和记忆复杂性导致的，改进方法包括轻量级注意力（如稀疏注意力变体）和采用递归和分层的注意力机制。</li>
<li>模型泛化。由于Transformer是一个灵活的架构，对输入数据的结构偏差几乎不作假设，所以很难在小规模的数据上进行训练，改进方法包括引入结构偏差或正则化，以及在大规模未标记数据上进行预训练。</li>
<li>模型适应性。这项工作的目的是使Transformer适应特定的下游任务和应用。</li>
</ol>
<h3 id="背景">背景</h3>
<h4 id="传统transformer">传统Transformer</h4>
<p>传统的Transformer是一个序列到序列的模型，由一个编码器和一个解码器组成，每个编码器是由<span class="math inline">\(L\)</span>个相同的块堆叠而成。每个编码器块主要是由一个多头注意力模块和一个前馈神经网络组成，为了建立更深层次的模型，在每个块周围采用了残差连接，然后是层归一化操作。与编码器块相比，解码器块在多头自注意力模块和前馈神经网络这件额外插入交叉主义模块。</p>
<blockquote>
<p><strong>需要注意的是，与编码器中的自注意模块不同的是，解码器中的自注意模块被调整为每个位置防止关注后续位置。</strong></p>
</blockquote>
<p>传统Transformer的架构如下图所示，</p>
<center>
<img src="/2022/02/26/Transformer%E7%BB%BC%E8%BF%B0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/001.png" width="500px">
<p style="font-family:KaiTi">
图1：传统Transformer架构图
</p>
</center>
<h5 id="注意力模块">注意力模块</h5>
<p>Transformer的注意力机制采用QKV模型，计算公式如下，</p>
<p><span class="math display">\[
\text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{\top}}{\sqrt{D_{k}}}\right) V=A V
\]</span></p>
<p>在计算过程中需要注意QKV矩阵的维度，<span class="math inline">\(\mathbf{Q} \in \mathbb{R}^{N \times D_{k}}\)</span>，<span class="math inline">\(\mathrm{K} \in \mathbb{R}^{M \times D_{k}}\)</span>，<span class="math inline">\(\mathrm{V} \in \mathbb{R}^{M \times D_{v}}\)</span>，其中<span class="math inline">\(N\)</span>和<span class="math inline">\(M\)</span>代表queries和keys(values)的长度，<span class="math inline">\(D_{k}\)</span>和<span class="math inline">\(D_{v}\)</span>代表keys(queries)和values的维度，</p>
<blockquote>
<p><strong>1. softmax归一化操作按行进行</strong><br> <strong>2. 点积缩放（除以<span class="math inline">\(\sqrt{D_{k}}\)</span>）减轻softmax函数的梯度消失问题</strong></p>
</blockquote>
<p>传统的Transformer模型采用多头注意力机制，将原本的<span class="math inline">\(D_{m}\)</span>维度的queries、keys和values分别映射成<span class="math inline">\(D_{k}\)</span>、<span class="math inline">\(D_{k}\)</span>、<span class="math inline">\(D_{v}\)</span>维，并使用<span class="math inline">\(H\)</span>组学习的投影集。对于每一个投影的query、key和value，根据QKV模型计算公式计算输出，然后，将所有的输出连接起来，并将它们投射到<span class="math inline">\(D_{m}\)</span>的维度表示，多头注意力机制计算公式如下，</p>
<p><span class="math display">\[
\begin{array}{r}
\text { MultiHeadAttn }(\mathbf{Q}, \mathbf{K}, \mathbf{V})=\text { Concat }\left(\text { head }_{1}, \cdots, \text { head }_{H}\right) \mathbf{W}^{O}, \\
\text { where head }_{i}=\text { Attention }\left(\mathbf{Q W}_{i}^{Q}, \mathbf{K W}_{i}^{K}, \mathbf{V W}_{i}^{V}\right) .
\end{array}
\]</span></p>
<p><strong>在Transformer中，有三种注意力机制方式：</strong></p>
<ol type="1">
<li><strong>自注意力机制</strong>。在transformer的编码器中，设置<span class="math inline">\(Q=K=V=X\)</span>，其中<span class="math inline">\(X\)</span>是上一层的输出。<br></li>
<li><strong>掩码自注意力机制</strong>。在transformer解码器中，自注意是受限制的，即每个位置的查询只能注意到该位置之前的所有键值对。为了实现并行训练，通常对非标准化注意力矩阵<span class="math inline">\(\hat{A}=\exp \left(\frac{Q K^{\top}}{\sqrt{D_{k}}}\right)\)</span>应用屏蔽函数，其中非法位置通过设置<span class="math inline">\(\hat{A}_{i j}=-\infty \text { if } i&lt;j\)</span>来进行实现。这种注意方式也被称为自回归注意或因果注意。</li>
<li><strong>交叉注意机制</strong>。交叉注意记住的query是由前一个解码层的输出投影出来的，而key和value是由编码器的输出投影出来的。</li>
</ol>
<h5 id="position-wise前馈神经网络">position-wise前馈神经网络</h5>
<blockquote>
<p><strong>在position-wise前馈神经网络中，参数在不同的位置上是共享的，因此position-wise前馈神经网络也可以理解为两个卷积层，核大小为1。</strong></p>
</blockquote>
<p>position-wise前馈神经网络的计算公式如下所示，</p>
<p><span class="math display">\[
\operatorname{FFN}\left(\mathbf{H}^{\prime}\right)=\operatorname{ReLU}\left(\mathbf{H}^{\prime} \mathbf{W}^{1}+\mathbf{b}^{1}\right) \mathbf{W}^{2}+\mathbf{b}^{2}
\]</span></p>
<p>其中，<span class="math inline">\(\mathbf{H}^{\prime}\)</span>是上一层输出，<span class="math inline">\(\mathbf{W}^{1} \in \mathbb{R}^{D_{m} \times D_{f}}, \mathbf{W}^{2} \in \mathbb{R}^{D_{f} \times D_{m}}, \mathbf{b}^{1} \in \mathbb{R}^{D_{f}}, \mathbf{b}^{2} \in \mathbb{R}^{D_{m}}\)</span>，需要额外注意的是，<strong>一般设置<span class="math inline">\({D_{f}}\)</span>大于<span class="math inline">\({D_{m}}\)</span></strong>。</p>
<h5 id="残差连接和标准化">残差连接和标准化</h5>
<p>为了建立一个深层次模型，transformer在每个模块周围采用了一个残差连接，然后进行层级标准化处理。例如，每一个transformer编码器模块可以表示为如下公式，</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{H}^{\prime} &amp;=\text { LayerNorm }(\text { SelfAttention }(\mathbf{X})+\mathbf{X}) \\
\mathbf{H} &amp;=\text { LayerNorm }\left(\text { FFN }\left(\mathbf{H}^{\prime}\right)+\mathbf{H}^{\prime}\right)
\end{aligned}
\]</span></p>
<h5 id="位置编码">位置编码</h5>
<p>由于transformer没有引入递归或卷积操作，因此对位置信息一无所知（尤其是对于编码器而言），因此需要额外的位置表示来模拟token的排序。</p>
<h4 id="模型用途">模型用途</h4>
<ol type="1">
<li>使用编码器-解码器结构，通常用于序列到序列的建模。</li>
<li>只使用编码器结构，编码器的输出被用作输入序列的表示，通常用于分类或序列标记问题。</li>
<li>只使用解码器结构，其中编码器-解码器交叉注意模块也被移除，通常用于序列生成问题，如语言建模。</li>
</ol>
<h4 id="模型复杂度和参数量分析">模型复杂度和参数量分析</h4>
<p>在假设序列长度为<span class="math inline">\(T\)</span>，维度为<span class="math inline">\(D\)</span>，FFN全连接层维度为<span class="math inline">\(4D\)</span>的情况下，自注意模块和position-wise前馈神经网络模块的复杂度和参数量如下表所示，</p>
<center>
<img src="/2022/02/26/Transformer%E7%BB%BC%E8%BF%B0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/002.png" width="600px">
</center>
<p>对于transformer结构中自注意模块和前馈神经网络复杂度和参数量的推导，可以参考以下博客：</p>
<p><a href="https://0809zheng.github.io/2021/07/12/efficienttransformer.html" target="_blank">传送门1：https://0809zheng.github.io/2021/07/12/efficienttransformer.html</a> <a href="https://zhuanlan.zhihu.com/p/264749298" target="_blank">传送门2：https://zhuanlan.zhihu.com/p/264749298</a></p>
<h4 id="transformer与其他模型的比较">transformer与其他模型的比较</h4>
<h5 id="对自注意操作的分析">对自注意操作的分析</h5>
<ol type="1">
<li>它具有与全连接层相同的最大路径长度，使其适合于长距离的依赖关系建模，与全连接层相比，它的参数效率更高，在处理可变长度的输入时更加灵活。</li>
<li>由于卷积层的感受野有限，人们通常需要堆叠一个深度网络来拥有一个全局感受野，另一方面，恒定的最大路径长度使自注意能够以恒定的层数来模拟长距离的依赖关系。</li>
<li>相比于循环层，自注意的并行度更高，更擅长长距离建模。</li>
</ol>
<p>下表展示了不同类型层的复杂度分析、最小序列操作数和最大路径长度。</p>
<center>
<img src="/2022/02/26/Transformer%E7%BB%BC%E8%BF%B0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/003.png" width="600px">
</center>
<h5 id="关于归纳偏置">关于归纳偏置</h5>
<blockquote>
<p>在机器学习中，很多学习算法经常会对学习的问题做一些关于目标函数的必要假设，称为归纳偏置 (Inductive Bias)</p>
</blockquote>
<p>卷积网络通过共享的局部核函数施加了平移不变性和局部性的归纳偏置，循环神经网络通过其马尔科夫结构带来了时间不变性和位置性的归纳偏置，而transformer架构对数据的结构信息几乎不作假设，这使得transformer成为一个通用和灵活的架构，带来的副作用是<strong>容易对小规模的数据进行过度拟合</strong>。另外，transformer可以看作是一个带有完整有向图上所定义的GNN，其中每个输入都可视为图中的一个节点，然而，transformer和GNN之间的主要区别在于transformer没有引入关于如何构造输入数据的先验知识，transformer中的信息传递过程完全依赖于内容的相似性度量。</p>
<blockquote>
<p>卷积网络的平移不变性体现在卷积核共享权重，局部性体现在卷积核大小是有限的；RNN的时间不变性体现在序列顺序中的每个时间步之间都是有关联的。</p>
</blockquote>
<h3 id="对transformer变体的分类">对transformer变体的分类</h3>
<p>到目前为止，人们已经从三个方面改进传统的transformer模型：架构修改、预训练和应用，其中架构修改又分为模块级别的改进和体系级别的改进，下图给出了transformer变体分类说明，</p>
<center>
<img src="/2022/02/26/Transformer%E7%BB%BC%E8%BF%B0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/004.png" width="600px">
<p style="font-family:KaiTi">
图2：Transformer变体分类
</p>
</center>
<p>详细transformer变体分类如下图所示，</p>
<center>
<img src="/2022/02/26/Transformer%E7%BB%BC%E8%BF%B0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/005.png" width="600px">
<p style="font-family:KaiTi">
图3：详细Transformer变体分类
</p>
</center>
<h3 id="模块级别改进">模块级别改进</h3>
<h4 id="attention机制">attention机制</h4>
<p>self-attention在实际应用中存在两大挑战：</p>
<ol type="1">
<li>复杂性。在处理长序列时，注意力模块称为瓶颈。</li>
<li>结构性先验。自注意对输入的数据不存在任何结构性偏向，甚至顺序信息也需要从训练数据中学习，因此，不含预训练的transformer在小规模或中等规模的数据上通常容易过拟合。</li>
</ol>
<p>attention机制的改进方向：</p>
<ol type="1">
<li>稀疏注意力机制，将稀疏偏置引入到注意力计算。</li>
<li>线性化注意，将注意力矩阵和特征映射分离，降低至线性复杂度。</li>
<li>原型和显存压缩，减少了查询或键值对的数量，从而减小注意力矩阵大小。</li>
<li>低秩自注意，主要抓住自注意力的低秩性。</li>
<li>带有先验的注意力，主要使用先验注意力分布来补充或替代标准注意力。</li>
<li>改进的多头机制。</li>
</ol>
<h5 id="稀疏注意力机制">稀疏注意力机制</h5>
<p>在标准的自注意机制中，每个token都需要注意其他所有token，但事实上，所学习到的注意力矩阵大多是非常稀疏的，因此，可以通过加入结构性偏差来限制每个query所关注的query-key对的数量，从而降低计算的复杂性。计算公式如下所示，</p>
<p><span class="math display">\[
\hat{\mathbf{A}}_{i j}=\left\{\begin{array}{ll}
\mathbf{q}_{i} \mathbf{k}_{j}^{\top} &amp; \text { if token } i \text { attends to token } j \\
-\infty &amp; \text { if token } i \text { does not attend to token } j
\end{array}\right.
\]</span></p>
<p>根据确定稀疏连接的指标，可以将这些方法分为两类，基于位置的稀疏注意和基于内容的稀疏注意。</p>
<h6 id="基于位置信息的稀疏化注意力">基于位置信息的稀疏化注意力</h6>
<p>在基于位置信息的稀疏化注意力中，注意力矩阵是根据一些预先定义的模式来限制的，尽管这些稀疏模式有不同的形式，但有些可以分解为一些原子的稀疏模式，如下图所示，</p>
<center>
<img src="/2022/02/26/Transformer%E7%BB%BC%E8%BF%B0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/006.png" width="650px">
<p style="font-family:KaiTi">
图4：原子稀疏注意力模式
</p>
</center>
<ol type="1">
<li>global attention：增加一些全局节点作为节点间信息传播的枢纽，这些全局节点可以注意序列中的所有节点，整个序列也注意这些全局节点。</li>
<li>band attention：利用数据具有很强局部性的特点，限制了每个query对其邻居节点的关注。</li>
<li>dilated attention：通过扩大扩张空隙来获得更大的感受野。</li>
<li>random attention：为了提高非局部信息互动的能力，对每个query随机抽取几条边。</li>
<li>block local attention：这类注意将输入序列分割成几个不重叠的query块，每个query块都与一个本地记忆块相关，一个query块中的所有query都只关注相应记忆块中的key，图4(e)描述了一种常见的情况，即记忆块与相应的query块是相同的。</li>
</ol>
<p>下图展示一些复合稀疏注意力模式，复合稀疏注意力模式通常是原子稀疏注意力模式组合而成的，</p>
<center>
<img src="/2022/02/26/Transformer%E7%BB%BC%E8%BF%B0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/007.png" width="550px">
<p style="font-family:KaiTi">
图5：复合稀疏注意力模式
</p>
</center>
<p>除了上述模式外，一些现有的研究还探索了针对特定数据类型的扩展稀疏模式。对于文本数据，BP-Transformer构建了一个二叉树，其中所有的token是叶子节点，内部节点是包含许多token的跨度节点，是分层组织的；对于视觉数据，有Image Transformer和Axial Transformer。三种transformer变体的扩展稀疏注意力模式如下图所示，</p>
<center>
<img src="/2022/02/26/Transformer%E7%BB%BC%E8%BF%B0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/008.png" width="550px">
<p style="font-family:KaiTi">
图6：扩展稀疏注意力模式
</p>
</center>
<h6 id="基于内容的稀疏注意力">基于内容的稀疏注意力</h6>
<p>构建基于内容的稀疏图的一个直接方法是选择那些可能与给定query有较大相似性分数的key。Routing Transformer使用K-means聚类算法，将query和key都集中在同一组中心点向量上，每个query只与其处在相同cluster下的key进行交互。Reformer采用LSH哈希方法来为每个query选择key-value，其主要思想是对query和key哈希，分到多个桶内，在同一个桶内的query、key参与交互。SAC将输入序列视为一个图，并学习构建注意力边，以使用自适应稀疏连接改善特定任务的性能，其中边预测器是通过强化学习来训练的。Sparse Sinkhorn Attention首先将query和key通过排序网络控制分为几个块，并为每个query块分配一个key块，每个query只允许关注被分配给其相应块中的key。</p>
<h5 id="线性注意力">线性注意力</h5>
<p>线性化注意力是一类用<span class="math inline">\(\phi(Q) \phi(K)^{\top}\)</span>近似或替换非标准化注意力矩阵<span class="math inline">\(\exp \left(Q K^{\top}\right)\)</span>的方法，其中<span class="math inline">\(\phi\)</span>是按行方式应用的特征图，因此，非归一化注意力矩阵的计算可以通过计算<span class="math inline">\(\phi(Q)\left(\phi(K)^{\top} V\right)\)</span>来线性化，如下图所示，</p>
<center>
<img src="/2022/02/26/Transformer%E7%BB%BC%E8%BF%B0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/009.png" width="600px">
<p style="font-family:KaiTi">
图7：标准注意力和线性注意力对比
</p>
</center>
<p>该模型通过聚合(由特征映射的)key和value的外积表示的关联来维护内存矩阵，然后通过将内存矩阵与具有适当归一化的特征映射查询相乘来检索值，这种方法有两个关键组件，包括特征图和聚合规则。</p>
<h5 id="query原型和显存压缩">query原型和显存压缩</h5>
<p>除了使用稀疏注意和基于内核的线性化注意，人们还可以通过减少query或键值对的数量来降低注意的复杂性。</p>
]]></content>
      <categories>
        <category>深度学习论文阅读</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2020/09/06/hello-world/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<span id="more"></span>
<h2 id="quick-start">Quick Start</h2>
<h3 id="create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
      <tags>
        <tag>hello world</tag>
      </tags>
  </entry>
  <entry>
    <title>test</title>
    <url>/2020/09/06/test/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script>
]]></content>
      <tags>
        <tag>test</tag>
      </tags>
  </entry>
  <entry>
    <title>多模态情感分析综述</title>
    <url>/2022/02/11/%E5%A4%9A%E6%A8%A1%E6%80%81%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>多模态情感分析是一个越来越受欢迎的研究领域，它将传统的基于文本的情感分析任务扩展到多模态的情景下，多模态包括文本、视频和语音模态，以往的情感分析任务通常聚焦于单个模态，但在某些情况下，仅仅通过文本模态来分析说话人的情感是不够的，如以下语句 <span id="more"></span></p>
<blockquote>
<p>The movie is sick.</p>
</blockquote>
<p>这句话本身所要表达的意思是模棱两可的，但如果说话者在说这句话的同时也在微笑，那么它就会被认为是积极的（positive），反过来说，如果说话者在说这句话的同时皱着眉头，则这句话被认为是消极的（negative），以上例子说明了双模态信息之间的互动，其中微笑和皱着眉头等信息能够在视频模态中体现出来。然后我们可以继续加入语音模态从而考虑三模态的情况，当说话声音较大时，这句话的积极情感会进一步增强，但加入使用fair替换掉sick这个单词，虽然同样是大声说话的情景，但考虑到fair这个词的强烈影响，这句话的情感并没有因为大声说话而产生巨大变化。以上例子说明了模态间信息交互的复杂性，但不可置疑的是，视频和语音模态中可能包含文本模态中的互补信息，因此，在多模态场景下进行情感分析任务更具有可信性。</p>
<center>
<img src="/2022/02/11/%E5%A4%9A%E6%A8%A1%E6%80%81%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/001.png" width="400px">
<p style="font-family:KaiTi">
图1：模态之间信息的互动
</p>
</center>
<p>现阶段的多模态情感分析任务，大多站在如何有效地将多模态的特征信息进行融合这一角度考虑问题，目的是排除与情感分析任务无关的噪声数据，最大化利用与情感分析任务相关的多模态数据，包括单模态内的数据交互与模态间的数据交互，最终达到分析情感极性的目标。本文主要就多模态数据集和多模态特征的融合方法进行介绍。</p>
<h3 id="数据集">数据集</h3>
<h3 id="多模态特征融合方法">多模态特征融合方法</h3>
<h4 id="早期融合">早期融合</h4>
<p>早期融合，也被称为特征级别的融合，直接将来自不同模式的特征表示连接成一个单一的表示，这是一种最直观的方法。下图展示了早期融合的具体操作。</p>
<center>
<img src="/2022/02/11/%E5%A4%9A%E6%A8%A1%E6%80%81%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/002.png" width="200px">
<p style="font-family:KaiTi">
图2：早期融合具体操作
</p>
</center>
<p>但是，由于来自不同模态的特征表示可能存在巨大差异，我们必须考虑时间同步问题，即不同模态的特征表示必须在时间层面上进行对齐，以便在融合前将这些特征表示转化为相同的格式，所以在某一个模态或某几个模态有信息缺失的情况下，早期融合的方法不能对模态间的互动进行有效建模。 另外，因为早期融合直接将不同模态的特征表示进行连接，可能会造成融合特征表示较为复杂的情况，最终会导致过拟合的发生。</p>
<h4 id="后期融合">后期融合</h4>
<p>后期融合，也被称为决策级别的融合，通常是整合来自每一个单一模态的预测结果，即首先利用每一个模态的数据信息进行预测，再将每一个模态的预测结果进行整合，流行的整合方法包括平均法、投票法和信号差异法。</p>
<p>后期融合的优点包括：</p>
<ol type="1">
<li>灵活性，可以为每个模态的数据选择最佳的分类器。</li>
<li>鲁棒性，当某些模态的数据有缺失时，后期融合的方法依然可以工作。</li>
</ol>
<p>但是后期融合的方法也有相当大的缺点，即不能够有效学习模态间的信息交互，不同模态数据之间的相关性和差异性被忽略。</p>
<h4 id="基于张量的融合">基于张量的融合</h4>
<p>基于张量的融合方法建立了一个融合层，具体融合方法为在三维向量场中将每种模态的向量表示进行外积操作得到融合向量表示，操作方法如图所示，</p>
<center>
<img src="/2022/02/11/%E5%A4%9A%E6%A8%A1%E6%80%81%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/003.png" width="350px">
<p style="font-family:KaiTi">
图3：基于张量的融合方法
</p>
</center>
<p>需要注意的是，不同模态的特征在进行融合时还需要保留原模态的特征，这样可以更好地利用单模态内的信息，基于张量的融合方法也考虑到这一点，在每一个模态的向量末尾连接1，这样在进行外积的同时还能够保留原有模态的特征，下图以视频模态和文本模态的特征融合为例说明了这一点，</p>
<center>
<img src="/2022/02/11/%E5%A4%9A%E6%A8%A1%E6%80%81%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/004.png" width="300px">
<p style="font-family:KaiTi">
图4：视频模态和文本模态的特征融合
</p>
</center>
<p>可以使用如下公式来表示文本模态、语音模态和视频模态的融合过程，其中<span class="math inline">\(\mathbf{z}^{m}\)</span>表示融合特征向量，<span class="math inline">\(\mathbf{z}^{l}\)</span>表示文本模态特征向量，<span class="math inline">\(\mathbf{z}^{v}\)</span>表示视频模态特征向量，<span class="math inline">\(\mathbf{z}^{a}\)</span>表示语音模态特征向量，可以结合图4来理解以下公式，其中<span class="math inline">\(\mathbf{z}^{l} \otimes \mathbf{z}^{v}\)</span>、<span class="math inline">\(\mathbf{z}^{a} \otimes \mathbf{z}^{v}\)</span>和<span class="math inline">\(\mathbf{z}^{l} \otimes \mathbf{z}^{a}\)</span>表示双模态之间的信息互动，<span class="math inline">\(\mathbf{z}^{l} \otimes \mathbf{z}^{v} \otimes \mathbf{z}^{a}\)</span>表示三模态之间的信息互动。</p>
<p><span class="math display">\[
\mathbf{z}^{m}=\left[\begin{array}{c}
\mathbf{z}^{l} \\
1
\end{array}\right] \otimes\left[\begin{array}{c}
\mathbf{z}^{v} \\
1
\end{array}\right] \otimes\left[\begin{array}{c}
\mathbf{z}^{a} \\
1
\end{array}\right]
\]</span></p>
<p>基于张量的融合方法显式地建模了单模态、双模态、三模态之间的信息互动，并且虽然融合特征向量是高维向量，但由于单模态特征之间是通过外积操作来进行融合的，没有可学习的参数，所以不用考虑过拟合的问题。</p>
<h4 id="低秩多模态融合方法">低秩多模态融合方法</h4>
<p>以上我们已经介绍了基于张量的融合方法，但这种方法往往会因为将单模态向量转化为融合张量而导致维度和计算复杂性的指数级增长（外积操作所致），为了解决基于张量的多模态融合方法计算效率差的问题，提出了一种低秩多模态融合的方法，主要是利用了将张量和权重并行分解的思想。</p>
<p>首先我们需要明确输出<span class="math inline">\(h\)</span>的计算方法，如以下公式所示，</p>
<p><span class="math display">\[
h=g(\mathcal{Z} ; \mathcal{W}, b)=\mathcal{W} \cdot \mathcal{Z}+b, h, b \in \mathbb{R}^{d_{y}}
\]</span></p>
<p>其中<span class="math inline">\(\mathcal{Z}\)</span>为融合张量，低秩多模态融合方法首先将权重<span class="math inline">\(\mathcal{W}\)</span>进行分解为如下形式,</p>
<p><span class="math display">\[
\mathcal{W}=\sum_{i=1}^{r} \bigotimes_{m=1}^{M} \mathbf{w}_{m}^{(i)}
\]</span></p>
<p>可以首先尝试理解一下作者这样分解的目的，即为了利用类似于分配律的原理让外积操作变为对应元素的乘积，最终达到每种模态数据乘以对应权重，然后再进行element wise（对应元素乘积）的操作的目标，以下我们可以尝试证明一下，</p>
<p><span class="math display">\[
\begin{aligned}
h &amp;=\left(\sum_{i=1}^{r} \bigotimes_{m=1}^{M} \mathbf{w}_{m}^{(i)}\right) \cdot \mathcal{Z} \\
&amp;=\sum_{i=1}^{r}\left(\bigotimes_{m=1}^{M} \mathbf{w}_{m}^{(i)} \cdot \mathcal{Z}\right) \\
&amp;=\sum_{i=1}^{r}\left(\bigotimes_{m=1}^{M} \mathbf{w}_{m}^{(i)} \cdot \bigotimes_{m=1}^{M} z_{m}\right) \\
&amp;=\Lambda_{m=1}^{M}\left[\sum_{i=1}^{r} \mathbf{w}_{m}^{(i)} \cdot z_{m}\right]
\end{aligned}
\]</span></p>
<p>其中公式中的<span class="math inline">\(\Lambda\)</span>符号表示对应元素乘积的操作，特别地，有以下记法：<span class="math inline">\(\Lambda_{t=1}^{3} x_{t}=x_{1} \circ x_{2} \circ x_{3}\)</span>，最终可以使用下图来表示低秩多模态融合方法，</p>
<center>
<img src="/2022/02/11/%E5%A4%9A%E6%A8%A1%E6%80%81%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/005.png" width="550px">
<p style="font-family:KaiTi">
图5：低秩多模态融合方法
</p>
</center>
<h4 id="基于神经网络的融合方法">基于神经网络的融合方法</h4>
<p>基于神经网络的融合采用了一种直接和直观的策略，通过神经网络融合不同模态的特征表示。基于注意力的融合使用注意力机制来获得一组具有标量权重的特征表示的加权和，这些标量权重是由一个注意力模块动态学习的。还有一些融合方法参考了当前比较流行的Transformer和BERT结构的思想，这些使用深度模型的融合方法能够以端到端的方式从大量的数据中学习，具有良好的性能，但也存在可解释性低的问题。</p>
<h5 id="基于注意力机制的融合方法">基于注意力机制的融合方法</h5>
<p>注意力机制被用于多模态融合是很容易想到的操作，注意力机制天然地能够发掘源模态和目标模态的联系，从而生成注意力权重，通过乘积进一步加强目标模态的特征表示，注意力机制很适用于发掘模态之间的共同特征，但对于发掘模态的特有特征就显得有些力不从心。这里我们以记忆融合网络（MFN）为例来探索注意力机制在多模态融合方面的应用。</p>
<p>注意力机制通常不会单独应用在多模态特征融合中，可能还会结合其他结构，如MFN由三个主要部分组成，分别是LSTM系统、Delta-Memory注意力网络、多模态门控记忆，其中LSTM系统有多个长短期记忆网络（LSTM）组成，每个模态对应一个LSTM网络，LSTM对模态内的信息的交互进行建模，或者说对每个时间步的模态信息进行记忆；Delta-Memory注意力网络是一种特殊的注意力机制，旨在发现LSTM系统中不同模态记忆间的横向交互关系；多模态门控记忆结构是一种统一的记忆存储，存储随着时间步推移的跨模态信息交互，总结下来，LSTM结构致力于模态内信息的建模，Delta-Memory注意力网络和多模态门控记忆结构致力于模态间信息的建模，模型总体结构如下所示，</p>
<center>
<img src="/2022/02/11/%E5%A4%9A%E6%A8%A1%E6%80%81%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/008.png" width="500px">
<p style="font-family:KaiTi">
图6：记忆融合网络（MFN）的整体架构
</p>
</center>
<p>Delta-Memory注意力网络截取<span class="math inline">\(t-1\)</span>和<span class="math inline">\(t\)</span>两个时间步的LSTM系统输出信息，目的是为了防止重复的跨模态交互信息被赋高权重的情况，如果有了两个时间步的模态信息之后，就可以将两个时间步的信息进行比较从而将高系数分配给那些将要变化的模态信息，文中指出，理想情况下，在LSTM系统的记忆状态发生变化之前，每个跨模态交互只分配一次高系数。Delta-Memory注意力网络中的计算公式如下所示，首先计算出注意力权重系数，再将系数乘以原模态特征表示。</p>
<p><span class="math display">\[
\begin{gather*}
a^{[t-1, t]}=\mathcal{D}_{a}\left(c^{[t-1, t]}\right) \\
\hat{c}^{[t-1, t]}=c^{[t-1, t]} \odot a^{[t-1, t]}
\end{gather*}
\]</span></p>
<p>多模态门控记忆结构存储了一段时间内跨模态信息交互的历史，包括两个门，分别是保持门和更新门，其中保持门分配上一个时间步的记忆，更新门根据更新建议（或称为当前时间步状态）来进行更新，计算公式如下所示，</p>
<p><span class="math display">\[
\begin{gather*}
\hat{u}^{t}=\mathcal{D}_{u}\left(\hat{c}^{[t-1, t]}\right) \\
\gamma_{1}^{t}=\mathcal{D}_{\gamma_{1}}\left(\hat{c}^{[t-1, t]}\right), \gamma_{2}^{t}=\mathcal{D}_{\gamma_{2}}\left(\hat{c}^{[t-1, t]}\right) \\
u^{t}=\gamma_{1}^{t} \odot u^{t-1}+\gamma_{2}^{t} \odot \tanh \left(\hat{u}^{t}\right)
\end{gather*}
\]</span></p>
<h5 id="多模态transformer融合方法">多模态Transformer融合方法</h5>
<p>这里介绍的多模态Transformer融合方法出自<em>Multimodal Transformer for Unaligned Multimodal Language Sequences</em>一文，文章提出了MulT结构，主要应用于非对齐多模态语言序列场景。相关研究表明，在对多模态的语言时间序列数据进行建模时主要存在两个挑战：</p>
<ol type="1">
<li>由于每种模态序列的采样率不同，导致数据没有在时间层面上对齐。</li>
<li>不同模态的元素之间存在长距离依赖的现象。</li>
</ol>
<p>文章中提出的MulT结构以端到端的方法解决了上述问题。MulT结构以跨模态的Transformer作为主要结构来建模模态与模态（双模态）之间的关系，具体来说，每个跨模态Transformer通过学习两个模态之间的注意力，用另一个源模态的低级特征反复强化目标模态，以视频模态和文本模态为例，第一种情况下，以视频模态为源模态，以文本模态为目标模态，在每一个跨模态的注意力机制块中，将源模态特征向量作为注意力机制的<span class="math inline">\(Q\)</span>，将目标模态特征向量作为注意力机制的<span class="math inline">\(K\)</span>、<span class="math inline">\(V\)</span>，经过注意力机制计算得到注意力权重，将权重乘以目标模态特征向量得到强化后的目标模态特征向量，计算公式如下所示，</p>
<p><span class="math display">\[
\text { Attention }(\mathrm{Q}, \mathrm{K}, \mathrm{V})=\operatorname{softmax}\left(\frac{\mathrm{QK}^{\mathrm{T}}}{\sqrt{\mathrm{d}_{\mathrm{k}}}}\right) \mathrm{V}
\]</span></p>
<p>实际情况下采用的多为多头注意力机制；第二种情况下，以文本模态为源模态，以视频模态为目标模态，其他计算过程相同。跨模态的注意力机制块的堆叠组成了跨模态的Transformer结构，另外，这里只着重强调了跨模态的注意力机制块中的跨模态的注意力机制计算细节，跨模态的注意力机制块除了交叉注意力结构，还包括前馈神经网络等结构。跨模态的注意力机制块结构如下所示，</p>
<center>
<img src="/2022/02/11/%E5%A4%9A%E6%A8%A1%E6%80%81%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/006.png" width="300px">
<p style="font-family:KaiTi">
图7：跨模态注意力机制块结构
</p>
</center>
<p>MulT的整体架构如下图所示，在这里我们只关注模态信息融合的部分，即架构的中间部分，可以从图中看到，在完成特征向量的处理之后，将两个模态的特征向量分别作为源模态和目标模态输入到跨模态的Transformer结构中，两个Transformer的输出进行连接，再输入到Transformer结构中，不同的是，跨模态的Transformer中进行的主要是交叉注意力计算，而上部的Transformer中进行的主要是自注意计算。架构的底部和顶部在这里不作主要关注，只需要明确架构的底部为特征向量的处理，架构的顶部为下游任务的适应。</p>
<center>
<img src="/2022/02/11/%E5%A4%9A%E6%A8%A1%E6%80%81%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/007.png" width="350px">
<p style="font-family:KaiTi">
图8：MulT的整体架构
</p>
</center>
<h5 id="基于神经网络的融合方法在可解释性方面的改进">基于神经网络的融合方法在可解释性方面的改进</h5>
<p>一般的基于神经网络的融合方法在可解释性方面都表现较差，所以出现了一些提高可解释性的尝试，现有大多数的尝试是在挖掘共同和独特信息方面的改进。一些框架可以从多模态特征表示中提取公共信息，同时获取一些独特信息，这里的独特信息通常是指特定于模态的信息，最后再通过融合层将共同信息和独特信息进行整合。</p>
<p>将DeepCU结构作为示例进行说明，DeepCU包括两个子网络，包括可以获取模态特性的独特子网络，以及由深度卷积张量网络组成的共同子网络。DeepCU可以通过两个子网络获取到互补的信息，进而通过融合层进行信息整合。DeepCU框架示意图如下所示，</p>
<center>
<img src="/2022/02/11/%E5%A4%9A%E6%A8%A1%E6%80%81%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/009.png" width="500px">
<p style="font-family:KaiTi">
图9：DeepCU的整体架构
</p>
</center>
<p>框架示意图左边是TFN和LMF架构的示意图，DeepCU相较于TFN和LMF更具表现力，因为它在共同子网络中捕获了非线性的多模态交互关系，在独特子网络中不会了分解的非线性特征关系，缓解了缺失值场景，增强了模型的泛化能力。</p>
<p>首先介绍独特子网络获得特定于模态信息的方法，从三个模态提取潜在特征之后，为了能对单模态中信息与信息的交互进行建模，这里采取的是分解机的方法，采用分解机的另一个好处是可以缓解缺失值场景，分解机公式如下所示，</p>
<p><span class="math display">\[
\hat{y}_{F M(\boldsymbol{x})}=w_{0}+\sum_{i=1}^{n} \boldsymbol{w}_{i} \boldsymbol{x}_{i}+\sum_{i=1}^{n} \sum_{j=i+1}^{n} \boldsymbol{v}_{i}^{T} \boldsymbol{v}_{j} \cdot \boldsymbol{x}_{i} \boldsymbol{x}_{j}
\]</span></p>
<p>然后我们对共同子网络进行介绍。共同子网络首先将三个模态的潜在特征进行外积，得到联合表示张量，类似于TFN的做法，其中张量的每个元素代表融合模态元素之间的相互作用强度，然后我们在这些张量上运用卷积操作，因为卷积核是非线性的特征提取器，所以比前馈层的泛化效果更好，并且能够有效降低复杂度，最后再将卷积操作输出输入到全连接层中。融合层的操作较为简单，只采取了简单的连接和转置操作。</p>
<p>上文提到过尝试挖掘共同和独特信息方面的改进，介绍了在挖掘模态共同和独特信息方面的方法，但是挖掘独特信息并不一定指模态独特信息，Deep-HOSeq架构的独特子网络挖掘的是时间步上的独特信息，旨在发现时间与语义观点上的协作，以下是Deep-HOSeq整体架构示意图，</p>
<center>
<img src="/2022/02/11/%E5%A4%9A%E6%A8%A1%E6%80%81%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/010.png" width="500px">
<p style="font-family:KaiTi">
图10：Deep-HOSeq的整体架构
</p>
</center>
<p>Deep-HOSeq处理独特子网络的方法是，将每个时间步的各个模态的向量进行外积操作，再进行卷积操作，得到每个时间步的独特信息，经过sum-pooling进行相加，得到独特向量。Deep-HOSeq处理共同子网络和DeepCU处理共同子网络的方法类似。</p>
]]></content>
      <categories>
        <category>深度学习论文阅读</category>
      </categories>
      <tags>
        <tag>多模态情感分析</tag>
      </tags>
  </entry>
</search>
