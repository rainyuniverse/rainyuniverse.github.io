<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>BINDING LANGUAGE MODELS IN SYMBOLIC LANGUAGES论文阅读</title>
    <url>/2022/10/17/BINDING-LANGUAGE-MODELS-IN-SYMBOLIC-LANGUAGES%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>终于保研结束啦！！928之后先划水了一周，然后老师就布置任务了，于是就对这一篇免训练的神经符号框架论文进行阅读，论文的一个背景是越来越多的端到端的模型在QA以及其他自然语言处理领域的任务上取得了良好的效果，但是它们具有缺乏可解释性的缺点，本文将符号方法绑定到语言模型中，增强了模型的可解释性，另外现有的一些模型在特定领域需要进行模型微调，本文提出的框架具有免训练的特点，只需要一些精心挑选的prompt和API调用就可以达到SOTA效果。</p>
<center>
<img src="/2022/10/17/BINDING-LANGUAGE-MODELS-IN-SYMBOLIC-LANGUAGES%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/1.jpg" width="600px">
</center>
<span id="more"></span>
<h3 id="摘要">摘要</h3>
<ol type="1">
<li>端到端的神经方法最近在性能和易用性方面在NLP任务中占主导地位，但它们缺乏可解释性和稳健性</li>
<li>本文提出<code>BINDER</code>，一个<strong>免训练</strong>的神经符号框架，它将任务输入映射到程序中
<ul>
<li>采用<code>GPT-3 Codex</code>作为语言模型</li>
<li>在<strong>解析阶段</strong>，只需几个上下文示例，<code>Codex</code>就能识别出任务输入中无法由原始编程语言回答的部分，正确地生成API调用以促使<code>Codex</code>解决无法回答的部分，并在于原始语法兼容的情况下确定API调用的位置</li>
<li>在<strong>执行阶段</strong>，鉴于API调用中的适当提示，<code>Codex</code>可以执行多种功能（如常识性的QA、信息提取）</li>
</ul></li>
</ol>
<h3 id="介绍">介绍</h3>
<ol type="1">
<li>自然语言处理任务的性能由直接将输入映射到输出的神经端到端系统主导
<ul>
<li>优点：灵活且易于使用</li>
<li>缺点：缺乏可解释性和稳健性</li>
</ul></li>
<li>端到端的方法与产生明确的中间表征（如逻辑形式、推理路径或程序代码）的符号方法形成对比
<ul>
<li>优点：这些符号方法产生的中间形式是可解释的，由此产生的执行结果使它们对输入的变化更加稳健</li>
<li>缺点：语义覆盖面受到所选符号语言的语法承受力的限制</li>
</ul></li>
<li>一些工作已经提出将神经模块和符号语言结合起来，以利用两种方法的优势
<ul>
<li>缺点：在不同的任务和领域中，需要定制不同的符号语言和神经模块，并且需要各种大量的训练数据，以确保所有模块都得到良好的训练</li>
</ul></li>
<li>提出<code>BINDER</code>，一个免训练的神经符号框架，它将任务输入映射到编程语言中的可执行程序，并绑定了一个统一的API来调用语言模型来执行多功能
<ul>
<li>首先，将输入问题通过<code>Codex</code>解析为<code>BINDER</code>程序，其中<code>Codex</code>需要决定以下三点：
<ul>
<li>输入中的哪些部分可以转换为目标编程语言</li>
<li>相应的任务API调用（可以理解为<code>prompt</code>）</li>
<li>在<code>BINDER</code>程序中插入API调用的位置</li>
</ul></li>
<li>接下来，<code>Codex</code>生成任务API调用的答案，将生成的结果整合回编程语言，并执行生成的编程语言表达式以得到最终答案</li>
</ul></li>
</ol>
<h3 id="方法">方法</h3>
<h4 id="binder框架"><code>BINDER</code>框架</h4>
<ol type="1">
<li><p>概述：解决NLP任务的<code>BINDER</code>框架定义如下：给定自然语言输入<span class="math inline">\(Q\)</span>和可选上下文<span class="math inline">\(D\)</span>（如段落、表格、图像或上述组合）作为输入，生成可执行的<code>BINDER</code>程序<span class="math inline">\(Z\)</span>，最后，输出答案<span class="math inline">\(A\)</span>是通过使用<code>BINDER</code>解释器执行<span class="math inline">\(Z\)</span>得到的</p></li>
<li><p><code>BINDER</code>解析：在解析阶段，输入的自然语言<span class="math inline">\(Q\)</span>被解析为<code>BINDER</code>程序<span class="math inline">\(Z\)</span>，<code>BINDER</code>程序是一种符号语言的表达，在核心符号语言无法提供所需功能的情况下，可以选择API调用</p>
<ul>
<li><p>将程序中的API调用定义为方法<span class="math inline">\(f(\hat{Q};\hat{D})\)</span>，其中<span class="math inline">\(\hat{Q}\)</span>是等待回答的问题，可以查询的上下文为<span class="math inline">\(\hat{D}\)</span>，这里<span class="math inline">\(\hat{Q}\)</span>是<span class="math inline">\(Q\)</span>中不能仅仅运用程序语言回答的部分，<span class="math inline">\(\hat{D}\)</span>是<span class="math inline">\({D}\)</span>中和<span class="math inline">\(\hat{Q}\)</span>有关的上下文</p>
<blockquote>
<p>这里文中举出了一个例子：</p>
<p><code>Which is the best-selling shirt made in North America and with no chemicals?</code></p>
<p>其中的<code>North America</code>是<span class="math inline">\(\hat{Q}\)</span>,<code>Made_in</code>列是对应的上下文</p>
</blockquote>
<p><span class="math inline">\(f(\hat{Q};\hat{D})\)</span>API调用的输出是<span class="math inline">\(\hat{Q}\)</span>查询的答案，它被表示为与符号语言语法兼容的变量，以便程序可以被执行</p></li>
</ul></li>
<li><p><code>BINDER</code>执行：在执行阶段，程序<span class="math inline">\(Z\)</span>由<code>BINDER</code>解释器执行得到答案a，<code>BINDER</code>解释器由一个<strong>标准符号语言解释器</strong>和实现API调用的模型组成，执行阶段包括词法分析、语法分析和程序评估，</p>
<ul>
<li>在<strong>词法和语法分析</strong>中，<span class="math inline">\(f(\hat{Q};\hat{D})\)</span>被添加为语法中的一个新标识符，程序被解析为基于该扩展语法的抽象语法树</li>
<li>在<strong>程序评估</strong>中，API调用是通过调用底层神经模型来评估的，API调用输出被保存为与标准符号语言兼容的变量，因此程序最终可以由一个现成的符号语言解释器执行，从而获得输出</li>
</ul></li>
</ol>
<h4 id="binder的上下文学习"><code>BINDER</code>的上下文学习</h4>
<ol type="1">
<li>在解析阶段，由于插入了API调用，<code>BINDER</code>程序的语法与原始编程语言的语法不同，这里利用<code>Codex</code>的少样本泛化能力，发现它只需要少量的上下文示例就可以有效地学习修改后的语法</li>
<li>在执行阶段，<code>Codex</code>作为底层的LM，通过连接API调用的输入，<span class="math inline">\(\hat{Q}\)</span>和<span class="math inline">\(\hat{D}\)</span>作为语言模型的提示，给API调用提供输出，而程序的其余部分则由编程语言解释器执行</li>
<li><code>BINDER</code>的上下文学习遵循下面这个过程：
<ul>
<li>输入由<span class="math inline">\(k\)</span>个上下文示例和推理问题组成</li>
<li>输出是<span class="math inline">\(n\)</span>个候选的<code>BINDER</code>程序，由<code>BINDER</code>解释器运行得到<span class="math inline">\(n\)</span>个答案，根据多数投票策略选出正确答案</li>
</ul></li>
</ol>
<h4 id="binder实现"><code>BINDER</code>实现</h4>
<ol type="1">
<li>实现了两个API
<ul>
<li><span class="math inline">\(f_{col}(\hat{Q};\hat{D})\)</span>：将已知的列（上下文）映射到一个新的列</li>
<li><span class="math inline">\(f_{val}(\hat{Q};\hat{D})\)</span>：将已知的列（上下文）映射到一个新的值</li>
</ul></li>
</ol>
<h3 id="实验">实验</h3>
<h4 id="实验设置">实验设置</h4>
<ol type="1">
<li><p><strong>数据集</strong></p>
<ul>
<li><code>WIKITQ</code></li>
<li><code>TABFACT</code></li>
</ul></li>
<li><p><strong>评价指标</strong>：评价指标是<code>WIKITQ</code>和<code>TABFACT</code>的执行精度（EA）。在实际情况中，程序的执行结果往往在语义上是正确的，但是不能匹配黄金答案，因此，<strong>文章在<code>WIKITQ</code>中为这些语义上正确的情况添加了一个预匹配检查</strong>，主要包括以下两种情况</p>
<ul>
<li>A或B的选择问题：1代表yes，0代表no</li>
<li>带单位的数字问题：去掉单位也是匹配的</li>
</ul></li>
<li><p>基线模型：除了一些传统的强有力的模型，为了进一步证明<code>BINDER</code>的有效性，还使用附加的推理模型来评估<code>Codex</code>，包括：</p>
<ul>
<li><code>Codex end-to-end QA</code>：端到端QA，即根据输入问题和表直接输出答案</li>
<li><code>Codex SQL</code>：使用标准SQL语言进行语义解析（同样的，先调用Codex生成中间SQL，再调用Codex生成答案）</li>
</ul></li>
<li><p>实现细节</p>
<ul>
<li><p>使用OpenAI中提供的Codex模型接口，作为生成程序的解析器和每个程序执行过程中API调用的基础模型</p></li>
<li><p>为了生成正确的<code>BINDER</code>程序，为每个数据集注释了14个上下文示例，每个上下文示例提示主要遵循表结构、前三行表、问题和对应的<code>BINDER</code>程序的格式，如下所示</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> Fabrice Santoro(</span><br><span class="line">	row_id <span class="type">int</span>,</span><br><span class="line">	name text,</span><br><span class="line">	<span class="number">2001</span> text,</span><br><span class="line">	<span class="number">2002</span> text,</span><br><span class="line">	<span class="number">2003</span> text,</span><br><span class="line">	<span class="number">2004</span> text,</span><br><span class="line">	<span class="number">2005</span> text,</span><br><span class="line">	<span class="number">2006</span> text,</span><br><span class="line">	<span class="number">2007</span> text,</span><br><span class="line">	<span class="number">2008</span> text,</span><br><span class="line">	<span class="number">2009</span> text,</span><br><span class="line">	<span class="number">2010</span> text,</span><br><span class="line">	career\nsr text,</span><br><span class="line">	career\nwin<span class="operator">-</span>loss text)</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">3 example rows:</span></span><br><span class="line"><span class="comment">SELECT * FROM w LIMIT 3;</span></span><br><span class="line"><span class="comment">row_id	name	2001	2002	2003	2004	2005	2006	2007	2008	2009	2010	career\nsr	career\nwin-loss</span></span><br><span class="line"><span class="comment">0	australian open	2r	1r	3r	2r	1r	qf	3r	2r	3r	1r	0 / 18	22–18</span></span><br><span class="line"><span class="comment">1	french open	4r	2r	2r	3r	1r	1r	1r	2r	1r	a	0 / 20	17–20</span></span><br><span class="line"><span class="comment">2	wimbledon	3r	2r	2r	2r	2r	2r	2r	1r	2r	a	0 / 14	11–14</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">Q: did he win more <span class="keyword">at</span> the australian <span class="keyword">open</span> <span class="keyword">or</span> indian wells?</span><br><span class="line">NeuralSQL: <span class="keyword">SELECT</span> name <span class="keyword">FROM</span> w <span class="keyword">WHERE</span> name <span class="keyword">IN</span> (<span class="string">&#x27;australian open&#x27;</span>, <span class="string">&#x27;indian wells&#x27;</span>) <span class="keyword">ORDER</span> <span class="keyword">BY</span> QA(&quot;map@how many wins?&quot;; `career\nwin<span class="operator">-</span>loss`) <span class="keyword">DESC</span> LIMIT <span class="number">1</span></span><br></pre></td></tr></table></figure></li>
</ul></li>
</ol>
<h4 id="主要结果">主要结果</h4>
<ol type="1">
<li><code>WIKITQ</code>
<ul>
<li>SOTA</li>
<li><code>Codex BINDER</code>比使用标准SQL在测试集上的执行精度提高，说明<code>BINDER</code>确实缓解了原始语言的覆盖限制</li>
<li><code>Codex BINDER</code>和<code>Codex SQL</code>都比端到端QA具有显著优势，说明在结构化知识基础和与代码相关的任务中，使用<code>Codex</code>进行语义解析（先生成代码）是更好的默认选择</li>
</ul></li>
<li><code>TABFACT</code>
<ul>
<li>SOTA</li>
<li>需要注意的是，符号方法在二元分类问题上通常落后于端到端的方法，因为答案空间很小，但是文章的方法取得了最先进的结果</li>
<li>然而，<strong>用于上下文学习的有效样本选择仍然是一个挑战</strong></li>
</ul></li>
</ol>
<h3 id="分析">分析</h3>
<h4 id="消融分析">消融分析</h4>
<ol type="1">
<li>将问题分为两种，一种是程序（SQL）可以解决的问题，一种是程序无法解决的问题，<code>Codex BINDER</code>在程序无法解决的问题上明显优于<code>Codex SQL</code>，说明其语义覆盖范围更广</li>
<li><code>BINDER</code>的假象率更低（没有意外得出正确答案）</li>
<li><code>BINDER</code>在程序可解性方面也取得了比SQL更高的分数</li>
</ol>
<h4 id="可解释性">可解释性</h4>
<ol type="1">
<li>显式程序可以帮助人工调试和错误分析，增加了可解释性</li>
</ol>
<h4 id="鲁棒性">鲁棒性</h4>
<h5 id="可伸缩性">可伸缩性</h5>
<ol type="1">
<li>端到端QA在预测答案时由于需要整个表作为输入，所以当知识源很大时不能很好地适应，而<code>BINDER</code>的输入只有三条表行，所以当知识源很大时能更好地适应</li>
</ol>
<h5 id="噪声内容">噪声内容</h5>
<ol type="1">
<li>端到端的方法对噪声输入比较脆弱，特别是当存在与问题相关内容相似的干扰物时，文章构建了一个具有干扰物的数据集并进行实验，发现<code>BINDER</code>比端到端QA更稳定</li>
</ol>
<h5 id="使用python的binder">使用<code>Python</code>的<code>BINDER</code></h5>
<ol type="1">
<li>文章设计的<code>BINDER</code>很容易扩展到各种编程语言</li>
</ol>
<h5 id="多模态应用">多模态应用</h5>
<ol type="1">
<li>文章在多模态数据集上应用<code>BINDER</code>，跨越文本、表格和图像，为了将图像输入到LM程序生成中，事先用视觉—文本预训练模型OFA将图像转换成文本图像说明，文章的工作是第一个证明跨文本、表格和图像的多模态QA可以用可解释和可执行的程序来处理，并且取得了和最先进的模型相媲美的性能</li>
</ol>
<h3 id="相关工作">相关工作</h3>
<ol type="1">
<li><p>语义解析：<strong>语义解析能够在给定自然语言输入的情况下产生可执行的程序</strong>，生成协助解决问题的中间结构，并比后来直接生成解决方案的神经方法提高可解释性，然而，语义解析方法受限于其语法覆盖范围，无法解决需要外部知识或功能的问题</p></li>
<li><p>神经—符号方法：一些工作将神经模块与符号语言整合起来，以获得两种方法的优势，即良好的性能和可解释性，然而，这些方法往往需要精心设计用于相应任务的函数和校准相应的神经模块，这需要复杂的训练步骤和大量的训练数据来解决特定领域的问题</p></li>
<li><p>与这些方法相比，BINDER具有很强的表现力和灵活性，能够处理现实世界中的各种问题，因为它能够通过适当的API调用来增强其功能。此外，BINDER不需要训练，只需要几十个基于特定符号语言的注释就可以完成一个特定领域的任务，同时保持：出色的性能、对输入的扩展能力、可解释性和对嘈杂内容的稳健性</p></li>
</ol>
<h3 id="总结">总结</h3>
<ol type="1">
<li>文章提出了<code>BINDER</code>，一个免训练的神经符号框架，它将任务输入映射到一个程序中，允许绑定一个统一的LM API来实现额外的功能</li>
<li><code>BINDER</code>旨在结合端到端方法（高覆盖率）和符号方法（高可解释性）的优势</li>
<li><code>BINDER</code>使用Codex作为LM，在选定的数据集上达到了最先进的性能</li>
<li>文章还对<code>BINDER</code>进行了一系列的分析，分解了性能提升，检查了对大的或嘈杂的输入的鲁棒性，将其应用于多模态知识源，并将其扩展到Python语言</li>
</ol>
]]></content>
      <categories>
        <category>深度学习论文阅读</category>
      </categories>
      <tags>
        <tag>自然语言处理</tag>
      </tags>
  </entry>
  <entry>
    <title>BLEU评估指标</title>
    <url>/2022/12/28/BLEU%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>BLEU评估指标在机器翻译任务中经常使用，本文主要对BLEU评估指标的计算过程以及计算工具的使用进行总结。 <span id="more"></span></p>
<h2 id="定义">定义</h2>
<ol type="1">
<li>BLEU（全称为Bilingual Evaluation Understudy），其意思为双语评估替补，用于机器翻译任务的评价，原文如下<a href="https://aclanthology.org/P02-1040.pdf">BLEU: a Method for Automatic Evaluation of Machine Translation</a></li>
<li>BLEU算法实际上就是在判断两个句子的相似程度</li>
<li>BLEU有许多变种，根据<code>n-gram</code>可以划分成多种评价指标，常见的评价指标有BLEU-1、BLEU-2、BLEU-3、BLEU-4四种，其中<code>n-gram</code>指的是连续的单词个数为n，BLEU-1衡量的是单词级别的准确性，更高阶的BLEU可以衡量句子的流畅性</li>
</ol>
<h2 id="计算">计算</h2>
<ol type="1">
<li><p>BLEU计算的一个大致步骤是：</p>
<ul>
<li><p>分别计算<code>candidate</code>句和<code>reference</code>句的<code>N-grams</code>模型，然后统计其匹配的个数，计算匹配度 <span class="math display">\[
candidate和reference中匹配的n-gram的个数/candidate中n-gram的个数
\]</span></p>
<blockquote>
<p>举例说明：</p>
<p>candidate: It is a nice day today</p>
<p>reference: Today is a nice day</p>
</blockquote>
<ul>
<li><p>使用<code>1-gram</code>进行匹配</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">candidate: &#123;it, is, a, nice, day, today&#125;</span><br><span class="line">reference: &#123;today, is, a, nice, day&#125;</span><br></pre></td></tr></table></figure>
<p>其中<code>&#123;today, is, a, nice, day&#125;</code>匹配，所以匹配度为5/6</p></li>
<li><p>使用<code>2-gram</code>进行匹配</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">candidate: &#123;it is, is a, a nice, nice day, day today&#125;</span><br><span class="line">reference: &#123;today is, is a, a nice, nice day&#125;</span><br></pre></td></tr></table></figure>
<p>其中<code>&#123;is a, a nice, nice day&#125;</code>匹配，所以匹配度为3/5</p></li>
<li><p>使用<code>3-gram</code>进行匹配</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">candidate: &#123;it is a, is a nice, a nice day, nice day today&#125;</span><br><span class="line">reference: &#123;today is a, is a nice, a nice day&#125;</span><br></pre></td></tr></table></figure>
<p>其中<code>&#123;is a nice, a nice day&#125;</code>匹配，所以匹配度为2/4</p></li>
<li><p>使用<code>4-gram</code>进行匹配</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">candidate: &#123;it is a nice, is a nice day, a nice day today&#125;</span><br><span class="line">reference: &#123;today is a nice, is a nice day&#125;</span><br></pre></td></tr></table></figure>
<p>其中<code>&#123;is a nice day&#125;</code>匹配，所以匹配度为1/3</p></li>
</ul></li>
<li><p>对匹配的<code>N-grams</code>计数进行修改，以确保它考虑到<code>reference</code>文本中单词的出现，而非奖励生成大量合理翻译单词的候选结果</p>
<blockquote>
<p>举例说明：</p>
<p>candidate: the the the the</p>
<p>reference: The cat is standing on the ground</p>
<p>如果按照<code>1-gram</code>的方法进行匹配，则匹配度为1，显然是不合理的，所以计算某个词的出现次数进行改进</p>
</blockquote>
<p>将计算某个词的出现次数的方法改为计算<strong>某个词在译文中出现的最小次数</strong>，如下所示， <span class="math display">\[
\operatorname{count}_{k}=\min ({c}_{k}, {s}_{k})
\]</span> 其中<span class="math inline">\(k\)</span>表示在机器译文（candidate）中出现的第<span class="math inline">\(k\)</span>个词语，<span class="math inline">\(c_{k}\)</span>则代表在机器译文中这个词语出现的次数，而<span class="math inline">\(s_{k}\)</span>则代表在人工译文（reference）中这个词语出现的次数。</p>
<p>由此，可以定义BLEU计算公式，首先定义几个数学符号：</p>
<ul>
<li>人工译文表示为<span class="math inline">\(s_{j}\)</span>，其中<span class="math inline">\({j} \in \mathrm{M}\)</span>，<span class="math inline">\(\mathrm{M}\)</span>表示有<span class="math inline">\(\mathrm{M}\)</span>个参考答案</li>
<li>翻译译文表示为<span class="math inline">\(c_{i}\)</span>，其中<span class="math inline">\(i \in \mathrm{E}\)</span>，<span class="math inline">\(\mathrm{E}\)</span>表示共有<span class="math inline">\(\mathrm{E}\)</span>个翻译</li>
<li><span class="math inline">\(n\)</span>表示<span class="math inline">\(n\)</span>个单词长度的词组集合，令<span class="math inline">\(k\)</span>表示第<span class="math inline">\(k\)</span>个词组</li>
<li><span class="math inline">\(h_{k}(c_{i})\)</span>表示第<span class="math inline">\(k\)</span>个词组在翻译译文<span class="math inline">\(c_{i}\)</span>中出现的次数</li>
<li><span class="math inline">\(h_{k}(s_{i,j})\)</span>表示第<span class="math inline">\(k\)</span>个词组在人工译文<span class="math inline">\(s_{i,j}\)</span>中出现的次数</li>
</ul>
<p>最后可以得到计算每个<code>n-gram</code>的公式， <span class="math display">\[
P_{n}=\frac{\sum_{i}^{\mathrm{E}} \sum_{k}^\mathrm{K} \min(h_{k}(c_{i}), \max_{j \in \mathrm{M}}h_{k}(s_{i,j})) }
{\sum_{i}^{\mathrm{E}} \sum_{k}^\mathrm{K}\min(h_{k}(c_{i}))}
\]</span> 第一个求和符号统计的是所有的翻译句子，因为计算时可能有多个句子；第二个求和符号是统计一条翻译句子中所有的<code>n-gram</code>，<span class="math inline">\(\max_{j \in \mathrm{M}}h_{k}(s_{i,j})\)</span>表示第<span class="math inline">\(i\)</span>条翻译句子对应的<span class="math inline">\(\mathrm{M}\)</span>条人工译文中包含最多第<span class="math inline">\(k\)</span>个词组的句子中第<span class="math inline">\(k\)</span>个词组的数量</p></li>
<li><p><code>n-gram</code>匹配度可能会随着句子长度的变短而变好，为了避免这种现象，BLEU在最后的评分结果中引入了长度惩罚因子（Brevity Penalty） <span class="math display">\[
B P=\left\{\begin{array}{lll}
1 &amp; \text { if } &amp; l_{c}&gt;l s \\
e^{1-\frac{l_{s}}{l_{c}}} &amp; \text { if } &amp; l_{c}&lt;=l_{s}
\end{array}\right.
\]</span> 其中，<span class="math inline">\(l_{c}\)</span>表示机器翻译译文的长度，<span class="math inline">\(l_{s}\)</span>表示参考译文的有效长度，<strong>当存在多个参考译文时，选取和翻译译文最接近的长度</strong>。当翻译译文长度大于参考译文长度时，惩罚因子为1，意味着不惩罚，只有翻译译文长度小于参考译文长度时，才会计算惩罚因子。</p></li>
<li><p>计算BLEU最终公式</p>
<p>为了平衡各阶统计量的作用，对各阶统计量进行加权求和，一般来说，<span class="math inline">\(N\)</span>取4，最多只统计<code>4-gram</code>的精度，<span class="math inline">\(\boldsymbol{W}_{n}\)</span>取<span class="math inline">\(1/N\)</span>，进行均匀加权，最终公式如下： <span class="math display">\[
B L E U=B P \times \exp \left(\sum_{n=1}^{N} \boldsymbol{W}_{n} \log P_{n}\right)
\]</span></p></li>
</ul></li>
<li><p>计算工具</p>
<ul>
<li><p><code>nltk</code></p>
<ul>
<li><p>计算独立的BLEU：也就是只计算某一种<code>n-gram</code>的BLEU</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.translate.bleu_score <span class="keyword">import</span> sentence_bleu</span><br><span class="line"></span><br><span class="line">sentence1 = <span class="string">&quot;it is a guide to action which ensures that the military always obeys the commands of the party&quot;</span></span><br><span class="line">sentence2 = <span class="string">&quot;it is a guide to action that ensures that the military will forever heed party commands&quot;</span></span><br><span class="line">sentence3 = <span class="string">&quot;it is the guiding principle which guarantees the military forces always being under the command of the party&quot;</span></span><br><span class="line">sentence4 = <span class="string">&quot;it is the practical guide for the army always to heed the directions of the party&quot;</span></span><br><span class="line"></span><br><span class="line">candidate = <span class="built_in">list</span>(sentence1.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">reference = [<span class="built_in">list</span>(sentence2.split(<span class="string">&quot; &quot;</span>)), <span class="built_in">list</span>(sentence3.split(<span class="string">&quot; &quot;</span>)), <span class="built_in">list</span>(sentence4.split(<span class="string">&quot; &quot;</span>))]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Individual 1-gram: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(sentence_bleu(reference, candidate, weights=(<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>))))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Individual 2-gram: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(sentence_bleu(reference, candidate, weights=(<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>))))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Individual 3-gram: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(sentence_bleu(reference, candidate, weights=(<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>))))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Individual 4-gram: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(sentence_bleu(reference, candidate, weights=(<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>))))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Individual 1-gram: 0.9444444444444444</span></span><br><span class="line"><span class="comment"># Individual 2-gram: 0.5882352941176471</span></span><br><span class="line"><span class="comment"># Individual 3-gram: 0.4375</span></span><br><span class="line"><span class="comment"># Individual 4-gram: 0.26666666666666666</span></span><br></pre></td></tr></table></figure>
<ol type="1">
<li>计算<span class="math inline">\(P_{1}\)</span>：</li>
</ol>
<table>
<colgroup>
<col style="width: 7%">
<col style="width: 7%">
<col style="width: 7%">
<col style="width: 7%">
<col style="width: 7%">
<col style="width: 25%">
<col style="width: 36%">
</colgroup>
<thead>
<tr class="header">
<th>词</th>
<th>候选译文</th>
<th>参考译文1</th>
<th>参考译文2</th>
<th>参考译文3</th>
<th><span class="math inline">\(\max_{j \in \mathrm{M}}h(s)\)</span></th>
<th><span class="math inline">\(\min(h(c), \max_{j \in \mathrm{M}}h(s))\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>it</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="even">
<td>is</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="odd">
<td>a</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="even">
<td>guide</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="odd">
<td>to</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="even">
<td>action</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="odd">
<td>which</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="even">
<td>ensures</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="odd">
<td>that</td>
<td>1</td>
<td>2</td>
<td>0</td>
<td>0</td>
<td>2</td>
<td>1</td>
</tr>
<tr class="even">
<td>the</td>
<td>3</td>
<td>1</td>
<td>3</td>
<td>3</td>
<td>3</td>
<td>3</td>
</tr>
<tr class="odd">
<td>military</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="even">
<td>always</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="odd">
<td>obeys</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>commands</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="odd">
<td>of</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="even">
<td>party</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
<p><span class="math display">\[
P_{1}=\frac{1+1+1+1+1+1+1+1+1+3+1+1+0+1+1+1}{1+1+1+1+1+1+1+1+1+3+1+1+1+1+1+1}=\frac{17}{18}=0.9444444444444444
\]</span></p>
<ol start="2" type="1">
<li>计算<span class="math inline">\(P_{2}\)</span></li>
</ol>
<table>
<colgroup>
<col style="width: 12%">
<col style="width: 6%">
<col style="width: 7%">
<col style="width: 7%">
<col style="width: 7%">
<col style="width: 24%">
<col style="width: 34%">
</colgroup>
<thead>
<tr class="header">
<th>词</th>
<th>候选译文</th>
<th>参考译文1</th>
<th>参考译文2</th>
<th>参考译文3</th>
<th><span class="math inline">\(\max_{j \in \mathrm{M}}h(s)\)</span></th>
<th><span class="math inline">\(\min(h(c), \max_{j \in \mathrm{M}}h(s))\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ensures that</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="even">
<td>guide to</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="odd">
<td>which ensures</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>obeys the</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td>commands of</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>that the</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="odd">
<td>a guide</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="even">
<td>of the</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="odd">
<td>always obeys</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>the commands</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td>to action</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="even">
<td>the party</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="odd">
<td>is a</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="even">
<td>action which</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td>It is</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="even">
<td>military always</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td>the military</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
<p><span class="math display">\[
P_{2}=\frac{10}{17}=0.5882352941176471
\]</span></p>
<ol start="3" type="1">
<li>计算<span class="math inline">\(P_{3}\)</span></li>
</ol>
<table>
<colgroup>
<col style="width: 16%">
<col style="width: 6%">
<col style="width: 7%">
<col style="width: 7%">
<col style="width: 7%">
<col style="width: 23%">
<col style="width: 32%">
</colgroup>
<thead>
<tr class="header">
<th>词</th>
<th>候选译文</th>
<th>参考译文1</th>
<th>参考译文2</th>
<th>参考译文3</th>
<th><span class="math inline">\(\max_{j \in \mathrm{M}}h(s)\)</span></th>
<th><span class="math inline">\(\min(h(c), \max_{j \in \mathrm{M}}h(s))\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ensures that the</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="even">
<td>which ensures that</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td>action which ensures</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>a guide to</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="odd">
<td>military always obeys</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>the commands of</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td>commands of the</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>to action which</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td>the military always</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>obeys the commands</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td>It is a</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="even">
<td>of the party</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="odd">
<td>is a guide</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="even">
<td>that the military</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="odd">
<td>always obeys the</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>guide to action</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
<p><span class="math display">\[
P_{3}=\frac{7}{16}=0.4375
\]</span></p>
<ol start="4" type="1">
<li>计算<span class="math inline">\(P_{4}\)</span></li>
</ol>
<table style="width:100%;">
<colgroup>
<col style="width: 19%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 22%">
<col style="width: 31%">
</colgroup>
<thead>
<tr class="header">
<th>词</th>
<th>候选译文</th>
<th>参考译文1</th>
<th>参考译文2</th>
<th>参考译文3</th>
<th><span class="math inline">\(\max_{j \in \mathrm{M}}h(s)\)</span></th>
<th><span class="math inline">\(\min(h(c), \max_{j \in \mathrm{M}}h(s))\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>to action which ensures</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>action which ensures that</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td>guide to action which</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>obeys the commands of</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td>which ensures that the</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>commands of the party</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td>ensures that the military</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="even">
<td>a guide to action</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="odd">
<td>always obeys the commands</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>that the military always</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td>the commands of the</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>the military always obeys</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td>military always obeys the</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>is a guide to</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="odd">
<td>It is a guide</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
<p><span class="math display">\[
P_{4}=\frac{4}{15}=0.26666666666666666
\]</span></p></li>
<li><p>计算累积的BLEU：指的是为各个<code>gram</code>对应的权重加权，来计算得到一个加权几何平均，需要注意以下几点：</p>
<ol type="1">
<li>BLEU-4并不是只看<code>4-gram</code>的情况，而是计算从<code>1-gram</code>到<code>4-gram</code>的累积分数，加权策略为<code>1-gram</code>、<code>2-gram</code>、<code>3-gram</code>、<code>4-gram</code>的权重各占25%</li>
<li><strong>默认情况下（不加<code>weights</code>参数的情况下），<code>sentence_bleu()</code>和<code>corpus_bleu()</code>都是计算BLEU-4分数的</strong></li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Cumulative 1-gram: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(sentence_bleu(reference, candidate, weights=(<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>))))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Cumulative 2-gram: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(sentence_bleu(reference, candidate, weights=(<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0</span>, <span class="number">0</span>))))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Cumulative 3-gram: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(sentence_bleu(reference, candidate, weights=(<span class="number">0.33</span>, <span class="number">0.33</span>, <span class="number">0.33</span>, <span class="number">0</span>))))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Cumulative 4-gram: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(sentence_bleu(reference, candidate, weights=(<span class="number">0.25</span>, <span class="number">0.25</span>, <span class="number">0.25</span>, <span class="number">0.25</span>))))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Cumulative 1-gram: 0.9444444444444444</span></span><br><span class="line"><span class="comment"># Cumulative 2-gram: 0.7453559924999299</span></span><br><span class="line"><span class="comment"># Cumulative 3-gram: 0.6270220769211224</span></span><br><span class="line"><span class="comment"># Cumulative 4-gram: 0.5045666840058485</span></span><br></pre></td></tr></table></figure>
<ol type="1">
<li><p>计算<code>BLEU-1</code>：</p>
<p>首先翻译句子的长度为18，而参考译文句子长度分别为16、18、16，选择与翻译句子长度最接近的参考译文句子，此时惩罚因子为1，即不惩罚。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">math.exp(<span class="number">1</span> * math.log(<span class="number">0.9444444444444444</span>))</span><br><span class="line"><span class="comment"># 0.9444444444444444</span></span><br></pre></td></tr></table></figure></li>
<li><p>计算<code>BLEU-2</code>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">math.exp(<span class="number">0.5</span> * math.log(<span class="number">0.9444444444444444</span>) + <span class="number">0.5</span> * math.log(<span class="number">0.5882352941176471</span>))</span><br><span class="line"><span class="comment"># 0.7453559924999299</span></span><br></pre></td></tr></table></figure></li>
<li><p>计算<code>BLEU-3</code>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">math.exp(<span class="number">0.33</span> * math.log(<span class="number">0.9444444444444444</span>) + <span class="number">0.33</span> * math.log(<span class="number">0.5882352941176471</span>) + <span class="number">0.33</span> * math.log(<span class="number">0.4375</span>))</span><br><span class="line"><span class="comment"># 0.6270220769211224</span></span><br></pre></td></tr></table></figure></li>
<li><p>计算<code>BLEU-4</code>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">math.exp(<span class="number">0.25</span> * math.log(<span class="number">0.9444444444444444</span>) + <span class="number">0.25</span> * math.log(<span class="number">0.5882352941176471</span>) </span><br><span class="line">         + <span class="number">0.25</span> * math.log(<span class="number">0.4375</span>) + <span class="number">0.25</span> * math.log(<span class="number">0.26666666666666666</span>))</span><br><span class="line"><span class="comment"># 0.5045666840058485</span></span><br></pre></td></tr></table></figure></li>
</ol></li>
<li><p>调用<code>corpus_bleu()</code>方法求得语料级别的BLEU</p>
<ol type="1">
<li><p><code>sentence_bleu()</code>和<code>corpus_bleu()</code>输入参数的对比（重点关注<code>sentence_bleu()</code>方法的<code>references</code>和<code>hypothesis</code>参数，以及<code>corpus_bleu()</code>方法的<code>list_of_references</code>和<code>hypotheses</code>参数）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sentence_bleu</span>(<span class="params">references, hypothesis, weights=(<span class="params"><span class="number">0.25</span>, <span class="number">0.25</span>, <span class="number">0.25</span>, <span class="number">0.25</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="function">                  smoothing_function=<span class="literal">None</span></span>):</span></span><br><span class="line">		<span class="string">&quot;&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param references: reference sentences</span></span><br><span class="line"><span class="string">    :type references: list(list(str))</span></span><br><span class="line"><span class="string">    :param hypothesis: a hypothesis sentence</span></span><br><span class="line"><span class="string">    :type hypothesis: list(str)</span></span><br><span class="line"><span class="string">    :param weights: weights for unigrams, bigrams, trigrams and so on</span></span><br><span class="line"><span class="string">    :type weights: list(float)</span></span><br><span class="line"><span class="string">    :return: The sentence-level BLEU score.</span></span><br><span class="line"><span class="string">    :rtype: float</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> corpus_bleu([references], [hypothesis], weights, smoothing_function)</span><br><span class="line"></span><br><span class="line">references = [ [<span class="string">&quot;This&quot;</span>, <span class="string">&quot;is&quot;</span>, <span class="string">&quot;a&quot;</span>, <span class="string">&quot;cat&quot;</span>], [<span class="string">&quot;This&quot;</span>, <span class="string">&quot;is&quot;</span>, <span class="string">&quot;a&quot;</span>, <span class="string">&quot;feline&quot;</span>] ]</span><br><span class="line">hypothesis = [<span class="string">&quot;This&quot;</span>, <span class="string">&quot;is&quot;</span>, <span class="string">&quot;cat&quot;</span>]</span><br><span class="line">sentence_bleu(references, hypothesis)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">corpus_bleu</span>(<span class="params">list_of_references, hypotheses, weights=(<span class="params"><span class="number">0.25</span>, <span class="number">0.25</span>, <span class="number">0.25</span>, <span class="number">0.25</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="function">                smoothing_function=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param references: a corpus of lists of reference sentences, w.r.t. hypotheses</span></span><br><span class="line"><span class="string">    :type references: list(list(list(str)))</span></span><br><span class="line"><span class="string">    :param hypotheses: a list of hypothesis sentences</span></span><br><span class="line"><span class="string">    :type hypotheses: list(list(str))</span></span><br><span class="line"><span class="string">    :param weights: weights for unigrams, bigrams, trigrams and so on</span></span><br><span class="line"><span class="string">    :type weights: list(float)</span></span><br><span class="line"><span class="string">    :return: The corpus-level BLEU score.</span></span><br><span class="line"><span class="string">    :rtype: float</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure></li>
<li><p>计算语料级别的BLEU值</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.translate.bleu_score <span class="keyword">import</span> corpus_bleu</span><br><span class="line"></span><br><span class="line">s1 = <span class="string">&quot;the dog bit the man&quot;</span></span><br><span class="line">s2 = <span class="string">&quot;the dog had bit the man&quot;</span></span><br><span class="line"></span><br><span class="line">s3 = <span class="string">&quot;it was not unexpected&quot;</span></span><br><span class="line">s4 = <span class="string">&quot;no one was surprised&quot;</span></span><br><span class="line"></span><br><span class="line">s5 = <span class="string">&quot;the man bit him first&quot;</span></span><br><span class="line">s6 = <span class="string">&quot;the man had bitten the dog&quot;</span></span><br><span class="line"></span><br><span class="line">s7 = <span class="string">&quot;the dog bit the man&quot;</span></span><br><span class="line">s8 = <span class="string">&quot;it was not surprising&quot;</span></span><br><span class="line">s9 = <span class="string">&quot;the man had just bitten him&quot;</span></span><br><span class="line"></span><br><span class="line">candidates = [<span class="built_in">list</span>(s7.split(<span class="string">&quot; &quot;</span>)), <span class="built_in">list</span>(s8.split(<span class="string">&quot; &quot;</span>)), <span class="built_in">list</span>(s9.split(<span class="string">&quot; &quot;</span>))]</span><br><span class="line">references = [</span><br><span class="line">    [<span class="built_in">list</span>(s1.split(<span class="string">&quot; &quot;</span>)), <span class="built_in">list</span>(s2.split(<span class="string">&quot; &quot;</span>))],</span><br><span class="line">    [<span class="built_in">list</span>(s3.split(<span class="string">&quot; &quot;</span>)), <span class="built_in">list</span>(s4.split(<span class="string">&quot; &quot;</span>))],</span><br><span class="line">    [<span class="built_in">list</span>(s5.split(<span class="string">&quot; &quot;</span>)), <span class="built_in">list</span>(s6.split(<span class="string">&quot; &quot;</span>))]</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Corpus BLEU: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(corpus_bleu(references, candidates)))</span><br><span class="line"><span class="comment"># Corpus BLEU: 0.5719285395120957</span></span><br></pre></td></tr></table></figure>
<p>PS：需要注意的是，<strong>计算所有单个句子的BLEU值然后求平均和直接计算<code>corpus</code>级别的BLEU值不同</strong>，如下所示，</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">reference1 = [<span class="built_in">list</span>(s1.split(<span class="string">&quot; &quot;</span>)), <span class="built_in">list</span>(s2.split(<span class="string">&quot; &quot;</span>))]</span><br><span class="line">candidate1 = <span class="built_in">list</span>(s7.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line"></span><br><span class="line">reference2 = [<span class="built_in">list</span>(s3.split(<span class="string">&quot; &quot;</span>)), <span class="built_in">list</span>(s4.split(<span class="string">&quot; &quot;</span>))]</span><br><span class="line">candidate2 = <span class="built_in">list</span>(s8.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line"></span><br><span class="line">reference3 = [<span class="built_in">list</span>(s5.split(<span class="string">&quot; &quot;</span>)), <span class="built_in">list</span>(s6.split(<span class="string">&quot; &quot;</span>))]</span><br><span class="line">candidate3 = <span class="built_in">list</span>(s9.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Sentence1 BLEU: &#x27;</span>, sentence_bleu(reference1, candidate1))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Sentence2 BLEU: &#x27;</span>, sentence_bleu(reference2, candidate2))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Sentence3 BLEU: &#x27;</span>, sentence_bleu(reference3, candidate3))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Average Sentence BLEU: &#x27;</span>, (sentence_bleu(reference1, candidate1) + </span><br><span class="line">                                        sentence_bleu(reference2, candidate2) + </span><br><span class="line">                                        sentence_bleu(reference3, candidate3)) / <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sentence1 BLEU:  1.0</span></span><br><span class="line"><span class="comment"># Sentence2 BLEU:  8.636168555094496e-78</span></span><br><span class="line"><span class="comment"># Sentence3 BLEU:  6.562069055463047e-78</span></span><br><span class="line"><span class="comment"># Average Sentence BLEU:  0.3333333333333333</span></span><br></pre></td></tr></table></figure>
<p>正确的计算方法如下所示，将每个句子的<code>i-gram</code>概率的分子和分母对应相加，最后得出统一的4个独立BLEU（<span class="math inline">\(P_{i},i \in {1,2,3,4}\)</span>），再按照公式进行计算，特别地，在计算BP惩罚因子时，翻译句子长度由所有翻译句子长度相加得到，参考译文长度由所有与对应翻译句子长度最接近的参考译文长度相加得到， <span class="math display">\[
B L E U=B P \times \exp \left(\sum_{n=1}^{4} 0.25* \log P_{n}\right)
\]</span></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">from</span> nltk.translate.bleu_score <span class="keyword">import</span> modified_precision</span><br><span class="line">p_numerators = Counter()</span><br><span class="line">p_denominators = Counter()</span><br><span class="line"><span class="keyword">for</span> refs, hyps <span class="keyword">in</span> <span class="built_in">zip</span>(references, candidates):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">5</span>):</span><br><span class="line">        p_i = modified_precision(refs, hyps, i)</span><br><span class="line">        p_numerators[i] += p_i.numerator</span><br><span class="line">        p_denominators[i] += p_i.denominator</span><br><span class="line">        </span><br><span class="line"><span class="built_in">print</span>(p_numerators, p_denominators)</span><br><span class="line"><span class="comment"># Counter(&#123;1: 13, 2: 8, 3: 5, 4: 2&#125;) Counter(&#123;1: 15, 2: 12, 3: 9, 4: 6&#125;)</span></span><br><span class="line">res = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">5</span>):</span><br><span class="line">    res += <span class="number">0.25</span> * math.log(p_numerators[i] / p_denominators[i])</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 本例中惩罚因子为1</span></span><br><span class="line"><span class="built_in">print</span>(math.exp(res))</span><br><span class="line"><span class="comment"># 0.5719285395120957</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">list</span>(<span class="built_in">zip</span>(references, candidates))</span><br><span class="line"><span class="comment"># [([[&#x27;the&#x27;, &#x27;dog&#x27;, &#x27;bit&#x27;, &#x27;the&#x27;, &#x27;man&#x27;],</span></span><br><span class="line"><span class="comment">#    [&#x27;the&#x27;, &#x27;dog&#x27;, &#x27;had&#x27;, &#x27;bit&#x27;, &#x27;the&#x27;, &#x27;man&#x27;]],</span></span><br><span class="line"><span class="comment">#   [&#x27;the&#x27;, &#x27;dog&#x27;, &#x27;bit&#x27;, &#x27;the&#x27;, &#x27;man&#x27;]),</span></span><br><span class="line"><span class="comment">#  ([[&#x27;it&#x27;, &#x27;was&#x27;, &#x27;not&#x27;, &#x27;unexpected&#x27;], [&#x27;no&#x27;, &#x27;one&#x27;, &#x27;was&#x27;, &#x27;surprised&#x27;]],</span></span><br><span class="line"><span class="comment">#   [&#x27;it&#x27;, &#x27;was&#x27;, &#x27;not&#x27;, &#x27;surprising&#x27;]),</span></span><br><span class="line"><span class="comment">#  ([[&#x27;the&#x27;, &#x27;man&#x27;, &#x27;bit&#x27;, &#x27;him&#x27;, &#x27;first&#x27;],</span></span><br><span class="line"><span class="comment">#    [&#x27;the&#x27;, &#x27;man&#x27;, &#x27;had&#x27;, &#x27;bitten&#x27;, &#x27;the&#x27;, &#x27;dog&#x27;]],</span></span><br><span class="line"><span class="comment">#   [&#x27;the&#x27;, &#x27;man&#x27;, &#x27;had&#x27;, &#x27;just&#x27;, &#x27;bitten&#x27;, &#x27;him&#x27;])]</span></span><br></pre></td></tr></table></figure></li>
</ol></li>
</ul></li>
<li><p><code>sacrebleu</code></p>
<ul>
<li><p>计算<code>sentence bleu</code>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sacrebleu</span><br><span class="line">sentence1 = <span class="string">&quot;it is a guide to action which ensures that the military always obeys the commands of the party&quot;</span></span><br><span class="line">sentence2 = <span class="string">&quot;it is a guide to action that ensures that the military will forever heed party commands&quot;</span></span><br><span class="line">sentence3 = <span class="string">&quot;it is the guiding principle which guarantees the military forces always being under the command of the party&quot;</span></span><br><span class="line">sentence4 = <span class="string">&quot;it is the practical guide for the army always to heed the directions of the party&quot;</span></span><br><span class="line">bleu = sacrebleu.sentence_bleu(sentence1, [sentence2, sentence3, sentence4])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Sentence BLEU: &quot;</span>, bleu)</span><br><span class="line"><span class="comment"># Sentence BLEU:  BLEU = 50.46 94.4/58.8/43.8/26.7 (BP = 1.000 ratio = 1.000 hyp_len = 18 ref_len = 18)</span></span><br></pre></td></tr></table></figure></li>
<li><p>计算<code>corpus bleu</code>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">refs = [[<span class="string">&#x27;the dog bit the man&#x27;</span>, <span class="string">&#x27;it was not unexpected&#x27;</span>, <span class="string">&#x27;the man bit him first&#x27;</span>],</span><br><span class="line">        [<span class="string">&#x27;the dog had bit the man&#x27;</span>, <span class="string">&#x27;no one was surprised&#x27;</span>, <span class="string">&#x27;the man had bitten the dog&#x27;</span>]]</span><br><span class="line">sys = [<span class="string">&#x27;the dog bit the man&#x27;</span>, <span class="string">&quot;it was not surprising&quot;</span>, <span class="string">&#x27;the man had just bitten him&#x27;</span>]</span><br><span class="line">bleu = sacrebleu.corpus_bleu(sys, refs)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Corpus BLEU: &quot;</span>, bleu)</span><br><span class="line"><span class="comment"># Corpus BLEU:  BLEU = 57.19 86.7/66.7/55.6/33.3 (BP = 1.000 ratio = 1.000 hyp_len = 15 ref_len = 15)</span></span><br></pre></td></tr></table></figure></li>
</ul></li>
<li><p><a href="https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl"><code>multi-bleu.perl</code></a>：使用<code>multi-bleu</code>进行评测要求事先把句子进行<code>tokenize</code>，这意味着<code>multi-bleu</code>得到的分数受<code>tokenizer</code>如何分词的影响</p></li>
<li><p><a href="https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/mteval-v14.pl"><code>mteval-v14.pl</code></a>：脚本内部有一套标准的分词器，不需要分词直接输入句子就可以进行评测，计算的值与<code>sacrebleu</code>计算的值相同</p></li>
</ul></li>
</ol>
<p>参考文献：</p>
<ol type="1">
<li><a href="https://zhuanlan.zhihu.com/p/404381278">BLEU指标及评测脚本使用的一些误解</a></li>
<li><a href="https://blog.csdn.net/g11d111/article/details/100103208">机器翻译评价指标BLEU介绍</a></li>
<li><a href="https://blog.csdn.net/qq_30232405/article/details/104219396">BLEU算法（例子和公式解释）</a></li>
<li><a href="https://www.cnblogs.com/by-dream/p/7679284.html">机器翻译评测——BLEU算法详解 (新增 在线计算BLEU分值)</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/338488036">BLEU score评估模型</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/223048748">BLEU详解</a></li>
<li><a href="https://blog.csdn.net/guolindonggld/article/details/56966200">机器翻译评价指标之BLEU详细计算过程</a></li>
<li><a href="https://github.com/nltk/nltk/blob/develop/nltk/translate/bleu_score.py">nltk/bleu_score.py at develop · nltk/nltk (github.com)</a></li>
<li><a href="https://github.com/moses-smt/mosesdecoder">moses-smt/mosesdecoder: Moses, the machine translation system (github.com)</a></li>
</ol>
]]></content>
      <categories>
        <category>机器翻译</category>
      </categories>
      <tags>
        <tag>机器翻译</tag>
      </tags>
  </entry>
  <entry>
    <title>LibRec学习笔记（二）：SVD++算法</title>
    <url>/2022/04/04/LibRec%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89-SVD-%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>上一篇我们具体介绍了BiasedMF算法在LibRec库中的实现，实际上是为了本篇介绍SVD++算法实现做铺垫，在代码中我们也可以看到，<code>SVDPlusPlusRecommender</code>类继承了<code>BiasedMFRecommender</code>类，本篇主要也是从三个方面展开，分别是预测公式、损失函数公式和更新公式。 <span id="more"></span> <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> net.librec.recommender.cf.rating;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="keyword">import</span> net.librec.annotation.ModelData;</span><br><span class="line"><span class="keyword">import</span> net.librec.common.LibrecException;</span><br><span class="line"><span class="keyword">import</span> net.librec.math.structure.DenseMatrix;</span><br><span class="line"><span class="keyword">import</span> net.librec.math.structure.DenseVector;</span><br><span class="line"><span class="keyword">import</span> net.librec.math.structure.SequentialSparseVector;</span><br><span class="line"><span class="keyword">import</span> net.librec.math.structure.VectorBasedDenseVector;</span><br><span class="line"><span class="keyword">import</span> net.librec.math.structure.Vector.VectorEntry;</span><br><span class="line"></span><br><span class="line"><span class="meta">@ModelData(&#123;&quot;isRating&quot;, &quot;svdplusplus&quot;, &quot;userFactors&quot;, &quot;itemFactors&quot;, &quot;userBiases&quot;, &quot;itemBiases&quot;, &quot;impItemFactors&quot;, &quot;trainMatrix&quot;&#125;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SVDPlusPlusRecommender</span> <span class="keyword">extends</span> <span class="title">BiasedMFRecommender</span> </span>&#123;</span><br><span class="line">    <span class="keyword">protected</span> DenseMatrix impItemFactors;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">double</span> regImpItem;</span><br><span class="line">    <span class="keyword">private</span> DenseVector factorVector;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">SVDPlusPlusRecommender</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">()</span> <span class="keyword">throws</span> LibrecException </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.setup();</span><br><span class="line">        <span class="keyword">this</span>.regImpItem = <span class="keyword">this</span>.conf.getDouble(<span class="string">&quot;rec.impItem.regularization&quot;</span>, <span class="number">0.015D</span>);</span><br><span class="line">        <span class="comment">// 物品隐因子偏置向量 对应公式中的y</span></span><br><span class="line">        <span class="keyword">this</span>.impItemFactors = <span class="keyword">new</span> DenseMatrix(<span class="keyword">this</span>.numItems, <span class="keyword">this</span>.numFactors);</span><br><span class="line">        <span class="comment">// 物品隐因子偏置向量初始化</span></span><br><span class="line">        <span class="keyword">this</span>.impItemFactors.init((<span class="keyword">double</span>)<span class="keyword">this</span>.initMean, (<span class="keyword">double</span>)<span class="keyword">this</span>.initStd);</span><br><span class="line">        <span class="keyword">this</span>.factorVector = <span class="keyword">new</span> VectorBasedDenseVector(<span class="keyword">this</span>.numFactors);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">trainModel</span><span class="params">()</span> <span class="keyword">throws</span> LibrecException </span>&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> iterationStep = <span class="number">1</span>; iterationStep &lt;= <span class="keyword">this</span>.numIterations; ++iterationStep) &#123;</span><br><span class="line">            <span class="keyword">this</span>.loss = <span class="number">0.0D</span>;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> userIndex = <span class="number">0</span>; userIndex &lt; <span class="keyword">this</span>.numUsers; ++userIndex) &#123;</span><br><span class="line">                SequentialSparseVector userVector = <span class="keyword">this</span>.trainMatrix.row(userIndex);</span><br><span class="line">                <span class="keyword">if</span> (userVector.size() != <span class="number">0</span>) &#123;</span><br><span class="line">                    <span class="keyword">double</span>[] steps = <span class="keyword">new</span> <span class="keyword">double</span>[<span class="keyword">this</span>.numFactors];</span><br><span class="line">                    <span class="keyword">this</span>.factorVector.assign((indexx, value) -&gt; &#123;</span><br><span class="line">                        <span class="keyword">return</span> <span class="number">0.0D</span>;</span><br><span class="line">                    &#125;);</span><br><span class="line">                    Iterator var5 = userVector.iterator();</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">while</span>(var5.hasNext()) &#123;</span><br><span class="line">                        VectorEntry vectorEntry = (VectorEntry)var5.next();</span><br><span class="line">                        <span class="keyword">this</span>.factorVector.assign((indexx, value) -&gt; &#123;</span><br><span class="line">                            <span class="keyword">return</span> <span class="keyword">this</span>.impItemFactors.row(vectorEntry.index()).get(indexx) + value;</span><br><span class="line">                        &#125;);</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">double</span> scale = Math.pow((<span class="keyword">double</span>)userVector.getNumEntries(), -<span class="number">0.5D</span>);</span><br><span class="line">                    <span class="keyword">this</span>.factorVector.assign((indexx, value) -&gt; &#123;</span><br><span class="line">                        <span class="keyword">return</span> value * scale;</span><br><span class="line">                    &#125;);</span><br><span class="line">                    Iterator var7 = userVector.iterator();</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">double</span> factor;</span><br><span class="line">                    <span class="keyword">while</span>(var7.hasNext()) &#123;</span><br><span class="line">                        VectorEntry vectorEntry = (VectorEntry)var7.next();</span><br><span class="line">                        <span class="keyword">int</span> itemIndex = vectorEntry.index();</span><br><span class="line">                        <span class="keyword">double</span> error = vectorEntry.get() - <span class="keyword">this</span>.predict(userIndex, itemIndex, <span class="keyword">this</span>.factorVector);</span><br><span class="line">                        <span class="keyword">this</span>.loss += error * error;</span><br><span class="line">                        factor = <span class="keyword">this</span>.userBiases.get(userIndex);</span><br><span class="line">                        <span class="keyword">this</span>.userBiases.plus(userIndex, (<span class="keyword">double</span>)<span class="keyword">this</span>.learnRate * (error - <span class="keyword">this</span>.regBias * factor));</span><br><span class="line">                        <span class="keyword">this</span>.loss += <span class="keyword">this</span>.regBias * factor * factor;</span><br><span class="line">                        <span class="keyword">double</span> itemBias = <span class="keyword">this</span>.itemBiases.get(itemIndex);</span><br><span class="line">                        <span class="keyword">this</span>.itemBiases.plus(itemIndex, (<span class="keyword">double</span>)<span class="keyword">this</span>.learnRate * (error - <span class="keyword">this</span>.regBias * itemBias));</span><br><span class="line">                        <span class="keyword">this</span>.loss += <span class="keyword">this</span>.regBias * itemBias * itemBias;</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">for</span>(<span class="keyword">int</span> factorIndex = <span class="number">0</span>; factorIndex &lt; <span class="keyword">this</span>.numFactors; ++factorIndex) &#123;</span><br><span class="line">                            <span class="keyword">double</span> userFactor = <span class="keyword">this</span>.userFactors.get(userIndex, factorIndex);</span><br><span class="line">                            <span class="keyword">double</span> itemFactor = <span class="keyword">this</span>.itemFactors.get(itemIndex, factorIndex);</span><br><span class="line">                            <span class="keyword">this</span>.userFactors.plus(userIndex, factorIndex, (<span class="keyword">double</span>)<span class="keyword">this</span>.learnRate * (error * itemFactor - (<span class="keyword">double</span>)<span class="keyword">this</span>.regUser * userFactor));</span><br><span class="line">                            <span class="keyword">this</span>.itemFactors.plus(itemIndex, factorIndex, (<span class="keyword">double</span>)<span class="keyword">this</span>.learnRate * (error * (userFactor + <span class="keyword">this</span>.factorVector.get(factorIndex)) - (<span class="keyword">double</span>)<span class="keyword">this</span>.regItem * itemFactor));</span><br><span class="line">                            <span class="keyword">this</span>.loss += (<span class="keyword">double</span>)<span class="keyword">this</span>.regUser * userFactor * userFactor + (<span class="keyword">double</span>)<span class="keyword">this</span>.regItem * itemFactor * itemFactor;</span><br><span class="line">                            steps[factorIndex] += error * itemFactor * scale;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">int</span> size = userVector.getNumEntries();</span><br><span class="line">                    Iterator var23 = userVector.iterator();</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">while</span>(var23.hasNext()) &#123;</span><br><span class="line">                        VectorEntry vectorEntry = (VectorEntry)var23.next();</span><br><span class="line">                        <span class="keyword">int</span> index = vectorEntry.index();</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">for</span>(<span class="keyword">int</span> factorIndex = <span class="number">0</span>; factorIndex &lt; <span class="keyword">this</span>.numFactors; ++factorIndex) &#123;</span><br><span class="line">                            factor = <span class="keyword">this</span>.impItemFactors.get(index, factorIndex);</span><br><span class="line">                            <span class="keyword">this</span>.impItemFactors.plus(index, factorIndex, (<span class="keyword">double</span>)<span class="keyword">this</span>.learnRate * (steps[factorIndex] - <span class="keyword">this</span>.regImpItem * factor * (<span class="keyword">double</span>)size));</span><br><span class="line">                            <span class="keyword">this</span>.loss += <span class="keyword">this</span>.regImpItem * factor * factor * (<span class="keyword">double</span>)size;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">this</span>.loss *= <span class="number">0.5D</span>;</span><br><span class="line">            <span class="keyword">if</span> (<span class="keyword">this</span>.isConverged(iterationStep) &amp;&amp; <span class="keyword">this</span>.earlyStop) &#123;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">this</span>.updateLRate(iterationStep);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">double</span> <span class="title">predict</span><span class="params">(<span class="keyword">int</span> userIndex, <span class="keyword">int</span> itemIndex, DenseVector factorVector)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">double</span> value = <span class="keyword">this</span>.userBiases.get(userIndex) + <span class="keyword">this</span>.itemBiases.get(itemIndex) + <span class="keyword">this</span>.globalMean;</span><br><span class="line">        DenseVector userFactorVector = <span class="keyword">this</span>.userFactors.row(userIndex);</span><br><span class="line">        DenseVector itemFactorVector = <span class="keyword">this</span>.itemFactors.row(itemIndex);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> index = <span class="number">0</span>; index &lt; <span class="keyword">this</span>.numFactors; ++index) &#123;</span><br><span class="line">            value += (factorVector.get(index) + userFactorVector.get(index)) * itemFactorVector.get(index);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> value;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">double</span> <span class="title">predict</span><span class="params">(<span class="keyword">int</span> userIndex, <span class="keyword">int</span> itemIndex)</span> </span>&#123;</span><br><span class="line">        SequentialSparseVector userVector = <span class="keyword">this</span>.trainMatrix.row(userIndex);</span><br><span class="line">        <span class="keyword">this</span>.factorVector.assign((index, value) -&gt; &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0.0D</span>;</span><br><span class="line">        &#125;);</span><br><span class="line">        Iterator var4 = userVector.iterator();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(var4.hasNext()) &#123;</span><br><span class="line">            VectorEntry vectorEntry = (VectorEntry)var4.next();</span><br><span class="line">            <span class="keyword">this</span>.factorVector.assign((index, value) -&gt; &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">this</span>.impItemFactors.row(vectorEntry.index()).get(index) + value;</span><br><span class="line">            &#125;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">double</span> scale = Math.sqrt((<span class="keyword">double</span>)userVector.getNumEntries());</span><br><span class="line">        <span class="keyword">if</span> (scale &gt; <span class="number">0.0D</span>) &#123;</span><br><span class="line">            <span class="keyword">this</span>.factorVector.assign((index, value) -&gt; &#123;</span><br><span class="line">                <span class="keyword">return</span> value / scale;</span><br><span class="line">            &#125;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.predict(userIndex, itemIndex, <span class="keyword">this</span>.factorVector);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h1 id="预测公式">预测公式</h1>
<p><span class="math inline">\(\hat{r}_{u i}=\mu+b_{i}+b_{u}+q_{i}^{T}\left(p_{u}+|N(u)|^{-\frac{1}{2}} \sum_{j \in N(u)} y_{j}\right)\)</span></p>
<p>首先我们需要明确预测公式各部分的含义：</p>
<ul>
<li><p><span class="math inline">\(\mu\)</span>：训练集中所有评分的均值，表示训练数据的总体评分情况，对于固定的数据集是一个常数，在代码中使用<code>this.globalMean</code>表示。</p></li>
<li><p><span class="math inline">\(b_{u}\)</span>：用户偏置，独立于物品特征的因素，表示某一特定用户的打分习惯，例如，对于批判型用户倾向于打低分，乐观型用户倾向于打高分，在代码中使用<code>this.userBiases.get(userIndex)</code>表示，需要注意的是，特定用户的用户偏置需要使用用户ID来进行索引。</p></li>
<li><p><span class="math inline">\(b_{i}\)</span>：物品偏置，独立于用户兴趣的因素，表示某一特定物品得到的打分情况，例如，好的电影获得的总体评分偏高，而坏的电影获得的评分偏低，在代码中使用<code>this.itemBiases.get(itemIndex)</code>表示，需要注意的是，特定物品的物品偏置需要使用物品ID来进行索引。</p></li>
<li><p><span class="math inline">\(q_{i}\)</span>：物品隐向量，对用户物品评分矩阵分解后得到，在代码中使用<code>itemFactorVector.get(index)</code>表示，同样地，需要使用<code>index</code>来进行索引。</p></li>
<li><p><span class="math inline">\(p_{u}\)</span>：用户隐向量，对用户物品评分矩阵分解后得到，在代码中使用<code>userFactorVector.get(index)</code>表示，同样地，需要使用<code>index</code>来进行索引。</p></li>
<li><p><span class="math inline">\(|N(u)|\)</span>：用户<span class="math inline">\(u\)</span>在评分过的历史物品的数量，即在隐式反馈特征中有行为的个数。</p></li>
<li><p><span class="math inline">\(y_{j}\)</span>：用户<span class="math inline">\(u\)</span>对于交互过的物品<span class="math inline">\(j\)</span>的个人喜好偏置，在代码中，<span class="math inline">\(|N(u)|^{-\frac{1}{2}} \sum_{j \in N(u)} y_{j}\)</span>整体使用<code>factorVector.get(index)</code>表示。</p></li>
</ul>
<p>SVD++中加入了隐式反馈信息，除了假设评分矩阵中的物品有一个隐因子向量外，用户有过交互行为的物品集合也都有一个隐因子向量，维度相同，将用户操作过的物品隐因子向量加起来可以用来表示用户的兴趣爱好。</p>
<p>以下我们可以来看一下预测函数的代码，首先展示<span class="math inline">\(\sum_{j \in N(u)} y_{j}\)</span>这一部分：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">SequentialSparseVector userVector = <span class="keyword">this</span>.trainMatrix.row(userIndex);</span><br><span class="line"><span class="keyword">this</span>.factorVector.assign((index, value) -&gt; &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.0D</span>;</span><br><span class="line">&#125;);</span><br><span class="line">Iterator var4 = userVector.iterator();</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span>(var4.hasNext()) &#123;</span><br><span class="line">    VectorEntry vectorEntry = (VectorEntry)var4.next();</span><br><span class="line">    <span class="keyword">this</span>.factorVector.assign((index, value) -&gt; &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.impItemFactors.row(vectorEntry.index()).get(index) + value;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这块的代码非常的简洁，但不是很好理解，这个Lambda表达式一上来就有一种云里雾里的感觉，我们首先需要明确Lambda表达式的作用，有一个例子可以帮助我们理解，</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">(x, y) -&gt; x - y;</span><br></pre></td></tr></table></figure>
<p>在以上的匿名方法中，接收了<code>x</code>和<code>y</code>两个参数，并返回它们的差值。与以上案例不同的是，程序中的Lambda表达式重写了接口类的方法，我们可以首先来观察一下接口类<code>VectorAssigner</code>中被重写的<code>getValue()</code>方法，代码如下所示，</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">VectorAssigner</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">double</span> <span class="title">getValue</span><span class="params">(<span class="keyword">int</span> var1, <span class="keyword">double</span> var2)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以看到，接口类的方法传入两个参数，一个是<code>int</code>类型，另外一个是<code>double</code>类型，这和Lambda表达式中传入的<code>index</code>和<code>value</code>正好对应。明确了以上之后，我们还需要搞清楚<code>assign()</code>方法的使用，在<code>DenseVector</code>中<code>assign()</code>方法如下所示，</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> DenseVector <span class="title">assign</span><span class="params">(VectorAssigner vectorAssigner)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> index = <span class="number">0</span>; index &lt; <span class="keyword">this</span>.cardinality(); ++index) &#123;</span><br><span class="line">        <span class="keyword">this</span>.set(index, vectorAssigner.getValue(index, <span class="keyword">this</span>.get(index)));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在<code>assign()</code>方法中循环调用了<code>set()</code>方法对<code>factorVector</code>进行赋值，在初始的时候对其进行全0赋值，因为重写的<code>getValue()</code>方法<code>return 0</code>。</p>
<p>再来看以下一部分代码对<span class="math inline">\(\sum_{j \in N(u)} y_{j}\)</span>进行计算，代码如下所示，</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span>(var4.hasNext()) &#123;</span><br><span class="line">    VectorEntry vectorEntry = (VectorEntry)var4.next();</span><br><span class="line">    <span class="keyword">this</span>.factorVector.assign((index, value) -&gt; &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.impItemFactors.row(vectorEntry.index()).get(index) + value;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>首先我们需要明确迭代器是<code>userVector</code>的迭代器，也就是对用户交互过的每一个物品进行遍历，每一次调用<code>assign()</code>方法都遍历所有的<code>index</code>，这里的<code>index</code>可以理解为隐因子向量的维度索引，每一个用户交互过的物品都对应一个隐因子向量，迭代器将所有用户交互过的物品的隐因子向量进行相加，求得<span class="math inline">\(\sum_{j \in N(u)} y_{j}\)</span>。</p>
<p>与公式进行比对，我们发现还缺少一部分<span class="math inline">\(|N(u)|^{-\frac{1}{2}}\)</span>，公式的这一部分是为了消除不同<span class="math inline">\(|N(u)|\)</span>个数引起的差异，如果使用<span class="math inline">\(|N(u)|\)</span>，则用户交互过的物品个数也会对最后的效果产生影响，而这显然是不利的，我们再来看这一部分的代码实现，</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">double</span> scale = Math.sqrt((<span class="keyword">double</span>)userVector.getNumEntries());</span><br><span class="line">    <span class="keyword">if</span> (scale &gt; <span class="number">0.0D</span>) &#123;</span><br><span class="line">        <span class="keyword">this</span>.factorVector.assign((index, value) -&gt; &#123;</span><br><span class="line">            <span class="keyword">return</span> value / scale;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以直接通过调用<code>userVector.getNumEntries()</code>方法来获得当前用户交互过的物品数量，开平方后得到缩放比例，再将隐因子向量的和除以缩放比例最终得到<span class="math inline">\(|N(u)|^{-\frac{1}{2}} \sum_{j \in N(u)} y_{j}\)</span>这一部分的值。</p>
<p>预测函数到这里并没有结束，程序中将预测函数的上一部分进行讲解的代码进行了封装，然后在总的预测函数中进行调用，</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">double</span> <span class="title">predict</span><span class="params">(<span class="keyword">int</span> userIndex, <span class="keyword">int</span> itemIndex, DenseVector factorVector)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">double</span> value = <span class="keyword">this</span>.userBiases.get(userIndex) + <span class="keyword">this</span>.itemBiases.get(itemIndex) + <span class="keyword">this</span>.globalMean;</span><br><span class="line">    DenseVector userFactorVector = <span class="keyword">this</span>.userFactors.row(userIndex);</span><br><span class="line">    DenseVector itemFactorVector = <span class="keyword">this</span>.itemFactors.row(itemIndex);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> index = <span class="number">0</span>; index &lt; <span class="keyword">this</span>.numFactors; ++index) &#123;</span><br><span class="line">        value += (factorVector.get(index) + userFactorVector.get(index)) * itemFactorVector.get(index);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> value;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>value</code>在初始化时就被赋上<span class="math inline">\(\mu+b_{i}+b_{u}\)</span>的值，在这里主要是关注<code>index</code>的循环，<code>index</code>在这里指的是隐因子向量的维度索引，<code>factorVector</code>表示当前用户交互过的物品隐因子偏置向量，能够表现出用户对于交互过的一些物品的隐因子的倾向，<code>userFactorVector</code>表示用户隐因子向量，<code>itemFactorVector</code>表示物品隐因子向量，这些隐因子向量的维度是相同的，所以可以进行相同的循环操作，最后返回的值即预测值，至此预测函数的部分就结束了。</p>
<h1 id="损失函数公式">损失函数公式</h1>
<p><span class="math inline">\(\begin{aligned} \min _{b_{i}, b_{w}, q_{i}, p_{u}} &amp;\sum_{(u, i) \in K}\left(r_{u i}-\mu-b_{u}-b_{i}-q_{i}^{T}\left(p_{u}+|N(u)|^{-\frac{1}{2}} \sum_{j \in N(u)} y_{j}\right)\right)^{2} \\&amp;+\lambda\left\{\sum _ { u } \left(b_{u}^{2}\right.\right. \left.\left.+\left\|p_{u}\right\|^{2}\right)+\sum_{i}\left(b_{i}^{2}+\left\|q_{i}\right\|^{2}+\left\|y_{i}\right\|^{2}\right)\right\} \end{aligned}\)</span></p>
<p>我们首先来计算<span class="math inline">\(\sum_{(u, i) \in K}\left(r_{u i}-\mu-b_{u}-b_{i}-q_{i}^{T}\left(p_{u}+|N(u)|^{-\frac{1}{2}} \sum_{j \in N(u)} y_{j}\right)\right)^{2}\)</span>这一部分的值，虽然看起来繁杂，但可以简化为<span class="math inline">\((r_{ui}-\hat{r}_{ui})^2\)</span>，代码如下所示，</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">VectorEntry vectorEntry = (VectorEntry)var7.next();</span><br><span class="line"><span class="keyword">double</span> error = vectorEntry.get() - <span class="keyword">this</span>.predict(userIndex, itemIndex, <span class="keyword">this</span>.factorVector);</span><br><span class="line"><span class="keyword">this</span>.loss += error * error;</span><br></pre></td></tr></table></figure>
<p>通过<code>vectorEntry.get()</code>方法可以获得实际的真实评分值，调用<code>predict()</code>方法可以获得预测评分值，二者相减得到误差，再进行平方即可求得这一部分的值。需要注意的是，在调用<code>predict()</code>方法时需要首先计算用户交互过物品的隐因子向量<code>factorVector</code>，求法与<code>predict()</code>方法中的求法相同，代码如下所示，</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">this</span>.factorVector.assign((indexx, value) -&gt; &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.0D</span>;</span><br><span class="line">&#125;);</span><br><span class="line">Iterator var5 = userVector.iterator();</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span>(var5.hasNext()) &#123;</span><br><span class="line">    VectorEntry vectorEntry = (VectorEntry)var5.next();</span><br><span class="line">    <span class="keyword">this</span>.factorVector.assign((indexx, value) -&gt; &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.impItemFactors.row(vectorEntry.index()).get(indexx) + value;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">double</span> scale = Math.pow((<span class="keyword">double</span>)userVector.getNumEntries(), -<span class="number">0.5D</span>);</span><br><span class="line"><span class="keyword">this</span>.factorVector.assign((indexx, value) -&gt; &#123;</span><br><span class="line">    <span class="keyword">return</span> value * scale;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>
<p>再接下来我们对公式的<span class="math inline">\(\lambda({\sum _ { u } b_{u}^{2} + \sum _ { i } b_{i}^{2}})\)</span>进行求解，代码如下所示，其中<code>factor</code>指的是用户<code>u</code>的用户偏置，<code>itemBias</code>指的是物品<code>i</code>的物品偏置。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">factor = <span class="keyword">this</span>.userBiases.get(userIndex);</span><br><span class="line"><span class="keyword">this</span>.loss += <span class="keyword">this</span>.regBias * factor * factor;</span><br><span class="line"><span class="keyword">double</span> itemBias = <span class="keyword">this</span>.itemBiases.get(itemIndex);</span><br><span class="line"><span class="keyword">this</span>.loss += <span class="keyword">this</span>.regBias * itemBias * itemBias;</span><br></pre></td></tr></table></figure>
<p>然后我们对公式的<span class="math inline">\(\lambda({\sum _ { u } ||p_{u}||^{2} + \sum _ { i } ||q_{i}||^{2}})\)</span>进行求解，首先进入隐因子向量维度索引的循环，根据特定用户的用户索引<code>userIndex</code>和隐因子向量维度索引<code>factorIndex</code>得到用户隐因子值<code>userFactor</code>，根据特定物品的物品索引<code>itemIndex</code>和隐因子向量维度索引<code>factorIndex</code>得到物品隐因子值<code>itemFactor</code>，代码如下所示，</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> factorIndex = <span class="number">0</span>; factorIndex &lt; <span class="keyword">this</span>.numFactors; ++factorIndex) &#123;</span><br><span class="line">    <span class="keyword">double</span> userFactor = <span class="keyword">this</span>.userFactors.get(userIndex, factorIndex);</span><br><span class="line">    <span class="keyword">double</span> itemFactor = <span class="keyword">this</span>.itemFactors.get(itemIndex, factorIndex);</span><br><span class="line">    <span class="keyword">this</span>.loss += (<span class="keyword">double</span>)<span class="keyword">this</span>.regUser * userFactor * userFactor + (<span class="keyword">double</span>)<span class="keyword">this</span>.regItem * itemFactor * itemFactor;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>最后我们对公式的<span class="math inline">\(\lambda \sum _ i ||y_{i}||^2\)</span>进行求解，这里可以借鉴预测函数中<span class="math inline">\(y_{i}\)</span>的求和，不过损失函数中是对其先平方之后再进行求和，代码如下，</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span>(var23.hasNext()) &#123;</span><br><span class="line">    VectorEntry vectorEntry = (VectorEntry)var23.next();</span><br><span class="line">    <span class="keyword">int</span> index = vectorEntry.index();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> factorIndex = <span class="number">0</span>; factorIndex &lt; <span class="keyword">this</span>.numFactors; ++factorIndex) &#123;</span><br><span class="line">        factor = <span class="keyword">this</span>.impItemFactors.get(index, factorIndex);</span><br><span class="line">        <span class="keyword">this</span>.loss += <span class="keyword">this</span>.regImpItem * factor * factor * (<span class="keyword">double</span>)size;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="更新公式">更新公式</h1>
<p>最后我们关注一下更新函数的部分：</p>
<p><span class="math inline">\(\begin{array}{c} e_{u i}=r_{u i}-\hat{r}_{u i} \\ b_{u} \leftarrow b_{u}+\gamma \cdot\left(e_{u i}-\lambda \cdot b_{u}\right) \\ b_{i} \leftarrow b_{i}+\gamma \cdot\left(e_{u i}-\lambda \cdot b_{i}\right) \\ p_{u} \leftarrow p_{u}+\gamma \cdot\left(e_{u i} \cdot q_{i}-\lambda \cdot p_{u}\right) \\ q_{i} \leftarrow q_{i}+\gamma \cdot\left(e_{u i} \cdot\left(p_{u}+\frac{1}{\sqrt{\left\|R_{u}\right\|}} \sum_{j \in R_{u}} y_{j}\right)-\lambda \cdot q_{i}\right) \\ y_{j} \leftarrow y_{j}+\gamma \cdot\left(e_{u i} \cdot \frac{1}{\sqrt{\left\|R_{u}\right\|}} \cdot q_{i}-\lambda \cdot q_{u}\right) \end{array}\)</span></p>
<ul>
<li><span class="math inline">\(b_{u}\)</span>的更新：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">this</span>.userBiases.plus(userIndex, (<span class="keyword">double</span>)<span class="keyword">this</span>.learnRate * (error - <span class="keyword">this</span>.regBias * factor));</span><br></pre></td></tr></table></figure>
<ul>
<li><span class="math inline">\(b_{i}\)</span>的更新：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">this</span>.itemBiases.plus(itemIndex, (<span class="keyword">double</span>)<span class="keyword">this</span>.learnRate * (error - <span class="keyword">this</span>.regBias * itemBias));</span><br></pre></td></tr></table></figure>
<ul>
<li><span class="math inline">\(p_{u}\)</span>和<span class="math inline">\(q_{i}\)</span>的更新：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> factorIndex = <span class="number">0</span>; factorIndex &lt; <span class="keyword">this</span>.numFactors; ++factorIndex) &#123;</span><br><span class="line">    <span class="keyword">double</span> userFactor = <span class="keyword">this</span>.userFactors.get(userIndex, factorIndex);</span><br><span class="line">    <span class="keyword">double</span> itemFactor = <span class="keyword">this</span>.itemFactors.get(itemIndex, factorIndex);</span><br><span class="line">    <span class="keyword">this</span>.userFactors.plus(userIndex, factorIndex, (<span class="keyword">double</span>)<span class="keyword">this</span>.learnRate * (error * itemFactor - (<span class="keyword">double</span>)<span class="keyword">this</span>.regUser * userFactor));</span><br><span class="line">    <span class="keyword">this</span>.itemFactors.plus(itemIndex, factorIndex, (<span class="keyword">double</span>)<span class="keyword">this</span>.learnRate * (error * (userFactor + <span class="keyword">this</span>.factorVector.get(factorIndex)) - (<span class="keyword">double</span>)<span class="keyword">this</span>.regItem * itemFactor));</span><br><span class="line">    steps[factorIndex] += error * itemFactor * scale;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>需要注意的是，这里记录了<code>step</code>数组，其中记录的是<span class="math inline">\(e_{u i} \cdot \frac{1}{\sqrt{\left|R_{u}\right|}} \cdot q_{i}\)</span>的值（为了<span class="math inline">\(y_{j}\)</span>的更新）。</p>
<ul>
<li><span class="math inline">\(y_{j}\)</span>的更新：</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Iterator var23 = userVector.iterator();</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span>(var23.hasNext()) &#123;</span><br><span class="line">    VectorEntry vectorEntry = (VectorEntry)var23.next();</span><br><span class="line">    <span class="keyword">int</span> index = vectorEntry.index();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> factorIndex = <span class="number">0</span>; factorIndex &lt; <span class="keyword">this</span>.numFactors; ++factorIndex) &#123;</span><br><span class="line">        factor = <span class="keyword">this</span>.impItemFactors.get(index, factorIndex);</span><br><span class="line">        <span class="keyword">this</span>.impItemFactors.plus(index, factorIndex, (<span class="keyword">double</span>)<span class="keyword">this</span>.learnRate * (steps[factorIndex] - <span class="keyword">this</span>.regImpItem * factor * (<span class="keyword">double</span>)size));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里需要区分一下，<code>index</code>指的是用户的索引，<code>factorIndex</code>指的是隐因子向量的索引，<code>factor</code>指的是与当前用户交互过的物品隐因子值（已知隐因子向量索引<code>factorIndex</code>）。</p>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title>CHRF评估指标</title>
    <url>/2022/12/30/CHRF%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>不同于BLEU评估指标，CHRF评估指标可以衡量字符级的准确度和流畅度，改进后的CHRF评估指标（CHRF++）将字符级和单词级融合在一起，更全面的评估文本的生成质量，本文主要对CHRF和CHRF++评估指标的手动计算过程和使用<code>nltk</code>和<code>sacrebleu</code>工具的计算原理作出总结和说明。 <span id="more"></span></p>
<h2 id="chrf">CHRF</h2>
<ol type="1">
<li><p>CHRF指标从字符级别对译文质量进行评估，它考虑了一些形态—句法现象，除此之外，与其他评估指标相比，它很简单，不需要任何额外的工具或知识来源，它完全独立于语言，也独立于分词过程。</p></li>
<li><p>CHRF计算公式： <span class="math display">\[
\mathrm{chrF} \beta=\left(1+\beta^{2}\right) \frac{\mathrm{chrP} \cdot \mathrm{chrR}}{\beta^{2} \cdot \mathrm{chrP}+\mathrm{chrR}}
\]</span></p>
<ul>
<li><span class="math inline">\(\mathrm{chrP}\)</span>是精确度，指翻译句子和参考译文句子匹配的字符级<code>n-gram</code>在翻译句子中占的比例</li>
<li><span class="math inline">\(\mathrm{chrR}\)</span>是召回率，指翻译句子和参考译文句子匹配的字符级<code>n-gram</code>在参考译文句子中占的比例</li>
<li><span class="math inline">\(\beta\)</span>可以控制召回率和精确度两个指标的重要性（召回率比准确率重要<span class="math inline">\(\beta\)</span>倍），当<span class="math inline">\(\beta=1\)</span>时二者同样重要</li>
</ul></li>
<li><p>使用<code>nltk</code>计算<code>CHRF</code></p>
<ul>
<li><p>当<code>n-gram</code>词组长度为1时（词组的最小长度为1，最大长度也为1，<span class="math inline">\(\beta=3\)</span>）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.translate.chrf_score <span class="keyword">import</span> sentence_chrf</span><br><span class="line"></span><br><span class="line">ref = <span class="string">&#x27;the cat is on the mat&#x27;</span>.split()</span><br><span class="line">hyp = <span class="string">&#x27;the the the the the the the&#x27;</span>.split()</span><br><span class="line">sentence_chrf(ref, hyp, min_len=<span class="number">1</span>, max_len=<span class="number">1</span>, beta=<span class="number">3.0</span>)</span><br><span class="line"><span class="comment"># 0.48484848484848486</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>计算TP：重复出现的<code>1-gram</code>有<code>&#123;t, h, e&#125;</code>，总共有8次</p></li>
<li><p>计算TP + FP：翻译句子的长度为21</p></li>
<li><p>计算TP + TN：参考译文句子的长度为16</p></li>
<li><p><span class="math inline">\(\mathrm{chrP}=8/21,\mathrm{chrR}=8/16\)</span> <span class="math display">\[
\mathrm{chrF}=(1 + 3^2)\frac{\frac{8}{21}*\frac{1}{2}}{3^2*\frac{8}{21}+\frac{1}{2}}=\frac{16}{33}=0.48484848484848485
\]</span></p></li>
</ul></li>
<li><p>拓展到<code>max_len=2</code>的情况，此时<code>n-gram</code>词组的最小长度为1，最大长度为2，<span class="math inline">\(\beta=3\)</span></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.translate.chrf_score <span class="keyword">import</span> sentence_chrf</span><br><span class="line">ref = <span class="string">&#x27;the cat is on the mat&#x27;</span>.split()</span><br><span class="line">hyp = <span class="string">&#x27;the the the the the the the&#x27;</span>.split()</span><br><span class="line"><span class="built_in">print</span>(sentence_chrf(ref, hyp, min_len=<span class="number">1</span>, max_len=<span class="number">2</span>, beta=<span class="number">3.0</span>))</span><br><span class="line"><span class="comment"># 0.37145650048875856</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>计算<code>1-gram</code>的情况：此时<code>F-score=0.48484848484848486</code>（和CHRF计算相同）</p></li>
<li><p>计算<code>2-gram</code>的情况：</p>
<ul>
<li><p>计算TP：重复出现的<code>2-gram</code>有<code>&#123;th, he&#125;</code>，总共有4次</p></li>
<li><p>计算TP + FP： 翻译句子分成<code>2-gram</code>的长度为20</p></li>
<li><p>计算TP + TN：参考译文句子分成<code>2-gram</code>的长度为15</p></li>
<li><p><span class="math inline">\(\mathrm{chrP}=4/20,\mathrm{chrR}=4/15\)</span> <span class="math display">\[
\mathrm{chrF_{2-gram}}=(1 + 3^2)\frac{\frac{1}{5}*\frac{4}{15}}{3^2*\frac{1}{5}+\frac{4}{15}}=\frac{8}{31}=0.25806451612903225
\]</span></p></li>
</ul></li>
<li><p>计算总的CHRF <span class="math display">\[
\mathrm{chrF}=\frac{0.48484848484848486+0.25806451612903225}{2}=0.37145650048875856
\]</span></p></li>
</ul></li>
<li><p>计算语料级的CHRF（以上都是句子级的CHRF）：基本思想是计算出每个句子的CHRF，然后再求算术平均</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ref1 = <span class="built_in">str</span>(<span class="string">&#x27;It is a guide to action that ensures that the military will forever heed Party commands&#x27;</span>).split()</span><br><span class="line">ref2 = <span class="built_in">str</span>(<span class="string">&#x27;It is the guiding principle which guarantees the military forces always being under the command of the Party&#x27;</span>).split()</span><br><span class="line">hyp1 = <span class="built_in">str</span>(<span class="string">&#x27;It is a guide to action which ensures that the military always obeys the commands of the party&#x27;</span>).split()</span><br><span class="line">hyp2 = <span class="built_in">str</span>(<span class="string">&#x27;It is to insure the troops forever hearing the activity guidebook that party direct&#x27;</span>).split()</span><br><span class="line">corpus_chrf([ref1, ref2], [hyp1, hyp2]) </span><br><span class="line"><span class="comment"># 0.4166529443281564</span></span><br><span class="line"></span><br><span class="line">(sentence_chrf(ref1, hyp1) + sentence_chrf(ref2, hyp2)) / <span class="number">2</span></span><br><span class="line"><span class="comment"># 0.4166529443281564</span></span><br></pre></td></tr></table></figure></li>
</ul></li>
<li><p>使用<code>sacrebleu</code>计算<code>CHRF</code></p>
<ul>
<li><p>计算句子级CHRF</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(sacrebleu.sentence_chrf(hypothesis=<span class="string">&#x27;the the the the the the the&#x27;</span>,</span><br><span class="line">                            references=[<span class="string">&#x27;the cat is on the mat&#x27;</span>],</span><br><span class="line">                            char_order=<span class="number">1</span>, word_order=<span class="number">0</span>, beta=<span class="number">3</span>, remove_whitespace=<span class="literal">True</span>).score)</span><br><span class="line"><span class="comment"># 48.484848484848484</span></span><br><span class="line"><span class="built_in">print</span>(sacrebleu.sentence_chrf(hypothesis=<span class="string">&#x27;the the the the the the the&#x27;</span>,</span><br><span class="line">                            references=[<span class="string">&#x27;the cat is on the mat&#x27;</span>],</span><br><span class="line">                            char_order=<span class="number">2</span>, word_order=<span class="number">0</span>, beta=<span class="number">3</span>, remove_whitespace=<span class="literal">True</span>).score)</span><br><span class="line"><span class="comment"># 37.145882975906794</span></span><br></pre></td></tr></table></figure></li>
<li><p>计算语料级CHRF</p>
<ul>
<li>与<code>nltk</code>工具提供的计算方法不同，<code>sacrebleu</code>并不是计算出每个句子的CHRF，再求算术平均</li>
<li><code>sacrebleu</code>在计算<code>i-gram</code>的准确率和召回率时，将语料中的参考句子<code>i-gram</code>长度、翻译句子<code>i-gram</code>长度、参考句子和翻译句子匹配<code>i-gram</code>数量分别进行相加，即分数中的分子和分子进行相加，分母和分母进行相加，与<code>nltk</code>中的分数直接进行相加不同，这与<code>sacrebleu</code>中求BLEU的方法有异曲同工之妙</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ref1 = <span class="string">&#x27;It is a guide to action that ensures that the military will forever heed Party commands&#x27;</span></span><br><span class="line">ref2 = <span class="string">&#x27;It is the guiding principle which guarantees the military forces always being under the command of the Party&#x27;</span></span><br><span class="line">hyp1 = <span class="string">&#x27;It is a guide to action which ensures that the military always obeys the commands of the party&#x27;</span></span><br><span class="line">hyp2 = <span class="string">&#x27;It is to insure the troops forever hearing the activity guidebook that party direct&#x27;</span></span><br><span class="line"><span class="built_in">print</span>(sacrebleu.corpus_chrf(hypotheses=[hyp1, hyp2], references=[[ref1, ref2]], char_order=<span class="number">6</span>, word_order=<span class="number">0</span>, beta=<span class="number">3</span>).score)</span><br><span class="line"><span class="comment"># 39.364938843711016</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>将以上的代码作为示例，<code>sacrebleu</code>首先计算出每个句子各<code>n-gram</code>模型中的参考句子<code>n-gram</code>长度、翻译句子<code>n-gram</code>长度、参考句子和翻译句子匹配<code>n-gram</code>数量，如下所示</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[[77, 72, 65, 76, 71, 50, 75, 70, 44, 74, 69, 40, 73, 68, 36, 72, 67, 33], [70, 91, 60, 69, 90, 28, 68, 89, 12, 67, 88, 4, 66, 87, 1, 65, 86, 0]]</span><br></pre></td></tr></table></figure></li>
<li><p>列表中的第一项代表第一个句子，列表中的第二项代表第二个句子，以第一个句子为例，列表项中共有18个元素，分别是翻译句子<code>1-gram</code>长度（翻译句子长度）、参考句子<code>1-gram</code>长度（参考句子长度）、翻译句子和参考句子匹配的<code>1-gram</code>数量...以此类推，一直到<code>6-gram</code></p></li>
<li><p>将列表中的对应项进行相加，得出以下结果（相当于分子和分子相加，分母和分母相加）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[147, 163, 125, 145, 161, 78, 143, 159, 56, 141, 157, 44, 139, 155, 37, 137, 153, 33]</span><br></pre></td></tr></table></figure></li>
<li><p>然后求各<code>n-gram</code>的准确率和召回率，将准确率和召回率求算术平均（总和除以6），再用平均后的准确率和召回率求最终的CHRF，基础逻辑如下所示（仿照<code>sacrebleu</code>手搓的，可能有一些特殊情况不适用，比如分母不能为0）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data_list = [<span class="number">147</span>, <span class="number">163</span>, <span class="number">125</span>, <span class="number">145</span>, <span class="number">161</span>, <span class="number">78</span>, <span class="number">143</span>, <span class="number">159</span>, <span class="number">56</span>, <span class="number">141</span>, <span class="number">157</span>, <span class="number">44</span>, <span class="number">139</span>, <span class="number">155</span>, <span class="number">37</span>, <span class="number">137</span>, <span class="number">153</span>, <span class="number">33</span>]</span><br><span class="line">sum_prec, sum_rec = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">6</span>):</span><br><span class="line">    index = <span class="number">3</span> * i</span><br><span class="line">    n_hyp = data_list[index]</span><br><span class="line">    n_ref = data_list[index + <span class="number">1</span>]</span><br><span class="line">    n_match = data_list[index + <span class="number">2</span>]</span><br><span class="line">    </span><br><span class="line">    n_prec = n_match / n_hyp</span><br><span class="line">    n_rec = n_match / n_ref</span><br><span class="line">    </span><br><span class="line">    sum_prec += n_prec</span><br><span class="line">    sum_rec += n_rec</span><br><span class="line">    </span><br><span class="line">    n_fscore = (<span class="number">1</span> + <span class="number">9</span>) * n_prec * n_rec / (<span class="number">9</span> * n_prec + n_rec)</span><br><span class="line">    sum_fscore = sum_fscore + n_fscore</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>((<span class="number">1</span> + <span class="number">9</span>) * (sum_prec / <span class="number">6</span>) * (sum_rec / <span class="number">6</span>) / (<span class="number">9</span> * (sum_prec / <span class="number">6</span>) + (sum_rec / <span class="number">6</span>)))</span><br><span class="line"><span class="comment"># 0.39364938843711017</span></span><br></pre></td></tr></table></figure></li>
</ul></li>
</ul></li>
</ol>
<h2 id="chrf-1">CHRF++</h2>
<ol type="1">
<li><p>之前的工作中显示，对于评分较差的句子，CHRF和WORDF分数的标准差是相似的——两个指标都分配了相对相似的（低）分数，但对于人类评分较高的句子，CHRF的偏差相较于WORDF的偏差要低得多，此外，人类评分越高，WORDF与CHRF的偏差的差异越大，这些结果表明，CHRF是优于WORDF的，尤其是在翻译质量较高的片段上</p></li>
<li><p>但是考虑到CHRF的结果可能过于乐观，所以将CHRF和WORDF结合起来，得到CHRF++</p></li>
<li><p>当单词<code>n-grams</code>与字符<code>n-grams</code>相加并取平均值时，就会得到CHRF++分数，<strong>这种组合的最佳<code>n-gram</code>长度对于字符<code>n-gram</code>来说是<code>n=6</code></strong>，与CHRF中字符<code>n-gram</code>的最佳长度相同，<strong>对于单词<code>n-gram</code>来说是<code>n=1</code>或<code>n=2</code></strong></p></li>
<li><p>使用<code>sacrebleu</code>计算CHRF++</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(sacrebleu.sentence_chrf(hypothesis=<span class="string">&#x27;the the the the the the the&#x27;</span>,</span><br><span class="line">                                  references=[<span class="string">&#x27;the cat is on the mat&#x27;</span>],</span><br><span class="line">                                  char_order=<span class="number">1</span>, word_order=<span class="number">1</span>, beta=<span class="number">3</span>, remove_whitespace=<span class="literal">True</span>).score)</span><br><span class="line"><span class="comment"># 40.65040650406503</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>为了方便计算，这里字符和单词都选择<code>1-gram</code>，首先可以得到以下统计结果</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[21, 16, 8, 7, 6, 2]</span><br></pre></td></tr></table></figure></li>
<li><p>前三个数是字符级<code>1-gram</code>的统计结果，后三个数是单词级<code>1-gram</code>的统计结果（分别是翻译句子字符或单词<code>1-gram</code>长度、参考句子字符或单词<code>1-gram</code>长度、匹配的<code>1-gram</code>数量）</p></li>
<li><p>分别计算准确率和召回率并求平均值（<strong>实际上<code>sacrebleu</code>中求平均值是除以<code>self.order</code>实现的，此时<code>self.order</code>等于列表长度除以3，本例中为<span class="math inline">\(6/3=2\)</span></strong>），得到最后的准确率和召回率，再计算CHRF++</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">prec = <span class="number">0.6666666666666666</span> / <span class="number">2</span></span><br><span class="line">rec = <span class="number">0.8333333333333333</span> / <span class="number">2</span></span><br><span class="line">(<span class="number">1</span> + <span class="number">9</span>) * prec * rec / (<span class="number">9</span> * prec + rec)</span><br><span class="line"><span class="comment"># 0.40650406504065034</span></span><br></pre></td></tr></table></figure></li>
</ul></li>
</ol>
<p><strong>参考文献：</strong></p>
<ol type="1">
<li><a href="https://www.nltk.org/_modules/nltk/translate/chrf_score.html">NLTK :: nltk.translate.chrf_score</a></li>
<li><a href="https://blog.csdn.net/weixin_43647120/article/details/114396261">NLG评估指标chrF、chrF++介绍_地大陈参志的博客-CSDN博客</a></li>
<li><a href="https://github.com/mjpost/sacrebleu/blob/master/sacrebleu/metrics/chrf.py">sacrebleu/chrf.py at master · mjpost/sacrebleu (github.com)</a></li>
<li><a href="https://aclanthology.org/W15-3049/">chrF: character n-gram F-score for automatic MT evaluation - ACL Anthology</a></li>
<li><a href="https://aclanthology.org/W17-4770/">chrF++: words helping character n-grams - ACL Anthology</a></li>
</ol>
]]></content>
      <categories>
        <category>机器翻译</category>
      </categories>
      <tags>
        <tag>机器翻译</tag>
      </tags>
  </entry>
  <entry>
    <title>SOHO论文阅读</title>
    <url>/2022/03/09/SOHO%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>上一篇ViLBERT模型提出了使用two-stream的结构来分别处理图片和文本信息，然后再进行融合，在“vision-and-language”任务上还有其他模型使用了这种双流结构，因为其架构比单流结构更为复杂，所以架构的种类更加丰富，为了更深入地了解这种处理不同层次信息（图片和文本）的架构方式，我阅读了Seeing Out of tHe bOx:End-to-End Pre-training for Vision-Language Representation Learning这篇论文。特别地，这篇论文提出的视觉特征的提取方法也有其独到之处，能够不受目标特征局限性的影响。</p>
<center>
<img src="/2022/03/09/SOHO%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/001.png" width="600px">
</center>
<span id="more"></span>
<p>论文地址和相关源码链接如下：</p>
<p><a href="https://arxiv.org/abs/2104.03135" target="_blank">论文地址：https://arxiv.org/abs/2104.03135</a> <br> <a href="https://github.com/researchmm/soho" target="_blank">相关源码：https://github.com/researchmm/soho</a></p>
<h3 id="摘要">摘要</h3>
<p>文章研究了用于视觉语言预训练的卷积神经网络和transformer的联合学习，旨在从数百万的图像-文本对中学习跨模态对齐，截止文章发表，最先进的方法是提取突出的图像区域并逐步将区域与文字对齐，但是由于提取出的图像区域仅仅代表图像的一部分，所以理解对应的自然语言是有难度的。文章提出了SOHO模型，将整个图像作为输入，并以端到端方式学习视觉语言的联合表示，SOHO模型不需要边界框标注，这使得其推理的速度比基于区域的方法块10倍。特别地，SOHO通过视觉字典(VD)学习提取全面而紧凑的图像特征，以促进跨模态的理解。最终，文章将SOHO模型在四个视觉语言任务上进行实验，并得到不错的性能和准确率提升。</p>
<h3 id="介绍">介绍</h3>
<p>最近跨模态学习的研究工作不断增加，特别是在视觉语言预训练(VLPT)领域。显而易见地，视觉表示在VLPT模型中起着重要作用，有一些模型利用了基于区域的图像特征并取得了不错的效果，这些特征是由在视觉基因组数据集上预训练的目标检测器提取出来的，但是这种方法也有一些缺点：</p>
<ol type="1">
<li>区域专注于边界框内的物体，而忽略了边界框外的上下文信息。这些信息对于区域与区域之间的关系理解和推理非常重要。文章以下图为例说明了这个事实，图中可以很容易地检测到男人、女人和船目标，但是如果缺少边框外的上下文信息，就会误以为人们在划船，从而导致模型在下游任务应用时的错误。</li>
</ol>
<center>
<img src="/2022/03/09/SOHO%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/002.png" width="500px">
<p style="font-family:KaiTi">
图1：模型在下游任务上的结果对比
</p>
</center>
<ol start="2" type="1">
<li>对图像的视觉理解被限制在预先定义的区域类别中。</li>
<li>大多数区域特征是由检测模型提取的，存在质量低、噪声大、过采样等问题，并且依赖于大规模的box标注数据。</li>
</ol>
<p>除了区域图像特征提取方法外，一些工作也研究了采用弱监督的目标检测的方法、或是通过基于网格的卷积特征来学习视觉表征的方法，但存在性能不高、只针对单个任务设计等问题。</p>
<p>为了克服基于区域的图像特征的局限性，更好地利用图像文本数据对进行跨模态理解，文章提出了一个端到端的视觉语言预训练框架SOHO，直接学习图像和文本的嵌入及其语义对齐。与现有的VLPT模型相比，SOHO采取了一个简单的管道，不需要复杂的视觉骨干进行预训练。相比于现有的基于区域的图像特征的方法，SOHO不需要标注类别或边界框，可以通过更广泛的图像-文本数据直接优化视觉表示来丰富视觉语义。</p>
<p>文章选择采用像素级的视觉表征，但像素级的视觉表征比语言的嵌入更加多样化和密集化，并且缺乏明确的监督，给对齐学习增加了难度。为了解决上述问题，文章引入了<strong>视觉字典</strong>，它代表了视觉领域中更全面和紧凑的语义。为了学习视觉字典，文中设计了一个移动平均编码器，将具有类似视觉语义的像素进行分组。</p>
<p>在预训练过程中，文章采用了Masked Vision Modeling (MVM) ，Masked Language Modeling（MLM），Image-Text Matching （ITM）三个任务来优化模型。</p>
<p>文章的贡献可以总结如下：</p>
<ol type="1">
<li>文章提出了SOHO模型，这是第一个直接用图像-文本对学习跨模态表示的端到端VLPT模型之一。在不需要提取边界框的情况下，模型可以实现至少10倍的推理速度。</li>
<li>为了更好地对齐图像和文本数据，我们提出了一个新的动态更新的视觉字典，它代表了图像中类似语义的视觉抽象。</li>
<li>对四个下游任务进行实验，得到了性能和准确率提升。</li>
</ol>
<h3 id="相关工作">相关工作</h3>
<h4 id="vision-language中的视觉表示">Vision-Language中的视觉表示</h4>
<p>早期的工作采用CNN分类模型来提取视觉特征，后来，Anderson等人提出了在视觉基因数据集上预训练的BUTD检测模型，以提取突出的区域特征作为视觉输入，最近，一些工作提出在特定的Vision-Language任务上，用卷积神经网络可以直接学习网格特征形式的视觉表征。</p>
<p>VideoBERT和bag of words文献也使用矢量量化来表示视觉信息。VD与相关作品的关键区别在于，我们用可训练的视觉编码器的输出动态更新基于VD的嵌入，而不是预先计算的输入特征（即VD是<strong>动态更新</strong>的）。VD的动态更新机制可以从视觉语言数据集中获取文本指导语义。因此，该模型可以直接用高级语义来优化视觉语言的理解和对齐。</p>
<h4 id="vision-language的预训练">Vision-Language的预训练</h4>
<p>许多视觉语言预训练工作已经被提出用来学习跨模态表征，它们可以被分为single-stream模型和two-stream模型，双流模型分别处理视觉和语言信息，并在之后通过另一个transformer层将它们融合；相反，单流模型使用BERT来学习检测边界框特征（可以理解为ROI特征）和文本嵌入特征的双向联合分布。这两种类型都使用基于Transformer的模型来学习视觉-语言联合嵌入特征。虽然他们忽略了视觉表征学习对视觉-语言任务也很重要。</p>
<h3 id="方法">方法</h3>
<p>SOHO是一个端到端的框架，它由一个基于CNN的可训练的视觉编码器、一个视觉字典嵌入模块和一个多层transformer组成，视觉编码器将图像作为输入并产生视觉特征，视觉字典嵌入模块被设计用来将不同的视觉语义信息聚集到视觉token中，transformer被用来融合视觉和语言模态的特征，并产生特定任务的输出。SOHO可以通过MVM、MLM和ITM任务进行端到端的预训练，并很容易地适应下游任务。SOHO的整体架构如下图所示，</p>
<center>
<img src="/2022/03/09/SOHO%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/003.png" width="700px">
<p style="font-family:KaiTi">
图2：SOHO模型架构
</p>
</center>
<h4 id="可训练的视觉编码器">可训练的视觉编码器</h4>
<p>基于区域的图像特征的表现能力受到预先定义的对象和属性类别的限制，并且不能学习到某些边界框外的上下文信息。为了保留所有视觉信息，文章建议使用一个可训练的CNN视觉编码器，将整个图像作为输入，产生图像级别的视觉特征，而不是区域级别的特征。由于不受边界框的限制，视觉编码器可以从预训练的损失函数或下游任务的损失函数中进行端到端的更新，进而进一步优化跨模态学习。给定一张图片<span class="math inline">\(\mathcal{I}\)</span>，可以根据以下公式得到其特征<span class="math inline">\(\mathcal{V}\)</span>，</p>
<p><span class="math display">\[
\mathcal{V}=E(\mathcal{I}, \theta) \in \mathbf{R}^{l \times c}
\]</span></p>
<p>其中<span class="math inline">\(l\)</span>表示嵌入特征向量的数量，<span class="math inline">\(c\)</span>表示嵌入特征向量维度，另外，将在ImageNet上预训练的ResNet加上<span class="math inline">\(1 \times 1\)</span>的卷积层和<span class="math inline">\(2 \times 2\)</span>最大池化层作为编码器<span class="math inline">\(E\)</span>的结构。</p>
<h4 id="视觉字典">视觉字典</h4>
<p>视觉特征编码器提取的视觉特征<span class="math inline">\(\mathcal{V}\)</span>比语言词标记更加多样和密集，这将给跨模态理解的学习带来困难。为了弥补其与语言token的差距，我们提出了一个视觉字典，通过将相似的视觉语义聚合到同一图像特征中来标记视觉特征。</p>
<h5 id="视觉字典嵌入">视觉字典嵌入</h5>
<p>文章将视觉字典定义为一个矩阵<span class="math inline">\(\mathcal{D} \in \mathbf{R}^{k \times c}\)</span>，包含<span class="math inline">\(k\)</span>个嵌入向量，每个<span class="math inline">\(c\)</span>维，第<span class="math inline">\(j\)</span>个嵌入向量被表示为<span class="math inline">\(d_{j}\)</span>，对于每一个视觉特征<span class="math inline">\(v_{i}\)</span>，文章通过在<span class="math inline">\(\mathcal{D}\)</span>中搜索最近邻来计算其映射索引<span class="math inline">\(h_{i}\)</span>，记为如下公式，</p>
<p><span class="math display">\[
h_{i}=\operatorname{argmin}_{j}\left\|v_{i}-d_{j}\right\|_{2}
\]</span></p>
<p>因此可以定义以下映射函数<span class="math inline">\(f\)</span>，将视觉特征<span class="math inline">\(v_{i}\)</span>映射到视觉字典矩阵<span class="math inline">\(\mathcal{D}\)</span>：</p>
<p><span class="math display">\[
f\left(v_{i}\right)=d_{h_{i}}
\]</span></p>
<p>其使用视觉字典中最近的嵌入向量来表示视觉特征，<span class="math inline">\(f^{-1}(j)\)</span>表示逆映射函数，将索引<span class="math inline">\(j\)</span>映射回一组视觉特征。</p>
<h5 id="视觉字典的学习更新">视觉字典的学习更新</h5>
<p>视觉字典矩阵被随机初始化，并在一个小批次中通过移动平均操作进一步更新，记为如下公式，</p>
<p><span class="math display">\[
\hat{d}_{j}=\gamma * d_{j}+(1-\gamma) * \frac{\sum_{h_{i}=j} v_{i}}{\left|f^{-1}(j)\right|}
\]</span></p>
<p>即保留一部分原有的<span class="math inline">\(d_{j}\)</span>，根据小批次中被归为<span class="math inline">\(h_{j}\)</span>索引的视觉特征进行更新。</p>
<h5 id="梯度反向传播">梯度反向传播</h5>
<p>由于argmin操作时不可微的，梯度反向传播将被视觉字典停止，为了使视觉编码器可训练，采用了以下方法进行更新，</p>
<p><span class="math display">\[
f\left(v_{i}\right)=s g\left[d_{h_{i}}-v_{i}\right]+v_{i}
\]</span></p>
<p>其中<span class="math inline">\(s g[\cdot]\)</span>为停止梯度运算符。</p>
<p>视觉词典根据特征相似度对视觉特征图进行在线聚类，并通过聚类中心表示每个特征向量。具有相似语义的特征向量将被聚集到同一个聚类中，聚类的索引可以被视为一个虚拟的视觉语义标签。<strong>由于聚类可以受到视觉语言学习任务的影响，每个嵌入向量的学习语义更适合于跨模态的理解和对齐。</strong></p>
<p>视觉字典面临着一个冷启动问题，直接将梯度从随机初始化的嵌入向量复制到视觉特征图上会导致不正确的模型优化方向（即模式崩溃）。因此，我们在前10个训练历时中冻结了视觉特征编码器中ResNet的参数。</p>
<h4 id="预训练管道">预训练管道</h4>
<p>文章应用多层transformer来学习融合视觉和语言特征的跨模态表征，为了学习视觉和语言相关任务的通用表征，文章采用自监督的方法，在一个大型的数据集上对模型进行预训练。除了通用的掩蔽语言建模和图像-文本匹配预训练任务外，文章还提出了一个新颖的基于视觉字典产生的虚拟视觉语义标签的掩蔽视觉建模预训练任务。</p>
<h5 id="跨模态的transformer">跨模态的transformer</h5>
<p>对于视觉表示，文章利用正弦函数计算的二维位置嵌入来编码视觉token的空间信息，对于文本内容，按照输入BERT时的嵌入方法对其进行文本进行嵌入，最终将VD嵌入和文本嵌入连接起来形成输入序列，用于跨模态学习，需要特别注意的是，VD嵌入与文本嵌入的向量长度一致。</p>
<h5 id="掩蔽语言建模mlm">掩蔽语言建模(MLM)</h5>
<p>掩蔽语言建模预训练任务鼓励模型构建语言token和可视化内容之间的映射关系，MLM的目标是根据其他词token<span class="math inline">\(\mathcal{W}_{\backslash i}\)</span>和所有的图像特征<span class="math inline">\(f(\mathcal{V})\)</span>，通过最小化负对数似然来预测遮蔽的词token：</p>
<p><span class="math display">\[
\mathcal{L}_{\mathrm{MLM}}=-\mathbb{E}_{(\mathcal{W}, f(\mathcal{V})) \sim D} \log p\left(w_{i} \mid \mathcal{W}_{\backslash i}, f(\mathcal{V})\right)
\]</span></p>
<h5 id="掩蔽视觉建模mvm">掩蔽视觉建模(MVM)</h5>
<p>文章提出了基于视觉字典的掩蔽视觉建模，在将图像特征输入到transformer之前，随机进行遮蔽，MVM的目标是根据周围图像的特征<span class="math inline">\(f(\mathcal{V})_{\backslash j}\)</span>和所有的语言token<span class="math inline">\(W\)</span>，通过最小化负对数似然，预测被遮蔽图像的特征，</p>
<p><span class="math display">\[
\mathcal{L}_{\text {MVM }}=-\mathbb{E}_{(\mathcal{W}, f(\mathcal{V})) \sim D} \log p\left(f\left(v_{j}\right) \mid \mathcal{W}, f(\mathcal{V})_{\backslash j}\right)
\]</span></p>
<p>需要特别注意的是，在视觉特征图中，相邻的特征可能有相似的值，因此共享相同的视觉字典映射索引，这将导致模型以一种懒惰的方式直接复制周围特征的标签作为预测值，为了防止这种情况的出现，在遮蔽阶段，文章首先在视觉字典中随机选择一个标签索引，然后将索引对应的所有视觉特征进行遮蔽。</p>
<h5 id="图像-文本匹配itm">图像-文本匹配(ITM)</h5>
<p>为了增强跨模态匹配，文章采用图像文本匹配任务进行预训练，文章在[CLS]标签上应用二值分类器<span class="math inline">\(\phi(\cdot)\)</span>来预测输入图像和文本是否匹配，ITM任务由以下的损失函数驱动：</p>
<p><span class="math display">\[
\mathcal{L}_{\mathrm{ITM}}=-\mathbb{E}_{(\mathcal{W}, f(\mathcal{V})) \sim D} \log p(y \mid \phi(\mathcal{W}, f(\mathcal{V})))
\]</span></p>
<p>视觉特征编码器、基于VD的图像嵌入模块和跨模态transformer可以进行端到端的联合训练，文章给三个预训练目标分配相同的损失权重，因此，SOHO的预训练目标如下所示，</p>
<p><span class="math display">\[
\mathcal{L}_{\text {Pre-training }}=\mathcal{L}_{\mathrm{MLM}}+\mathcal{L}_{\mathrm{MVM}}+\mathcal{L}_{\mathrm{ITM}}
\]</span></p>
]]></content>
      <categories>
        <category>深度学习论文阅读</category>
      </categories>
      <tags>
        <tag>多模态模型</tag>
      </tags>
  </entry>
  <entry>
    <title>LibRec学习笔记（一）：BiasedMF算法</title>
    <url>/2022/04/04/LibRec%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9ABiasedMF%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>最近推荐系统导论课程需要使用LibRec库实现一些推荐算法，其实LibRec库已经封装了很多算法，并不需要再去实现，只需要调用命令行修改配置就可以运行，但为了更好地理解算法，我阅读了LibRec库的一些算法的源码，以下是BiaseMF算法在LibRec库中的实现，本文主要从三部分展开讲解，分别是预测公式、损失函数公式和更新公式。 <span id="more"></span></p>
<h1 id="代码展示">代码展示</h1>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// SVDPlusPlusRecommender类继承BiasedMFRecommender父类</span></span><br><span class="line"><span class="keyword">package</span> net.librec.recommender.cf.rating;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="keyword">import</span> net.librec.annotation.ModelData;</span><br><span class="line"><span class="keyword">import</span> net.librec.common.LibrecException;</span><br><span class="line"><span class="keyword">import</span> net.librec.math.structure.MatrixEntry;</span><br><span class="line"><span class="keyword">import</span> net.librec.math.structure.VectorBasedDenseVector;</span><br><span class="line"><span class="keyword">import</span> net.librec.recommender.MatrixFactorizationRecommender;</span><br><span class="line"></span><br><span class="line"><span class="meta">@ModelData(&#123;&quot;isRating&quot;, &quot;biasedMF&quot;, &quot;userFactors&quot;, &quot;itemFactors&quot;, &quot;userBiases&quot;, &quot;itemBiases&quot;&#125;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BiasedMFRecommender</span> <span class="keyword">extends</span> <span class="title">MatrixFactorizationRecommender</span> </span>&#123;</span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">double</span> regBias;</span><br><span class="line">    <span class="keyword">protected</span> VectorBasedDenseVector userBiases;</span><br><span class="line">    <span class="keyword">protected</span> VectorBasedDenseVector itemBiases;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">BiasedMFRecommender</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">()</span> <span class="keyword">throws</span> LibrecException </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.setup();</span><br><span class="line">        <span class="keyword">this</span>.regBias = <span class="keyword">this</span>.conf.getDouble(<span class="string">&quot;rec.bias.regularization&quot;</span>, <span class="number">0.01D</span>);</span><br><span class="line">        <span class="comment">// 用户偏差向量</span></span><br><span class="line">        <span class="keyword">this</span>.userBiases = <span class="keyword">new</span> VectorBasedDenseVector(<span class="keyword">this</span>.numUsers);</span><br><span class="line">        <span class="comment">// 物品偏差向量</span></span><br><span class="line">        <span class="keyword">this</span>.itemBiases = <span class="keyword">new</span> VectorBasedDenseVector(<span class="keyword">this</span>.numItems);</span><br><span class="line">        <span class="comment">// 用户偏差初始化</span></span><br><span class="line">        <span class="keyword">this</span>.userBiases.init((<span class="keyword">double</span>)<span class="keyword">this</span>.initMean, (<span class="keyword">double</span>)<span class="keyword">this</span>.initStd);</span><br><span class="line">        <span class="comment">// 物品偏差初始化</span></span><br><span class="line">        <span class="keyword">this</span>.itemBiases.init((<span class="keyword">double</span>)<span class="keyword">this</span>.initMean, (<span class="keyword">double</span>)<span class="keyword">this</span>.initStd);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">trainModel</span><span class="params">()</span> <span class="keyword">throws</span> LibrecException </span>&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> iter = <span class="number">1</span>; iter &lt;= <span class="keyword">this</span>.numIterations; ++iter) &#123;</span><br><span class="line">            <span class="comment">// 损失</span></span><br><span class="line">            <span class="keyword">this</span>.loss = <span class="number">0.0D</span>;</span><br><span class="line"></span><br><span class="line">            Iterator var2 = <span class="keyword">this</span>.trainMatrix.iterator();</span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span>(var2.hasNext()) &#123;</span><br><span class="line">                MatrixEntry matrixEntry = (MatrixEntry)var2.next();</span><br><span class="line">                <span class="keyword">int</span> userIdx = matrixEntry.row();</span><br><span class="line">                <span class="keyword">int</span> itemIdx = matrixEntry.column();</span><br><span class="line">                <span class="comment">// 实际评分</span></span><br><span class="line">                <span class="keyword">double</span> realRating = matrixEntry.get();</span><br><span class="line">                <span class="comment">// 预测评分</span></span><br><span class="line">                <span class="keyword">double</span> predictRating = <span class="keyword">this</span>.predict(userIdx, itemIdx);</span><br><span class="line">                <span class="comment">// 实际评分与预测评分的误差</span></span><br><span class="line">                <span class="keyword">double</span> error = realRating - predictRating;</span><br><span class="line">                <span class="keyword">this</span>.loss += error * error;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 根据当前用户ID得到当前用户偏差向量</span></span><br><span class="line">                <span class="keyword">double</span> userBiasValue = <span class="keyword">this</span>.userBiases.get(userIdx);</span><br><span class="line">                <span class="comment">// 用户偏置项的更新公式</span></span><br><span class="line">                <span class="keyword">this</span>.userBiases.plus(userIdx, (<span class="keyword">double</span>)<span class="keyword">this</span>.learnRate * (error - <span class="keyword">this</span>.regBias * userBiasValue));</span><br><span class="line"></span><br><span class="line">                <span class="keyword">this</span>.loss += <span class="keyword">this</span>.regBias * userBiasValue * userBiasValue;</span><br><span class="line">                <span class="comment">// 根据当前物品ID得到当前物品偏差向量</span></span><br><span class="line">                <span class="keyword">double</span> itemBiasValue = <span class="keyword">this</span>.itemBiases.get(itemIdx);</span><br><span class="line">                <span class="comment">// 物品偏置项的更新公式</span></span><br><span class="line">                <span class="keyword">this</span>.itemBiases.plus(itemIdx, (<span class="keyword">double</span>)<span class="keyword">this</span>.learnRate * (error - <span class="keyword">this</span>.regBias * itemBiasValue));</span><br><span class="line"></span><br><span class="line">                <span class="keyword">this</span>.loss += <span class="keyword">this</span>.regBias * itemBiasValue * itemBiasValue;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span>(<span class="keyword">int</span> factorIdx = <span class="number">0</span>; factorIdx &lt; <span class="keyword">this</span>.numFactors; ++factorIdx) &#123;</span><br><span class="line">                    <span class="keyword">double</span> userFactorValue = <span class="keyword">this</span>.userFactors.get(userIdx, factorIdx);</span><br><span class="line">                    <span class="keyword">double</span> itemFactorValue = <span class="keyword">this</span>.itemFactors.get(itemIdx, factorIdx);</span><br><span class="line">                    <span class="keyword">this</span>.userFactors.plus(userIdx, factorIdx, (<span class="keyword">double</span>)<span class="keyword">this</span>.learnRate * (error * itemFactorValue - (<span class="keyword">double</span>)<span class="keyword">this</span>.regUser * userFactorValue));</span><br><span class="line">                    <span class="keyword">this</span>.itemFactors.plus(itemIdx, factorIdx, (<span class="keyword">double</span>)<span class="keyword">this</span>.learnRate * (error * userFactorValue - (<span class="keyword">double</span>)<span class="keyword">this</span>.regItem * itemFactorValue));</span><br><span class="line">                    <span class="keyword">this</span>.loss += (<span class="keyword">double</span>)<span class="keyword">this</span>.regUser * userFactorValue * userFactorValue + (<span class="keyword">double</span>)<span class="keyword">this</span>.regItem * itemFactorValue * itemFactorValue;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">this</span>.loss *= <span class="number">0.5D</span>;</span><br><span class="line">            <span class="keyword">if</span> (<span class="keyword">this</span>.isConverged(iter) &amp;&amp; <span class="keyword">this</span>.earlyStop) &#123;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">this</span>.updateLRate(iter);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">double</span> <span class="title">predict</span><span class="params">(<span class="keyword">int</span> userIdx, <span class="keyword">int</span> itemIdx)</span> <span class="keyword">throws</span> LibrecException </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.userFactors.row(userIdx).dot(<span class="keyword">this</span>.itemFactors.row(itemIdx)) + <span class="keyword">this</span>.userBiases.get(userIdx) + <span class="keyword">this</span>.itemBiases.get(itemIdx) + <span class="keyword">this</span>.globalMean;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="预测公式">预测公式</h1>
<p><span class="math inline">\(\hat{r_{u i}}=\mu+b_{i}+b_{u}+p_{u} q_{i}^{T}\)</span></p>
<p>其中，<span class="math inline">\(\mu\)</span>表示全局均值，在代码中使用<code>this.globalMean</code>表示，<span class="math inline">\(b_{i}\)</span>表示物品偏差项，在代码中使用<code>this.itemBiases.get(itemIdx)</code>表示，需要注意的是，物品偏置项需要根据物品ID进行索引，<span class="math inline">\(b_{u}\)</span>表示用户偏差项，在代码中使用<code>this.userBiases.get(userIdx)</code>表示，同样地，用户偏置项需要根据用户ID来进行索引，<span class="math inline">\(p_{u}\)</span>表示对用户评分矩阵进行矩阵分解后得到的用户向量，<span class="math inline">\(q_{i}\)</span>表示对用户评分矩阵进行分解后得到的物品向量。</p>
<ul>
<li>PS：这里说是用户向量和物品向量的原因是都是针对某一用户和物品进行计算。</li>
</ul>
<p>预测公式代码如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">double</span> <span class="title">predict</span><span class="params">(<span class="keyword">int</span> userIdx, <span class="keyword">int</span> itemIdx)</span> <span class="keyword">throws</span> LibrecException </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>.userFactors.row(userIdx).dot(<span class="keyword">this</span>.itemFactors.row(itemIdx)) + <span class="keyword">this</span>.userBiases.get(userIdx) + <span class="keyword">this</span>.itemBiases.get(itemIdx) + <span class="keyword">this</span>.globalMean;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>传入<code>userIdx</code>和<code>itemIdx</code>的原因是需要明确特定用户和特定物品，才能进行预测评分，<code>userFactors</code>和<code>itemFactors</code>是对用户评分矩阵进行矩阵分解后得到的用户矩阵和物品矩阵，需要明确索引才能使用。</p>
<h1 id="损失函数公式">损失函数公式</h1>
<p><span class="math inline">\(\sum_{r_{u i} \in R_{\text {train }}}\left(r_{u i}-\hat{r}_{u i}\right)^{2}+\lambda\left(b_{i}^{2}+b_{u}^{2}+\left\|q_{i}\right\|^{2}+\left\|p_{u}\right\|^{2}\right)\)</span></p>
<p>将用户对物品的特定评分与真实评分之间的差值的平方作为损失函数的一部分是显而易见的，在代码中体现如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 实际评分</span></span><br><span class="line"><span class="keyword">double</span> realRating = matrixEntry.get();</span><br><span class="line"><span class="comment">// 预测评分</span></span><br><span class="line"><span class="keyword">double</span> predictRating = <span class="keyword">this</span>.predict(userIdx, itemIdx);</span><br><span class="line"><span class="comment">// 实际评分与预测评分的误差</span></span><br><span class="line"><span class="keyword">double</span> error = realRating - predictRating;</span><br><span class="line"><span class="keyword">this</span>.loss += error * error;</span><br></pre></td></tr></table></figure>
<p>通过以上展示的<code>predict()</code>函数可以直接根据用户ID与物品ID求出预测评分，但值得琢磨的是，实际评分是如何得到的？</p>
<p>可以看到，<code>MatrixEntry</code>接口是如下定义的，</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">MatrixEntry</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">row</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">column</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">double</span> <span class="title">get</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">set</span><span class="params">(<span class="keyword">double</span> var1)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">rowPosition</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">columnPosition</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在<code>SequentialSparseMatrixEntry</code>类中实现了<code>MatrixEntry</code>接口的<code>get()</code>方法，</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">double</span> <span class="title">get</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>.tempVector.getAtPosition(<span class="keyword">this</span>.columnPosition);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>值得注意的是，在<code>SequentialSparseMatrixEntry</code>类中，<code>columnPosition</code>属性被初始化为<code>-1</code>，这就让我们联想到，标签的值总是被设置在最后一列，我们可以验证这个猜想，在代码中，<code>var2</code>被强制转换为<code>MatrixEntry</code>类型，而<code>var2</code>又是<code>trainMatrix</code>的一个迭代器，不出所料地，<code>trainMatrix</code>是·<code>SequentialAccessSparseMatrix</code>类型，所以程序中可以直接调用<code>get()</code>方法来获取数据集中的实际评分值。</p>
<p>我们重新来回到损失函数的话题上，接下来需要关注正则项的那一部分，即</p>
<p><span class="math inline">\(\lambda\left(b_{i}^{2}+b_{u}^{2}+\left|q_{i}\right|^{2}+\left|p_{u}\right|^{2}\right)\)</span></p>
<p>首先我们关注<span class="math inline">\(\lambda(b_{i}^{2}+b_{u}^{2})\)</span>，这一部分的代码比较好写，根据索引找到特定的用户偏置向量和物品偏置向量就可以写出代码，代码如下所示，</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">double</span> userBiasValue = <span class="keyword">this</span>.userBiases.get(userIdx);</span><br><span class="line"><span class="keyword">this</span>.loss += <span class="keyword">this</span>.regBias * userBiasValue * userBiasValue;</span><br><span class="line"><span class="keyword">double</span> itemBiasValue = <span class="keyword">this</span>.itemBiases.get(itemIdx);</span><br><span class="line"><span class="keyword">this</span>.loss += <span class="keyword">this</span>.regBias * itemBiasValue * itemBiasValue;</span><br></pre></td></tr></table></figure>
<p>接下来的一部分就可能比较难懂，在对用户评分矩阵进行分解时，我们有一个超参数<span class="math inline">\(K\)</span>，表示的是隐因子的个数，在电影的用户评分矩阵中，隐因子指的可能是喜剧片、动画片等内容，换言之，用户是因为这些隐因子才作出的这些评分，所以矩阵分解的公式可以表示如下，</p>
<p><span class="math inline">\(\hat{r_{u i}}=p_{u} q_{i}^{T}=\sum_{k=1}^{K} p_{u k} q_{k i}^{T}=\sum_{k=1}^{K} p_{u k} q_{i k} \approx r_{u i}\)</span></p>
<p>所以在代码中我们需要增加一层遍历，即从1到<span class="math inline">\(K\)</span>对隐因子的遍历，对应每一种隐因子ID和用户ID有不同的用户隐因子值，同样的，对于每一种隐因子ID和物品ID有不同的物品隐因子值，对于<span class="math inline">\(\lambda\left(\left|q_{ik}\right|^{2}+\left|p_{uk}\right|^{2}\right)\)</span>这一部分的代码如下所示，</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> factorIdx = <span class="number">0</span>; factorIdx &lt; <span class="keyword">this</span>.numFactors; ++factorIdx) &#123;</span><br><span class="line">    <span class="keyword">double</span> userFactorValue = <span class="keyword">this</span>.userFactors.get(userIdx, factorIdx);</span><br><span class="line">    <span class="keyword">double</span> itemFactorValue = <span class="keyword">this</span>.itemFactors.get(itemIdx, factorIdx);</span><br><span class="line">    <span class="keyword">this</span>.loss += (<span class="keyword">double</span>)<span class="keyword">this</span>.regUser * userFactorValue * userFactorValue + (<span class="keyword">double</span>)<span class="keyword">this</span>.regItem * itemFactorValue * itemFactorValue;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>以上就是损失函数全部的内容。</p>
<h1 id="更新公式">更新公式</h1>
<p><span class="math inline">\(\begin{array}{l} b_{u} \leftarrow b_{u}+\gamma\left(e_{u i}-\lambda b_{u}\right) \\ b_{i} \leftarrow b_{i}+\gamma\left(e_{u i}-\lambda b_{i}\right) \\ p_{u} \leftarrow p_{u}+\gamma\left(e_{u i} \cdot q_{i}-\lambda p_{u}\right) \\ q_{i} \leftarrow q_{i}+\gamma\left(e_{u i} \cdot p_{u}-\lambda q_{i}\right) \end{array}\)</span></p>
<p>有了预测函数和损失函数的铺垫，这一部分就可以直接上代码，函数变量基本是差不多的，</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">this</span>.userBiases.plus(userIdx, (<span class="keyword">double</span>)<span class="keyword">this</span>.learnRate * (error - <span class="keyword">this</span>.regBias * userBiasValue));</span><br><span class="line"><span class="keyword">this</span>.itemBiases.plus(itemIdx, (<span class="keyword">double</span>)<span class="keyword">this</span>.learnRate * (error - <span class="keyword">this</span>.regBias * itemBiasValu));</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> factorIdx = <span class="number">0</span>; factorIdx &lt; <span class="keyword">this</span>.numFactors; ++factorIdx) &#123;</span><br><span class="line">    <span class="keyword">double</span> userFactorValue = <span class="keyword">this</span>.userFactors.get(userIdx, factorIdx);</span><br><span class="line">    <span class="keyword">double</span> itemFactorValue = <span class="keyword">this</span>.itemFactors.get(itemIdx, factorIdx);</span><br><span class="line">    <span class="keyword">this</span>.userFactors.plus(userIdx, factorIdx, (<span class="keyword">double</span>)<span class="keyword">this</span>.learnRate * (error * itemFactorValue - (<span class="keyword">double</span>)<span class="keyword">this</span>.regUser * userFactorValue));</span><br><span class="line">    <span class="keyword">this</span>.itemFactors.plus(itemIdx, factorIdx, (<span class="keyword">double</span>)<span class="keyword">this</span>.learnRate * (error * userFactorValue - (<span class="keyword">double</span>)<span class="keyword">this</span>.regItem * itemFactorValue));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>只需要注意一点，<code>plus()</code>函数是需要知道具体索引位置才能进行更新的，所以需要传入索引参数。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// DenseVector类中的plus方法</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">plus</span><span class="params">(<span class="keyword">int</span> index, <span class="keyword">double</span> value)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.set(index, value + <span class="keyword">this</span>.get(index));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// DenseMatrix类中的plus方法</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">plus</span><span class="params">(<span class="keyword">int</span> row, <span class="keyword">int</span> column, <span class="keyword">double</span> value)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">double</span>[] var10000 = <span class="keyword">this</span>.values[row];</span><br><span class="line">    var10000[column] += value;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer综述论文阅读</title>
    <url>/2022/02/26/Transformer%E7%BB%BC%E8%BF%B0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>最近想要研读一下关于transformer变体结构在多模态特征融合时的操作的论文，首先需要广泛地了解一下transformer结构及其变体，于是阅读了复旦大学邱锡鹏教授组里的transformer综述论文，本篇博客详细记录了综述论文中的重点以及本文作者对于这篇论文的理解。 <span id="more"></span></p>
<h3 id="介绍">介绍</h3>
<p>Transformer结构在许多人工智能领域都取得了巨大成功，如自然语言处理、计算机视觉和音频处理领域，到目前，已经提出了大量Transformer结构的变体（又称X-formers），综述论文从三个角度介绍了各种X-former：X-former对于传统Transformer架构的修改、基于Transformer变体的预训练模型以及Transformer变体模型在领域上的应用。</p>
<p>Transformer结构最初是作为一个用于机器翻译的序列到序列模型提出的，后来的工作表明，基于Transformer的预训练模型（PTMs）可以在各种任务上达到最先进的性能。由于Transformer取得的成功，在过去几年内，又有人陆续提出Transformer的变体，主要是在以下几个角度作出改进：</p>
<ol type="1">
<li>模型效率。应用Transformer的一个关键挑战是它在处理长序列时效率低下，这主要是自注意模块中的计算和记忆复杂性导致的，改进方法包括轻量级注意力（如稀疏注意力变体）和采用递归和分层的注意力机制。</li>
<li>模型泛化。由于Transformer是一个灵活的架构，对输入数据的结构偏差几乎不作假设，所以很难在小规模的数据上进行训练，改进方法包括引入结构偏差或正则化，以及在大规模未标记数据上进行预训练。</li>
<li>模型适应性。这项工作的目的是使Transformer适应特定的下游任务和应用。</li>
</ol>
<h3 id="背景">背景</h3>
<h4 id="传统transformer">传统Transformer</h4>
<p>传统的Transformer是一个序列到序列的模型，由一个编码器和一个解码器组成，每个编码器是由<span class="math inline">\(L\)</span>个相同的块堆叠而成。每个编码器块主要是由一个多头注意力模块和一个前馈神经网络组成，为了建立更深层次的模型，在每个块周围采用了残差连接，然后是层归一化操作。与编码器块相比，解码器块在多头自注意力模块和前馈神经网络这件额外插入交叉主义模块。</p>
<blockquote>
<p><strong>需要注意的是，与编码器中的自注意模块不同的是，解码器中的自注意模块被调整为每个位置防止关注后续位置。</strong></p>
</blockquote>
<p>传统Transformer的架构如下图所示，</p>
<center>
<img src="/2022/02/26/Transformer%E7%BB%BC%E8%BF%B0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/001.png" width="500px">
<p style="font-family:KaiTi">
图1：传统Transformer架构图
</p>
</center>
<h5 id="注意力模块">注意力模块</h5>
<p>Transformer的注意力机制采用QKV模型，计算公式如下，</p>
<p><span class="math display">\[
\text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{\top}}{\sqrt{D_{k}}}\right) V=A V
\]</span></p>
<p>在计算过程中需要注意QKV矩阵的维度，<span class="math inline">\(\mathbf{Q} \in \mathbb{R}^{N \times D_{k}}\)</span>，<span class="math inline">\(\mathrm{K} \in \mathbb{R}^{M \times D_{k}}\)</span>，<span class="math inline">\(\mathrm{V} \in \mathbb{R}^{M \times D_{v}}\)</span>，其中<span class="math inline">\(N\)</span>和<span class="math inline">\(M\)</span>代表queries和keys(values)的长度，<span class="math inline">\(D_{k}\)</span>和<span class="math inline">\(D_{v}\)</span>代表keys(queries)和values的维度，</p>
<blockquote>
<p><strong>1. softmax归一化操作按行进行</strong><br> <strong>2. 点积缩放（除以<span class="math inline">\(\sqrt{D_{k}}\)</span>）减轻softmax函数的梯度消失问题</strong></p>
</blockquote>
<p>传统的Transformer模型采用多头注意力机制，将原本的<span class="math inline">\(D_{m}\)</span>维度的queries、keys和values分别映射成<span class="math inline">\(D_{k}\)</span>、<span class="math inline">\(D_{k}\)</span>、<span class="math inline">\(D_{v}\)</span>维，并使用<span class="math inline">\(H\)</span>组学习的投影集。对于每一个投影的query、key和value，根据QKV模型计算公式计算输出，然后，将所有的输出连接起来，并将它们投射到<span class="math inline">\(D_{m}\)</span>的维度表示，多头注意力机制计算公式如下，</p>
<p><span class="math display">\[
\begin{array}{r}
\text { MultiHeadAttn }(\mathbf{Q}, \mathbf{K}, \mathbf{V})=\text { Concat }\left(\text { head }_{1}, \cdots, \text { head }_{H}\right) \mathbf{W}^{O}, \\
\text { where head }_{i}=\text { Attention }\left(\mathbf{Q W}_{i}^{Q}, \mathbf{K W}_{i}^{K}, \mathbf{V W}_{i}^{V}\right) .
\end{array}
\]</span></p>
<p><strong>在Transformer中，有三种注意力机制方式：</strong></p>
<ol type="1">
<li><strong>自注意力机制</strong>。在transformer的编码器中，设置<span class="math inline">\(Q=K=V=X\)</span>，其中<span class="math inline">\(X\)</span>是上一层的输出。<br></li>
<li><strong>掩码自注意力机制</strong>。在transformer解码器中，自注意是受限制的，即每个位置的查询只能注意到该位置之前的所有键值对。为了实现并行训练，通常对非标准化注意力矩阵<span class="math inline">\(\hat{A}=\exp \left(\frac{Q K^{\top}}{\sqrt{D_{k}}}\right)\)</span>应用屏蔽函数，其中非法位置通过设置<span class="math inline">\(\hat{A}_{i j}=-\infty \text { if } i&lt;j\)</span>来进行实现。这种注意方式也被称为自回归注意或因果注意。</li>
<li><strong>交叉注意机制</strong>。交叉注意记住的query是由前一个解码层的输出投影出来的，而key和value是由编码器的输出投影出来的。</li>
</ol>
<h5 id="position-wise前馈神经网络">position-wise前馈神经网络</h5>
<blockquote>
<p><strong>在position-wise前馈神经网络中，参数在不同的位置上是共享的，因此position-wise前馈神经网络也可以理解为两个卷积层，核大小为1。</strong></p>
</blockquote>
<p>position-wise前馈神经网络的计算公式如下所示，</p>
<p><span class="math display">\[
\operatorname{FFN}\left(\mathbf{H}^{\prime}\right)=\operatorname{ReLU}\left(\mathbf{H}^{\prime} \mathbf{W}^{1}+\mathbf{b}^{1}\right) \mathbf{W}^{2}+\mathbf{b}^{2}
\]</span></p>
<p>其中，<span class="math inline">\(\mathbf{H}^{\prime}\)</span>是上一层输出，<span class="math inline">\(\mathbf{W}^{1} \in \mathbb{R}^{D_{m} \times D_{f}}, \mathbf{W}^{2} \in \mathbb{R}^{D_{f} \times D_{m}}, \mathbf{b}^{1} \in \mathbb{R}^{D_{f}}, \mathbf{b}^{2} \in \mathbb{R}^{D_{m}}\)</span>，需要额外注意的是，<strong>一般设置<span class="math inline">\({D_{f}}\)</span>大于<span class="math inline">\({D_{m}}\)</span></strong>。</p>
<h5 id="残差连接和标准化">残差连接和标准化</h5>
<p>为了建立一个深层次模型，transformer在每个模块周围采用了一个残差连接，然后进行层级标准化处理。例如，每一个transformer编码器模块可以表示为如下公式，</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{H}^{\prime} &amp;=\text { LayerNorm }(\text { SelfAttention }(\mathbf{X})+\mathbf{X}) \\
\mathbf{H} &amp;=\text { LayerNorm }\left(\text { FFN }\left(\mathbf{H}^{\prime}\right)+\mathbf{H}^{\prime}\right)
\end{aligned}
\]</span></p>
<h5 id="位置编码">位置编码</h5>
<p>由于transformer没有引入递归或卷积操作，因此对位置信息一无所知（尤其是对于编码器而言），因此需要额外的位置表示来模拟token的排序。</p>
<h4 id="模型用途">模型用途</h4>
<ol type="1">
<li>使用编码器-解码器结构，通常用于序列到序列的建模。</li>
<li>只使用编码器结构，编码器的输出被用作输入序列的表示，通常用于分类或序列标记问题。</li>
<li>只使用解码器结构，其中编码器-解码器交叉注意模块也被移除，通常用于序列生成问题，如语言建模。</li>
</ol>
<h4 id="模型复杂度和参数量分析">模型复杂度和参数量分析</h4>
<p>在假设序列长度为<span class="math inline">\(T\)</span>，维度为<span class="math inline">\(D\)</span>，FFN全连接层维度为<span class="math inline">\(4D\)</span>的情况下，自注意模块和position-wise前馈神经网络模块的复杂度和参数量如下表所示，</p>
<center>
<img src="/2022/02/26/Transformer%E7%BB%BC%E8%BF%B0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/002.png" width="600px">
</center>
<p>对于transformer结构中自注意模块和前馈神经网络复杂度和参数量的推导，可以参考以下博客：</p>
<p><a href="https://0809zheng.github.io/2021/07/12/efficienttransformer.html" target="_blank">传送门1：https://0809zheng.github.io/2021/07/12/efficienttransformer.html</a> <a href="https://zhuanlan.zhihu.com/p/264749298" target="_blank">传送门2：https://zhuanlan.zhihu.com/p/264749298</a></p>
<h4 id="transformer与其他模型的比较">transformer与其他模型的比较</h4>
<h5 id="对自注意操作的分析">对自注意操作的分析</h5>
<ol type="1">
<li>它具有与全连接层相同的最大路径长度，使其适合于长距离的依赖关系建模，与全连接层相比，它的参数效率更高，在处理可变长度的输入时更加灵活。</li>
<li>由于卷积层的感受野有限，人们通常需要堆叠一个深度网络来拥有一个全局感受野，另一方面，恒定的最大路径长度使自注意能够以恒定的层数来模拟长距离的依赖关系。</li>
<li>相比于循环层，自注意的并行度更高，更擅长长距离建模。</li>
</ol>
<p>下表展示了不同类型层的复杂度分析、最小序列操作数和最大路径长度。</p>
<center>
<img src="/2022/02/26/Transformer%E7%BB%BC%E8%BF%B0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/003.png" width="600px">
</center>
<h5 id="关于归纳偏置">关于归纳偏置</h5>
<blockquote>
<p>在机器学习中，很多学习算法经常会对学习的问题做一些关于目标函数的必要假设，称为归纳偏置 (Inductive Bias)</p>
</blockquote>
<p>卷积网络通过共享的局部核函数施加了平移不变性和局部性的归纳偏置，循环神经网络通过其马尔科夫结构带来了时间不变性和位置性的归纳偏置，而transformer架构对数据的结构信息几乎不作假设，这使得transformer成为一个通用和灵活的架构，带来的副作用是<strong>容易对小规模的数据进行过度拟合</strong>。另外，transformer可以看作是一个带有完整有向图上所定义的GNN，其中每个输入都可视为图中的一个节点，然而，transformer和GNN之间的主要区别在于transformer没有引入关于如何构造输入数据的先验知识，transformer中的信息传递过程完全依赖于内容的相似性度量。</p>
<blockquote>
<p>卷积网络的平移不变性体现在卷积核共享权重，局部性体现在卷积核大小是有限的；RNN的时间不变性体现在序列顺序中的每个时间步之间都是有关联的。</p>
</blockquote>
<h3 id="对transformer变体的分类">对transformer变体的分类</h3>
<p>到目前为止，人们已经从三个方面改进传统的transformer模型：架构修改、预训练和应用，其中架构修改又分为模块级别的改进和体系级别的改进，下图给出了transformer变体分类说明，</p>
<center>
<img src="/2022/02/26/Transformer%E7%BB%BC%E8%BF%B0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/004.png" width="600px">
<p style="font-family:KaiTi">
图2：Transformer变体分类
</p>
</center>
<p>详细transformer变体分类如下图所示，</p>
<center>
<img src="/2022/02/26/Transformer%E7%BB%BC%E8%BF%B0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/005.png" width="600px">
<p style="font-family:KaiTi">
图3：详细Transformer变体分类
</p>
</center>
<h3 id="模块级别改进">模块级别改进</h3>
<h4 id="attention机制">attention机制</h4>
<p>self-attention在实际应用中存在两大挑战：</p>
<ol type="1">
<li>复杂性。在处理长序列时，注意力模块称为瓶颈。</li>
<li>结构性先验。自注意对输入的数据不存在任何结构性偏向，甚至顺序信息也需要从训练数据中学习，因此，不含预训练的transformer在小规模或中等规模的数据上通常容易过拟合。</li>
</ol>
<p>attention机制的改进方向：</p>
<ol type="1">
<li>稀疏注意力机制，将稀疏偏置引入到注意力计算。</li>
<li>线性化注意，将注意力矩阵和特征映射分离，降低至线性复杂度。</li>
<li>原型和显存压缩，减少了查询或键值对的数量，从而减小注意力矩阵大小。</li>
<li>低秩自注意，主要抓住自注意力的低秩性。</li>
<li>带有先验的注意力，主要使用先验注意力分布来补充或替代标准注意力。</li>
<li>改进的多头机制。</li>
</ol>
<h5 id="稀疏注意力机制">稀疏注意力机制</h5>
<p>在标准的自注意机制中，每个token都需要注意其他所有token，但事实上，所学习到的注意力矩阵大多是非常稀疏的，因此，可以通过加入结构性偏差来限制每个query所关注的query-key对的数量，从而降低计算的复杂性。计算公式如下所示，</p>
<p><span class="math display">\[
\hat{\mathbf{A}}_{i j}=\left\{\begin{array}{ll}
\mathbf{q}_{i} \mathbf{k}_{j}^{\top} &amp; \text { if token } i \text { attends to token } j \\
-\infty &amp; \text { if token } i \text { does not attend to token } j
\end{array}\right.
\]</span></p>
<p>根据确定稀疏连接的指标，可以将这些方法分为两类，基于位置的稀疏注意和基于内容的稀疏注意。</p>
<h6 id="基于位置信息的稀疏化注意力">基于位置信息的稀疏化注意力</h6>
<p>在基于位置信息的稀疏化注意力中，注意力矩阵是根据一些预先定义的模式来限制的，尽管这些稀疏模式有不同的形式，但有些可以分解为一些原子的稀疏模式，如下图所示，</p>
<center>
<img src="/2022/02/26/Transformer%E7%BB%BC%E8%BF%B0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/006.png" width="650px">
<p style="font-family:KaiTi">
图4：原子稀疏注意力模式
</p>
</center>
<ol type="1">
<li>global attention：增加一些全局节点作为节点间信息传播的枢纽，这些全局节点可以注意序列中的所有节点，整个序列也注意这些全局节点。</li>
<li>band attention：利用数据具有很强局部性的特点，限制了每个query对其邻居节点的关注。</li>
<li>dilated attention：通过扩大扩张空隙来获得更大的感受野。</li>
<li>random attention：为了提高非局部信息互动的能力，对每个query随机抽取几条边。</li>
<li>block local attention：这类注意将输入序列分割成几个不重叠的query块，每个query块都与一个本地记忆块相关，一个query块中的所有query都只关注相应记忆块中的key，图4(e)描述了一种常见的情况，即记忆块与相应的query块是相同的。</li>
</ol>
<p>下图展示一些复合稀疏注意力模式，复合稀疏注意力模式通常是原子稀疏注意力模式组合而成的，</p>
<center>
<img src="/2022/02/26/Transformer%E7%BB%BC%E8%BF%B0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/007.png" width="550px">
<p style="font-family:KaiTi">
图5：复合稀疏注意力模式
</p>
</center>
<p>除了上述模式外，一些现有的研究还探索了针对特定数据类型的扩展稀疏模式。对于文本数据，BP-Transformer构建了一个二叉树，其中所有的token是叶子节点，内部节点是包含许多token的跨度节点，是分层组织的；对于视觉数据，有Image Transformer和Axial Transformer。三种transformer变体的扩展稀疏注意力模式如下图所示，</p>
<center>
<img src="/2022/02/26/Transformer%E7%BB%BC%E8%BF%B0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/008.png" width="550px">
<p style="font-family:KaiTi">
图6：扩展稀疏注意力模式
</p>
</center>
<h6 id="基于内容的稀疏注意力">基于内容的稀疏注意力</h6>
<p>构建基于内容的稀疏图的一个直接方法是选择那些可能与给定query有较大相似性分数的key。Routing Transformer使用K-means聚类算法，将query和key都集中在同一组中心点向量上，每个query只与其处在相同cluster下的key进行交互。Reformer采用LSH哈希方法来为每个query选择key-value，其主要思想是对query和key哈希，分到多个桶内，在同一个桶内的query、key参与交互。SAC将输入序列视为一个图，并学习构建注意力边，以使用自适应稀疏连接改善特定任务的性能，其中边预测器是通过强化学习来训练的。Sparse Sinkhorn Attention首先将query和key通过排序网络控制分为几个块，并为每个query块分配一个key块，每个query只允许关注被分配给其相应块中的key。</p>
<h5 id="线性注意力">线性注意力</h5>
<p>线性化注意力是一类用<span class="math inline">\(\phi(Q) \phi(K)^{\top}\)</span>近似或替换非标准化注意力矩阵<span class="math inline">\(\exp \left(Q K^{\top}\right)\)</span>的方法，其中<span class="math inline">\(\phi\)</span>是按行方式应用的特征图，因此，非归一化注意力矩阵的计算可以通过计算<span class="math inline">\(\phi(Q)\left(\phi(K)^{\top} V\right)\)</span>来线性化，如下图所示，</p>
<center>
<img src="/2022/02/26/Transformer%E7%BB%BC%E8%BF%B0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/009.png" width="600px">
<p style="font-family:KaiTi">
图7：标准注意力和线性注意力对比
</p>
</center>
<p>该模型通过聚合(由特征映射的)key和value的外积表示的关联来维护内存矩阵，然后通过将内存矩阵与具有适当归一化的特征映射查询相乘来检索值，这种方法有两个关键组件，包括特征图和聚合规则。</p>
<h5 id="查询原型和键值内存压缩">查询原型和键值内存压缩</h5>
<p>除了使用稀疏注意和基于内核的线性化注意，人们还可以通过减少查询(query)或键值对的数量来降低注意的复杂性。使用原型查询的注意力机制和压缩键值内存的注意力机制如下图所示：</p>
<center>
<img src="/2022/02/26/Transformer%E7%BB%BC%E8%BF%B0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/010.png" width="600px">
<p style="font-family:KaiTi">
图8：查询原型和键值内存压缩
</p>
</center>
<ol type="1">
<li><p><strong>使用原型查询的注意力机制</strong>：在查询原型中，几个查询的原型作为计算注意力分布的主要来源。该模型或者将这些注意力分布复制到所代表的查询的位置上，或者使用离散的均匀分布来填充这些位置。例如clustered attention将查询分为几个集群，然后计算集群中心点的注意力分布，一个集群中的所有查询都共享相应中心点计算的注意力分布；Informer使用明确的查询稀疏度测量法从查询中选择原型，然后，在查询稀疏度测量下，只计算top-u查询的注意力分布，其他查询被分配为离散的均匀分布。</p></li>
<li><p><strong>压缩键值内存的注意力机制</strong>：可以通过在应用注意力机制之前减少键值对的数量来降低复杂度。例如，Liu等人提出了使用分层卷积的方法减少键和值的数量；Set Transformer和Luna使用一些外部可训练的全局节点总结来自输入的信息，然后将总结后的表征作为输入注意的压缩键值内存。</p></li>
</ol>
<h5 id="低秩自注意">低秩自注意</h5>
<p>一些经验和理论分析报告称，自注意矩阵通常是低秩的，这个属性的含义是双重的，一是低秩属性可以用参数化来显式建模，二是可以用低秩近似值代替自注意力矩阵。</p>
<h5 id="带有先验的注意力">带有先验的注意力</h5>
<p>传统意义上，注意力分布是由输入产生的，作为一种广义的情况，注意力分布也可以来自其他来源，我们称之为先验，先验注意力分布可以是对输入产生的分布的补充或替代，我们把这种注意力的表述抽象为带有先验的注意力。在大多数情况下，两种注意力分布的融合可以通过计算对应于先验注意力分布和输入产生的注意力分布的加权和来完成，然后再应用softmax。带有先验的注意力如下图所示，</p>
<center>
<img src="/2022/02/26/Transformer%E7%BB%BC%E8%BF%B0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/011.png" width="600px">
<p style="font-family:KaiTi">
图9：带有先验的注意力
</p>
</center>
<ol type="1">
<li><p>模型位置先验：一些类型的数据（如文本）可以表现出对位置性的强烈偏好，这个属性可以显式编码为先验注意力。一个简单的方法是在位置上使用一个高斯分布，具体来说，我们可以将输入生成的注意力分布与一些高斯密度相乘，然后重新规范化，这相当于在生成的注意力分数中加入偏置项。</p></li>
<li><p>底层模块先验：在transformer架构中，经常观察到相邻层的注意力分布相似，因此，很自然地将前一层的注意力分布作为下一层注意力计算的先验。例如，Predictive Attention Transformer提出将二维卷积层应用于以前的注意力分数，并将最终的注意力分数计算为输入生成的注意力分数和卷积分数的凸组合。</p></li>
<li><p>多任务适配器先验：适配器是依赖于任务的训练模块，它们附加在预训练网络的特定位置，用于跨任务高效参数共享。</p></li>
<li><p>仅注意力先验：一些工作探索了使用独立于输入之间成对交互的注意力分布，换句话说，他们的模型只利用了先验注意力分布。例如，You等人利用高斯分布作为注意力计算的硬编码注意力分布，完全放弃了输入生成的注意力，只使用高斯分布来计算注意力，在这种方法中，均值和方差被设计为超参数，实验表明，硬编码的注意力，当只应用于自注意力时，可以在机器翻译任务中取得与基线模型相当的性能。</p></li>
</ol>
<h5 id="改进的多头机制">改进的多头机制</h5>
<p>多头注意能够在不同的位置共同注意来自不同表征子空间的信息，然而，没有任何机制可以保证不同的注意头确实捕捉到不同的特征。</p>
<ol type="1">
<li><p>头部行为建模：一系列工作致力于通过引入更复杂的机制来改进多头机制，以指导不同注意力头的行为或允许注意力头之间的互动。例如，Li等人在损失函数中引入了一个辅助的分歧正则化项，以鼓励不同注意力头之间的多样性，两个正则化项分别用于最大化输入子空间和输出表征的余弦距离，而另一个正则化项则是通过相应注意力矩阵的逐元相乘来分散多个头所关注的位置。</p></li>
<li><p>跨度受限的多头：原版注意力采用完全注意力跨度假设，其中查询可以关注所有键值对，然而，经常观察到，一些头主要将注意力集中在局部环境中，而其他一些头则关注更广泛的环境，因此，限制注意力的跨度可能是有益的。限制注意力跨度可以表示为将每个注意力分布值与一个掩码值相乘，然后重新归一，传统的注意力为所有距离分配掩码值1。Sukhbaatar采取一个可学习的注意力跨度，即适用一个可学习的标量<span class="math inline">\(z\)</span>和一个超参数<span class="math inline">\(R\)</span>来生成mask进而控制跨度。Multi-Scale Transformer采用了固定的跨度，不同层的不同头使用不同的最大跨度，一般来说，底层网络的最大跨度较小，上层网络的最大跨度较大。三种跨度掩码函数如下图所示，</p></li>
</ol>
<center>
<img src="/2022/02/26/Transformer%E7%BB%BC%E8%BF%B0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/012.png" width="700px">
<p style="font-family:KaiTi">
图10：三种跨度掩码函数
</p>
</center>
<ol start="3" type="1">
<li>精细聚合的多头：在每个注意力头计算其输出表示后，原版多头注意力将这些表示连接起来，然后对连接后的表示应用线性变换以获得最终的输出表示。有研究提出说，这种简单的逐个聚合范式并没有充分利用多头注意力的表现力，而使用更为复杂的聚合更为可取。因此有人提出了使用为胶囊网络设计的路由方法，注意力头的输出首先转化为输入胶囊，然后经过迭代路由过程得到输出胶囊，然后将输出胶囊连接起来作为多头注意力的最终输出。</li>
</ol>
<h4 id="其他模块级别改进">其他模块级别改进</h4>
<h5 id="位置表示">位置表示</h5>
<p>很容易验证卷积和循环网络不是置换等变的。 然而，Transformer 中的自注意力模块和位置前馈层都是置换等变的，这可能在建模时成为一个问题，而不是需要输入结构的集合输入问题。 例如，在对文本序列建模时，单词的顺序很重要，因此在 Transformer 架构中正确编码单词的位置至关重要。 因此，需要额外的机制将位置信息注入到 Transformer 中。 一种常见的设计是首先使用向量表示位置信息，然后将向量作为附加输入注入模型。</p>
<h6 id="绝对位置表示">绝对位置表示</h6>
<p>在传统Transformer中，位置信息被编码为绝对正弦位置编码，在位置编码完毕后，将序列中每个位置的位置编码加入到token的词嵌入中，然后送入Transformer。</p>
<p>另一种表示绝对位置的方法是为每个位置学习一组位置嵌入，与手工制作的位置嵌入相比，学习的嵌入更加灵活，因为位置表示可以通过反向传播适应任务，但是，嵌入的数量被限制在训练前确定的最大序列长度内，这使得这种方法不再是归纳性的，也就是说，不能处理比训练时看到的序列更长的序列。</p>
<p>合并绝对位置表示的基本方法是在token嵌入中添加位置编码。然而，当输入信号在层间传播时，位置信息可能会在上层丢失。后来的工作发现，将位置表示添加到每个Transformer层的输入中是有益的。</p>
<h6 id="相对位置表示">相对位置表示</h6>
<p>另一系列工作侧重于表示token之间的位置关系，而不是单个token的位置，直觉认为，在自注意力中输入元素（方向和距离）之间的成对位置关系可能比元素的位置更有益。遵循这一原则的方法称为相对位置表示。</p>
<h6 id="其他表示">其他表示</h6>
<p>一些研究已经探索使用包含绝对和相对位置信息的混合位置表示。 Transformer with Untied Position Encoding (TUPE) 将注意力分数的计算重新设计为内容到内容项、绝对位置到位置项和表示相对位置关系的偏置项的组合。</p>
<h6 id="没有显式编码的位置表示">没有显式编码的位置表示</h6>
<p>Wang 等人没有明确引入额外的位置编码，建议通过将嵌入推广到位置上的连续（复值）函数来对词嵌入中的位置信息进行编码。</p>
<h6 id="transformer-decoder的位置表示">Transformer decoder的位置表示</h6>
<p>在解码器的交叉注意中采取了掩码的自注意，而掩码的自注意不是置换等变的，因此，仅利用Transformer解码器的模型具有在不包含显式位置表示的情况下感知位置信息的潜力。语言建模任务的一些实证结果证实了这一点，作者发现删除位置编码甚至可以提高性能。</p>
<h5 id="层归一化">层归一化</h5>
<p>层归一化（LN）和残差连接被认为是一种稳定深度网络训练的机制（例如，缓解不理想的梯度和模型退化的问题），目前已经有一些研究致力于分析和改进LN模块。</p>
<h6 id="ln的放置">LN的放置</h6>
<p>在传统Transformer中，LN层位于残差块之间，称为post-LN，后来的Transformer实现将LN层放在注意力或FFN之前的残差连接内，在最后一层之后有一个额外的LN来控制最终的输出大小，这被称为pre-LN，post-LN和pre-LN的架构如下图所示，</p>
<center>
<img src="/2022/02/26/Transformer%E7%BB%BC%E8%BF%B0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/013.png" width="600px">
<p style="font-family:KaiTi">
图11：post-LN和pre-LN的架构对比
</p>
</center>
<p>另外，warm-up阶段可以从pre-LN Transformer上安全地去除，而不必担心输出层附近梯度过大导致训练不稳定的情况。</p>
]]></content>
      <categories>
        <category>深度学习论文阅读</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>ViLBERT论文阅读</title>
    <url>/2022/03/08/ViLBERT%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>BERT模型的提出使得大规模的预训练成为可能，与BERT模型仅仅处理文本模态不同，ViLBERT模型结合了图片和文本信息的特征，使用two-stream结构，基于大型的图像标题数据库训练与下游任务无关的通用模型，基于该模型通过少量调整即可实现通过标题检索图片、视觉问答等具体任务，最近在学习多模态特征有效的融合和对齐方法，于是对这篇文章进行了研读。</p>
<center>
<img src="/2022/03/08/ViLBERT%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/001.png" width="600px">
</center>
<span id="more"></span>
<p>论文地址和相关源码链接如下，</p>
<p><a href="https://arxiv.org/pdf/1908.02265.pdf" target="_blank">论文地址：https://arxiv.org/pdf/1908.02265.pdf</a> <br> <a href="https://github.com/facebookresearch/vilbert-multi-task" target="_blank">相关源码：https://github.com/facebookresearch/vilbert-multi-task</a></p>
<h3 id="摘要">摘要</h3>
<p>ViLBERT模型是一个学习与任务无关的图像文本特征联合表示的模型，文章将流行的针对文本模态的BERT架构扩展到一个多模态的two-stream模型架构，在不同的流中处理视觉和文本输入，并通过共同注意的transformer层进行信息交互。文章通过自动收集的大型概念性标题数据集上定义两个代理任务，来对模型进行预训练，然后将其转移到多个既定的下游视觉和语言任务中，包括视觉问题回答、视觉常识推理、指代表达和基于标题的图像检索任务，在进行下游任务时只对基础架构做少量的补充。文章观察到，与现有的特定任务模型相比，在所有的四个下游任务上都实现了最先进的改进。文章认为，其工作代表了一种转变，即不再把学习视觉和语言之间的grounding作为任务训练的一部分，而是将visual grounding作为一种可预训练和可转移的能力。</p>
<blockquote>
<p>PS: visual grounding涉及视觉和文本两个模态，输入是图片和对应的物体描述，输出是描述物体的box，与目标检测不同的是，输入多了语言信息，在对物体进行定位时，要先对语言模态的输入进行理解，并且和视觉模态的信息进行融合，最后利用得到的特征进行定位预测。</p>
</blockquote>
<h3 id="介绍">介绍</h3>
<p>近年来，通过图像、视频等生成文本的研究已有了很多丰硕的成果，这些方法和任务可归结为“vision-and-language”。虽然这些任务都需要将自然语言和视觉特征结合，但是“vision-and-language”任务还没有一个统一的基础来提升这种结合能力。“vision-and-language”现在通常的做法是先分别预训练语言和视觉模型，然后通过任务进行基础知识的学习。通过这种方法学到的基础知识并不可靠，如果数据量不足或者是有bias的，那么模型的泛化能力会很差。</p>
<p>首先对模型进行预训练，然后再在目标任务上进行微调的手段已经被广泛使用，ViLBERT也采用了这种先预训练后转移的方案，并在预训练时尽可能地学习视觉和文本之间的关系。</p>
<p>为了学习这些联合的视觉-语言表征，我们借鉴了自监督学习方面的成功经验，这些成功通过训练模型来执行代理任务，从大量的无标签数据源中获取丰富的语义和结构信息，这些代理任务包括预测掩蔽词等等。为了通过类似的方法学习visual grounding，必须确定一个视觉和语言能够对齐的数据源来学习模态之间的关系，文章采用了概念性标题数据集。</p>
<p>文章提出了一个联合模型ViLBERT，用于从成对的视觉语言数据中学习与目标任务无关的visual grounding。文章的方法扩展了最近的BERT语言模型来共同推理文本和图像，关键技术创新是为视觉和语言处理引入单独的流，通过共同注意的transformer层进行信息交互，这种结构可以适应每种模态的不同处理需求，并在不同深度的模态之间提供互动，文章在实验中证明，two-stream结构优于single-stream结构。模型结构如下图所示，</p>
<center>
<img src="/2022/03/08/ViLBERT%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/002.png" width="700px">
<p style="font-family:KaiTi">
图1：ViLBERT模型结构
</p>
</center>
<p>文章使用两种代理任务来训练模型：</p>
<ol type="1">
<li>遮蔽部分图片区域和文本单词，根据上下文内容预测被遮蔽部分。</li>
<li>预测文字和图片是否匹配。</li>
</ol>
<p>将预训练模型作为基础，用于四个目标视觉语言任务，分别是视觉问题回答、视觉常识推理、指代表达和基于标题的图像检索。</p>
<h3 id="方法">方法</h3>
<h4 id="bert">BERT</h4>
<p>BERT模型时一个基于注意力机制的双向语言模型，当在大型语言语料库上进行预训练时，BERT已被证明对多种自然语言处理任务的迁移学习非常有效。</p>
<p>ViLBERT修改了BERT的键值注意机制，开发了一个多模态的共同注意transformer模块，通过在多头注意中交换键值对，这种结构使视觉注意到的文本特征被纳入到视觉表征中，同时将文本注意大的视觉特征纳入到文本表征中，BERT结构的标准transformer模块和ViLBERT结构的共同注意transformer模块对比图如下所示，</p>
<center>
<img src="/2022/03/08/ViLBERT%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/003.png" width="600px">
<p style="font-family:KaiTi">
图2：标准transformer模块和共同注意transformer模块对比
</p>
</center>
<p>下面简单地介绍BERT的输入文本表示和预训练任务。BERT的输入文本表示由三部分构成，分别是位置嵌入(position embedding)、段嵌入(segment embedding)和目标词嵌入(token embedding)。BERT的预训练任务包括掩蔽语言建模和下一句预测。具体的对于BERT模型的介绍可以参考原论文，传送门如下，</p>
<p><a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank">传送门：https://arxiv.org/pdf/1810.04805.pdf</a></p>
<h4 id="vilbert扩展bert以联合表示图像和文本">ViLBERT：扩展BERT以联合表示图像和文本</h4>
<p>受BERT模型在语言建模方面的成功启发，我们希望开发类似的模型和训练任务，从成对的数据中学习语言和视觉内容的联合表示，具体来说，文章考虑联合表示静态图像和相应的描述性文本。</p>
<p>文章首先叙述了一些直观的想法。可以首先将视觉输入的空间离散化，将这些视觉标记完全视为文本输入，并输入到预训练的BERT模型中，这种方法为single-stream方法。文章说明了single-stream方法的一些缺点：</p>
<ol type="1">
<li>最初的聚类可能会导致离散化错误，并失去重要的视觉细节。</li>
<li>以上方法对两种模态的输入进行了相同的处理，忽略了它们可能由于其固有的复杂性或其输入表征的初始抽象水平而需要不同的处理水平。</li>
<li>强迫预训练的权重去适应大量额外的视觉标记，可能会损害学到的BERT语言模型。</li>
</ol>
<p>基于以上缺点，文章开发了一个two-stream架构，分别对每个模态进行建模，然后通过基于注意力的互动来融合它们，这种方法允许每个模态的网络深度不同，并能在不同的深度上实现跨模态连接。</p>
<p>如图1所示，ViLBERT学习的是静态图像及其对应描述文本的联合表征，分别对两种模态进行建模，然后通过一组基于注意力机制的信息交互将它们融合在一起。对每种模态都可使用不同深度的网络，并支持不同深度的跨模态交互。双流架构中的每个流都是由一系列的TRM(transformer block)和Co-TRM组成，可以观察到，流之间的信息交互被限制在特定层，文本特征需要先经过TRM模块处理才能进行信息交互，文章给出的解释是所选择的视觉特征已经相对高级，与句子中的单词相比，视觉特征需要有限的上下文聚合。</p>
<h5 id="共同注意的transformer层">共同注意的transformer层</h5>
<p>ViLBERT中引入了共同注意的transformer层，该模块为每个模态产生以其他模态为条件的注意力集合特征，实际上是在视觉流中执行以图像为条件的语言注意力，在语言流中执行以语言为条件的图像注意力。</p>
<h5 id="图像表示">图像表示</h5>
<p>从预先训练好的物体检测网络中提取边界框和它们的视觉特征来生成图像区域特征，与文本中的文字不同，图像区域缺乏自然排序，文章采用一个5维的向量对区域进行位置编码，五个维度的元素分别为归一化后的bounding boxes的左上角和右下角的坐标以及图像区域覆盖占比，然后使用映射将位置编码与视觉特征维数匹配，进行相加后得到图像区域特征。使用一个特定的IMG token作为图像序列的开始，并用它的输出表征整个图像。</p>
<h5 id="预训练任务">预训练任务</h5>
<p>训练ViLBERT模型时采用了两个预训练任务，分别是掩蔽多模态建模和多模态对齐预测，如下图所示，</p>
<center>
<img src="/2022/03/08/ViLBERT%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/004.png" width="600px">
<p style="font-family:KaiTi">
图3：ViLBERT预训练任务
</p>
</center>
<p>掩蔽多模态建模任务来自于标准BERT中的掩蔽语言建模任务，屏蔽大约15%的单词和图像区域输入，并要求模型在剩余输入的情况下对屏蔽进行预测。在对图像进行掩蔽时，90%的概率是直接遮挡，另外10%的概率保持不变，文本的掩蔽方案与BERT一致。ViLBERT并不直接预测被掩蔽区域的图像区域特征值（原因：语言往往只能识别视觉内容的高级语义，而不太可能重建准确的图像特征），而是预测对应区域在语义类别上的分布（不预测位置信息），为了监督这一点，文章从用于特征提取的同一预训练检测模型中获取该区域的特征分布，训练模型以最小化这两个分布之间的KL散度。</p>
<p>多模态对齐预测任务必须预测图像和文本是否对齐，即文本是否描述了图像，文章把输出的<span class="math inline">\(h_{IMG}\)</span>和<span class="math inline">\(h_{CLS}\)</span>作为视觉和语言输入的整体表示，借用视觉和语言模型的另一个常见结构，文章将整体表征计算为<span class="math inline">\(h_{IMG}\)</span>和<span class="math inline">\(h_{CLS}\)</span>之间的元素乘积，并添加一个线性层来进行图像和文字是否对齐的二元预测。</p>
]]></content>
      <categories>
        <category>深度学习论文阅读</category>
      </categories>
      <tags>
        <tag>多模态模型</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2020/09/06/hello-world/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<span id="more"></span>
<h2 id="quick-start">Quick Start</h2>
<h3 id="create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
      <tags>
        <tag>hello world</tag>
      </tags>
  </entry>
  <entry>
    <title>test</title>
    <url>/2020/09/06/test/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script>
]]></content>
      <tags>
        <tag>test</tag>
      </tags>
  </entry>
  <entry>
    <title>《计算机网络：自顶向下方法》第一章笔记整理</title>
    <url>/2021/02/02/%E3%80%8A%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%87%AA%E9%A1%B6%E5%90%91%E4%B8%8B%E6%96%B9%E6%B3%95%E3%80%8B%E7%AC%AC%E4%B8%80%E7%AB%A0%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>对大黑书《计算机网络：自顶向下方法》第一章进行笔记整理...... <span id="more"></span></p>
<h1 id="计算机网络和因特网">计算机网络和因特网</h1>
<h2 id="什么是internet">什么是Internet</h2>
<ul>
<li>从具体构成角度解释什么是互联网：</li>
</ul>
<ol type="1">
<li><strong>节点</strong>：包括<strong>主机节点</strong>和<strong>数据交换节点</strong>，主机节点（<strong>端节点</strong>，包括PC、Server、笔记本、智能手机等）指的是主机及其上运行的应用程序，数据交换节点指的是<strong>路由器、交换机</strong>等网络交换设备<br></li>
<li><strong>边</strong>：通信链路，通信链路中包括<strong>接入网链路</strong>和<strong>主干链路</strong>，接入网链路指的是主机连接到互联网的链路，主干链路指的是路由器之间的链路<br></li>
<li><strong>协议</strong>：<strong>对等层的实体在通讯连接的过程中遵守的规则的集合</strong>，定义了在两个或多个通信实体之间交换的报文<strong>格式</strong>和<strong>次序</strong>，以及在报文传输和/ 或接收或其他事件方面所采取的的<strong>动作</strong>。互联网以TCP、IP等协议为支撑，另外，以TCP、IP等协议为支撑的也可能为内部网，与互联网不连。<br></li>
<li><strong>端系统</strong>：端系统包括设备本身、支持设备能够通讯的网络操作系统、操作系统上驻留的网络应用程序，主机=端系统<br></li>
<li><strong>Internet是网络的网络</strong><br></li>
<li><strong>Internet标准</strong>:包括RFC和IETF<br></li>
</ol>
<ul>
<li><p>从服务角度解释什么是互联网：</p>
<ul>
<li><strong>基础设施</strong>（应用层以下）和<strong>分布式的应用</strong>：<strong>基础设施为分布式应用提供通讯服务</strong>，提供的通讯服务分为两种，分别是面向连接的可靠服务和无连接的不可靠服务</li>
</ul></li>
</ul>
<h2 id="网络边缘">网络边缘</h2>
<ol type="1">
<li><p>网络边缘指的是主机和应用程序，也可以解释为分布式应用和通讯基础设施的一部分。<br></p></li>
<li><p>应用进程和应用进程之间：</p>
<ul>
<li>采用<strong>客户/服务器模式</strong>：客户端向服务器请求、接收服务，在此种模式中，客户端是主动的、后运行起来，服务器是被动的、先运行起来。此种模式的一个缺点是，在客户端很多的情况下，需要服务器农场，可扩展性很差。</li>
<li>采用<strong>P2P模式</strong>（又称<strong>对等模式</strong>）：在此种模式中，很少有专门的服务器，每一个分布式的应用既是客户端又是服务器，请求资源和提供服务的节点都很多。</li>
</ul></li>
<li><p>基础设施提供的通讯服务：</p>
<ul>
<li>面向连接服务：目标是在端系统之间传输数据，在数据传输之前先握手做好准备。TCP提供的服务是可靠的（确认和重传），具有流量控制（发送方不会淹没接收方）、拥塞控制（能够感知路径是否拥塞来调节传输速度）的特点。使用TCP的应用有HTTP（Web）、FTP（文件传送）、Telnet（远程登录）、SMTP（email）。<em>（注：区分面向连接和有连接，面向连接只有端系统知道，有连接网络中的节点也知道）</em></li>
<li>无连接服务：目标是在端系统之间传输数据。UDP的特点是无连接、不可靠、无流量控制、无拥塞控制。使用UDP的应用包括流媒体、远程会议、DNS和Internet电话。</li>
</ul></li>
</ol>
<h2 id="网络核心">网络核心</h2>
<ol type="1">
<li><p>网络核心：路由器的网状网络<br></p></li>
<li><p>数据通过网络进行传输的方式包括<strong>电路交换</strong>和<strong>分组交换</strong>。<br></p></li>
<li><p><strong>电路交换</strong>：为每个呼叫预留一条专有电路</p>
<ul>
<li>电路交换独享资源，<strong>不共享</strong>，提供性能保障。</li>
<li>如果呼叫没有数据传送，被分配的资源就会被浪费。</li>
<li>通常被传统电话网络采用。</li>
<li>网络资源（如带宽）被分成片，有频分（FDM）、时分（TDM）、波分的方法。</li>
<li>电路交换不适合计算机之间的通信，具体表现在连接建立时间长；计算机之间的通信有突发性（不是一直都在通信），如果使用电路交换，则浪费的片较多。 <img src="https://img-blog.csdnimg.cn/20210201220529741.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3JhaW55X3VuaXZlcnNl,size_16,color_FFFFFF,t_70" alt="电路交换"> 图中，每段链路有4条线路，该呼叫采用了上面链路的第2个线路，右边链路的第1个线路。</li>
</ul></li>
<li><p><strong>分组交换</strong>：以分组为单位存储-转发方式</p>
<ul>
<li>节点之间的通讯链路不再细分为pieces</li>
<li>主机与主机之间的通讯数据分成一个个packet</li>
<li>以packet为单位在每个交换节点中存储、转发，并且每次只用某两个交换节点之间的一条通讯链路，此时其他主机对的通信可以使用其他链路，<strong>资源共享</strong>，按需使用</li>
<li>分组交换在每个节点的延迟比电路交换长得多（存储-转发时间+排队延迟）</li>
<li><strong>存储-转发</strong>：被传输到下一个链路之前，整个分组必须到达路由器，在一个速率为R bps的链路，一个长度为L bits的分组的存储转发延迟为L/R s</li>
<li><strong>排队延迟和丢失</strong>：如果到达速率&gt;链路的输出速率，分组将会排队等待传输，如果路由器的缓存用完了，分组将会抛弃</li>
<li><strong>统计多路复用</strong>：<img src="https://img-blog.csdnimg.cn/20210202123344874.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3JhaW55X3VuaXZlcnNl,size_16,color_FFFFFF,t_70" alt="统计多路复用"> 分组交换划分时间片没有固定模式，如图所示，可以先为A划分时间片，中间有一段空闲，再为B划分时间片</li>
<li>分组的存储转发一段一段从源端传到目标端，按照有无网络层的连接，分为<strong>数据报网络</strong>和<strong>虚电路网络</strong>，数据报的工作原理为在通信之前，无需建立起一个连接，有数据就传输，每一个分组都独立路由（路径不一样，可能会失序），路由器根据分组的目标地址进行路由</li>
</ul></li>
<li><p>网络交换的核心功能：</p>
<ul>
<li>路由：决定分组采用的源到目标的路径</li>
<li>转发：将分组从路由器的输入链路转入到输出链路</li>
</ul></li>
<li><p>分组交换和电路交换对比：</p>
<ul>
<li>同样的网络资源，分组交换允许更多用户使用网络</li>
<li>分组交换资源共享，简单，不必建立呼叫，适合于突发数据传输，但过度使用会造成网络拥塞</li>
</ul></li>
</ol>
<h2 id="接入网和物理媒体">接入网和物理媒体</h2>
<ol type="1">
<li><strong>住宅接入</strong>：modem，将上网数据调制加载音频信号上，在电话线上传输，在局端将其中的数据解调出来，反之亦然。此种方式在打电话的同时不能上网
<ul>
<li>接入网：DSL，带宽分为语音通信，上行传输和下行传输（上行比下行小）。DSL线路上的数据被传到互联网，DSL线路上的语音被传到电话网</li>
<li>接入网：线缆网络，有线电视信号线缆双向改造，带宽分为广播电视、上行传输和下行传输，各客户共享到线缆头端的接入网络（与DSL不同，DSL每个用户一个专用线路到CO）</li>
</ul></li>
<li>住宅接入：电缆模式
<ul>
<li>接入网：家庭网络<img src="https://img-blog.csdnimg.cn/20210202133021232.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3JhaW55X3VuaXZlcnNl,size_16,color_FFFFFF,t_70" alt="家庭网络"></li>
</ul></li>
<li><strong>企业接入网络</strong>：无线接入网络，分为无线LANs和广域无线接入。</li>
<li><strong>物理媒体</strong>：分为<strong>导引性媒体</strong>和<strong>非导引性媒体</strong>，其中导引性媒体包括<strong>同轴电缆</strong>、<strong>光纤</strong>和<strong>双绞线</strong>，非导引性媒体包括无线电。</li>
</ol>
<h2 id="internet结构和isp">Internet结构和ISP</h2>
<ol type="1">
<li>互联网网络结构：网络的网络
<ul>
<li>端系统通过接入ISPs连接到互联网</li>
<li>接入ISPs相应的必须是互联的，保证任何两个端系统可相互发送分组到对方<img src="https://img-blog.csdnimg.cn/20210202134721946.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3JhaW55X3VuaXZlcnNl,size_16,color_FFFFFF,t_70" alt="互联网网络结构"><img src="https://img-blog.csdnimg.cn/20210202134829333.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3JhaW55X3VuaXZlcnNl,size_16,color_FFFFFF,t_70" alt="互联网网络结构"></li>
<li>第一层ISP：国家/国际覆盖，速率极高（带宽较宽），直接与其他第一层ISP相连，与大量的第二层ISP和其他客户相连；第二层ISP：更小些（通常是区域性的）的ISP；第三层ISP与其他本地ISP。</li>
<li>POP：高层ISP面向客户网络的接入点，涉及费用结算；对等接入：2个ISP对等互接，不涉及费用结算；IXP：多个对等ISP互联互通之处，通常不涉及费用结算。</li>
</ul></li>
</ol>
<h2 id="分组延时丢失和吞吐量">分组延时、丢失和吞吐量</h2>
<ol type="1">
<li>分组延时和丢失的产生：在路由器缓冲区的分组队列分组到达链路的速率超过了链路输出的能力</li>
<li>四种分组延时：
<ul>
<li><strong>节点处理延时</strong>：检查bit级差错，检查分组首部和决定将分组导向何处</li>
<li><strong>排队延时</strong>：在输出链路上等待传输的时间，依赖于路由器的拥塞程度</li>
<li><strong>传输延时</strong>：将分组发送到链路上的时间 L/R（分组长度/链路带宽）</li>
<li><strong>传播延时</strong>：d/s（物理链路的长度/在媒体上的传播速度）</li>
<li><strong>节点延时为以上四种延时相加</strong></li>
</ul></li>
<li>Traceroute诊断程序：提供从源端，经过路由器，到目的的延时测量（和TTL有关，TTL每经过一个路由器减1，TTL减为0时报告原主机）</li>
<li>分组丢失：
<ul>
<li>产生原因：链路的队列缓冲区容量有限，当分组达到一个满的队列时，该分组将会丢失</li>
<li>分组丢失后有三种情况：由上一个节点重传；原主机重传；原主机不重传</li>
</ul></li>
<li><strong>吞吐量</strong>：在源端和目标端之间传输的速率，分为瞬间吞吐量和平均吞吐量（短板效应）</li>
</ol>
<h2 id="协议层次及服务模型">协议层次及服务模型</h2>
<ol type="1">
<li><strong>层次化方式实现复杂网络功能</strong>：本层协议实体相互交互执行本层的协议动作，目的是实现本层功能，通过接口为上层提供更好的服务，在实现本层协议的时候，直接利用了下层所提供的的服务</li>
<li><strong>协议的目的是为了向上层提供更好的服务，协议通过层间接口访问下层所提供的服务来实现</strong></li>
<li>服务和服务访问点：
<ul>
<li><strong>服务</strong>：低层实体向上层实体提供它们之间的通信的能力</li>
<li><strong>原语</strong>：上层使用下层服务的形式，高层使用低层的服务，以及低层向高层提供服务都是通过服务访问原语来进行交互的</li>
<li><strong>服务访问点（SAP）</strong>：上层使用下层提供的服务通过层间的接口—地点（可以用来区分不同的服务用户） 服务用户通过SAP告诉服务提供者访问什么服务；服务提供者通过原语告诉服务用户提供什么服务</li>
</ul></li>
<li>服务的类型：
<ul>
<li>面向连接的服务</li>
<li>无连接的服务：两个应用进程在进行实质性通讯前不需要握手</li>
</ul></li>
<li>服务与协议：
<ul>
<li>关系：本层协议实现时需借助下层提供服务，实现本层协议的目的是为了向上层提供更好的服务</li>
<li>区别：服务是低层实体向上层实体提供它们之间的通信的能力，是通过原语来操作的，垂直；协议是对等层实体之间在相互通信的过程中，需要遵循的规则的集合，水平</li>
</ul></li>
<li><strong>数据单元（DU）</strong>：<img src="https://img-blog.csdnimg.cn/20210202192330704.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3JhaW55X3VuaXZlcnNl,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></li>
<li>Internet协议栈：
<ul>
<li>应用层：为人类用户或者其他应用进程提供网络应用服务</li>
<li>传输层：提供进程到进程的数据传输，另外将不可靠的通信变成可靠的通信</li>
<li>网络层：在链路层点到点的基础上提供端到端的数据传输</li>
<li>链路层：在相邻两点间传输以帧为单位的数据</li>
<li>物理层：在线路上传输bit</li>
</ul></li>
<li>ISO/OSI参考模型：在应用层和传输层中间新增了表示层和会话层</li>
<li>封装和解封装</li>
<li>各层次的协议数据单元： - 应用层：报文 - 传输层：报文段：TCP段，UDP数据报 - 网络层：分组（如果无连接方式：数据报） - 链路层：帧 - 物理层：位</li>
</ol>
<h2 id="历史">历史</h2>
<ol type="1">
<li>1960年以前采用线路交换网络</li>
<li>1961-1972 早期的分组交换概念</li>
<li>1972-1980 专用网络和网络互联</li>
<li>1980-1990 体系结构变化，网络数量激增，应用丰富</li>
<li>1990, 2000’s 商业化, Web, 新的应用</li>
<li>2005-现在 移动互联时代</li>
</ol>
]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title>《计算机网络：自顶向下方法》第六章笔记整理</title>
    <url>/2021/03/12/%E3%80%8A%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%87%AA%E9%A1%B6%E5%90%91%E4%B8%8B%E6%96%B9%E6%B3%95%E3%80%8B%E7%AC%AC%E5%85%AD%E7%AB%A0%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>对大黑书《计算机网络：自顶向下方法》第六章进行笔记整理...... <span id="more"></span></p>
<h2 id="引证和服务">引证和服务</h2>
<h3 id="网络节点的连接方式">网络节点的连接方式</h3>
<ol type="1">
<li>点到点的网络（广域网采用）</li>
<li>多点连接（局域网）</li>
</ol>
<h3 id="节点主机路由器">节点：主机/路由器</h3>
<ol type="1">
<li>链路：沿着通信路径、连接相邻节点通信信道的是链路，分为有线链路、无线链路、局域网和共享性链路。
<ul>
<li>链路层负责从一个节点通过链路将帧中的数据报发送到相邻物理节点（以帧为单位）。</li>
<li>数据报在不同的链路上以不同的链路协议传送（例如第一跳是以太网，中间链路是帧中继链路，最后一跳是802.11）。</li>
<li>不同的链路协议提供不同的服务。 ### 链路层提供服务 （一般化的链路层服务，不是所有的链路层都提供这些服务，一个特定的链路层只提供其中一部分的服务）</li>
</ul></li>
<li>成帧、链路接入：
<ul>
<li>将数据报封装在帧中，加上帧头部、帧尾部。</li>
<li>如果采用的是共享性介质，信道接入获得信道访问权。</li>
<li>在帧头部使用MAC地址来表示源和目的。</li>
</ul></li>
<li>在相邻两个节点完成可靠数据传输：
<ul>
<li>在低出错率的链路上（光纤和双绞线电缆）很少使用。</li>
<li>在无线链路经常使用，出错率高（防止源端重发）。</li>
</ul></li>
<li>流量控制</li>
<li>差错检测、纠正</li>
<li>半双工和全双工
<ul>
<li>半双工：链路可以双向传输，但一次只有一个方向。</li>
</ul></li>
</ol>
<h3 id="链路层功能实现">链路层功能实现</h3>
<ol type="1">
<li>在适配器（网卡）上实现或者在一个芯片组上，网卡可以同时发同时接。</li>
</ol>
<h2 id="差错检测和纠正">差错检测和纠正</h2>
<h3 id="奇偶校验">奇偶校验</h3>
<ol type="1">
<li>单bit奇偶校验</li>
<li>二维bit奇偶校验</li>
</ol>
<h3 id="internet校验和">Internet校验和</h3>
<ol type="1">
<li>加起来取反码</li>
</ol>
<h3 id="crc循环冗余检测">CRC（循环冗余检测）</h3>
<ol type="1">
<li>模二运算</li>
<li>位串的两种表示（二进制和多项式）</li>
<li>生成多项式</li>
<li>发送方和接收方的发送接收校验过程</li>
</ol>
<h2 id="多点访问协议">多点访问协议</h2>
<h3 id="两种类型的链路一个子网内部链路连接形式">两种类型的链路（一个子网内部链路连接形式）</h3>
<ol type="1">
<li>点对点
<ul>
<li>拨号访问的PPP</li>
<li>以太网交换机和主机之间的点对点链路</li>
</ul></li>
<li>广播（共享链路或媒体）
<ul>
<li>传统以太网</li>
<li>HFC上行链路</li>
<li>802.11无线局域网</li>
</ul></li>
</ol>
<h3 id="单个共享的广播型链路">单个共享的广播型链路</h3>
<ol type="1">
<li>2个或更多的站点同时传送产生冲突；</li>
<li>分布式算法决定节点如何使用共享信道，即决定哪个节点什么时候可以发送；</li>
<li>关于共享控制的通信必须借助信道本身传输。</li>
</ol>
<h3 id="理想的多路访问协议">理想的多路访问协议</h3>
<ol type="1">
<li>当一个节点要发送时，以最大速率发送；当M个节点要发送时，每个节点以M分之一的最大速率发送。</li>
<li>完全分布，没有特殊节点协调发送，没有时钟和时隙的同步。</li>
<li>简单。</li>
</ol>
<h3 id="mac协议分类">MAC协议分类</h3>
<h4 id="信道划分协议tdmafdmacdma">信道划分协议：TDMA、FDMA、CDMA</h4>
<h4 id="随机访问协议">随机访问协议</h4>
<h5 id="时隙aloha">时隙ALOHA</h5>
<ol type="1">
<li>假设：所有帧是等长的；时间被划分为相等的时隙，每个时隙可发送一帧；节点只在时隙开始时发送帧；节点在时隙上是同步的；如果两个或多个节点在一个时隙传输，所有的站点都能检测到冲突。</li>
<li>运行：当节点获取新的帧，在下一个时隙传输；
<ul>
<li>Re1:传输时没有检测到冲突，成功，节点能够在下一时隙发送新帧；</li>
<li>Re2:传输时检测到冲突，失败，节点在每一个随后的时隙以概率P重传帧直至成功。</li>
</ul></li>
<li>优点：
<ul>
<li>节点可以以信道带宽全速连续传输；</li>
<li>高度分布；</li>
<li>简单</li>
</ul></li>
<li>缺点：
<ul>
<li>存在冲突，浪费时隙；</li>
<li>即使有帧要发送，仍然有可能存在空闲的时隙；</li>
<li>发现冲突也必须要传完；</li>
<li>需要时钟上同步；</li>
</ul></li>
</ol>
<h5 id="纯aloha非时隙">纯ALOHA（非时隙）</h5>
<ol type="1">
<li>简单，无需在时钟上同步。</li>
<li>当有帧需要传输，马上传输。</li>
<li>冲突的概率增加。</li>
</ol>
<h5 id="csma">CSMA</h5>
<ol type="1">
<li>CSMA/CD
<ul>
<li>发送前侦听信息，发送时侦听信息</li>
<li>以太网CSMA/CD算法
<ul>
<li>（1）适配器获取数据报，创建帧；</li>
<li>（2） 发送前，侦听信道CS，如果闲，则开始传送帧，如果忙，则一直等到闲再发送；</li>
<li>（3）发送过程中，冲突检测CD，如果没有冲突，则成功，如果检测到冲突，则放弃，之后尝试重发；</li>
<li>（4） 发送方适配器检测到冲突，除放弃外，还发送一个Jam信号，所有听到冲突的适配器也是如此；（强化冲突）</li>
<li>（5） 如果放弃，适配器进入指数退避状态；（二进制指数退避算法）</li>
</ul></li>
</ul></li>
<li>无线局域网中的MAC协议：CSMA/CA</li>
</ol>
<h4 id="轮流mac协议">轮流MAC协议</h4>
<ol type="1">
<li><p>信道划分MAC协议：共享信道在高负载时是有效的和公平的；在低负载时效率低下。</p></li>
<li><p>随机访问协议：在低负载时效率高，单个节点可以完全利用信道全部带宽；高负载时，冲突开销较大，效率极低，时间很多浪费在冲突中。</p></li>
<li><p>轮流协议在高负载和低负载时都很好，但太复杂。</p>
<p>轮询：</p>
<ul>
<li>主节点邀请从节点依次传送</li>
<li>缺点：
<ul>
<li>轮询开销：轮询本身消耗信道带宽；</li>
<li>等待时间：每个节点需要等到主节点轮询后开始传输，即使只有一个节点，也需要等到轮询一周之后才能发送；</li>
<li>单点故障：主节点失效时造成整个系统无法工作；</li>
</ul></li>
</ul>
<p>令牌传递：</p>
<ul>
<li>控制令牌循环从一个节点到下一个节点传递</li>
<li>令牌报文：特殊的帧</li>
<li>传输数据时传输一圈，直到重新回到发送节点再将令牌传给下一个节点</li>
<li>缺点：
<ul>
<li>令牌开销：本身消耗带宽</li>
<li>延迟：只有等到抓住令牌，才可传输</li>
<li>单点故障：令牌丢失系统级故障，整个系统无法传输（复杂机制重新生成令牌）</li>
</ul></li>
</ul></li>
</ol>
<h2 id="lans">LANS</h2>
<h3 id="mac地址和arp">MAC地址和ARP</h3>
<ol type="1">
<li>32比特IP地址
<ul>
<li>网络层地址</li>
<li>用于使数据报到达目的子网，前n-1跳</li>
<li>从而到达子网中的目标节点，最后一跳</li>
</ul></li>
<li>LAN地址
<ul>
<li>用于使帧从一个网卡传递到与其物理连接的另一个网卡（在同一个物理网络中）</li>
<li>48比特MAC地址固化在适配器的ROM，有时也可以通过软件设定</li>
<li>理论上全球任何两个网卡的MAC地址都不相同</li>
<li>MAC地址由IEEE管理和分配，制造商购入MAC地址空间 ### ARP： Address Resolution Protocol</li>
</ul></li>
<li>在LAN的每个IP节点都有一个ARP表，ARP表中包括一些LAN节点IP/MAC地址的映射</li>
<li>运行过程</li>
</ol>
<h2 id="以太网">以太网</h2>
<ol type="1">
<li>以太网是目前最主流的LAN技术
<ul>
<li>总线：所有节点在一个碰撞域内，一次只允许一个节点发送；可靠性差，如果介质破损，截面形成信号的反射，发送节点误以为是冲突，总是冲突</li>
<li>星形：
<ul>
<li>连接选择：hub或switch</li>
<li>现在一般是交换机在中心</li>
<li>每个节点以及相连的交换机端口使用（独立的）以太网协议，不会和其他节点的发送产生碰撞</li>
</ul></li>
</ul></li>
<li>以太帧结构</li>
</ol>
<ul>
<li>前导码：用来同步接收方和发送方的时钟速率，使得接收方将自己的时钟调到发送方的时钟，从而可以按照发送端的时钟来接收所发送的帧</li>
</ul>
<ol start="3" type="1">
<li>无连接、不可靠的服务</li>
<li>以太网使用CSMA/CD</li>
<li>10BaseT和100BaseT
<ul>
<li>T代表双绞线</li>
<li>节点连接到hub上</li>
<li>节点和hub间的最大距离是100m</li>
</ul></li>
<li>HUB</li>
</ol>
<ul>
<li>HUB本质上是物理层的中继器。
<ul>
<li>从一个端口收，转发到其他所有端口。</li>
<li>速率一致。</li>
<li>没有帧的缓存。</li>
<li>提供网络管理功能。</li>
</ul></li>
</ul>
<ol start="7" type="1">
<li>Manchester编码（物理层）
<ul>
<li>在10BaseT中使用。</li>
<li>每一个比特的位时中间有一个信号跳变。</li>
<li>允许在接收方和发送方节点之间进行时钟同步，节点间不需要集中的和全局的时钟。 10Mbps，使用20M带宽，效率50%，可以采用4b5b编码。</li>
</ul></li>
<li>千兆以太网
<ul>
<li>采用标准的以太帧格式。</li>
<li>允许点对点链路和共享广播信道。</li>
<li>物理编码：8b10b编码。</li>
<li>共享模式：继续使用CSMA/CD MAC技术，节点间需要较短距离以提高利用率。</li>
<li>交换模式：全双工千兆可用于点对点链路。</li>
</ul></li>
</ol>
<h2 id="集线器和交换机">集线器和交换机</h2>
<ol type="1">
<li>集线器：
<ul>
<li>网段：可以允许一个站点发送的网络范围。
<ul>
<li>在一个碰撞域，同时只允许一个站点在发送。</li>
<li>如果有两个节点在发送，则会碰撞。</li>
<li>通常拥有相同的前缀，让IP子网更详细的前缀。</li>
</ul></li>
<li>所有以hub连到一起的站点处在一个网段，处在一个碰撞域，骨干hub将所有的网段连到了一起。</li>
<li>通过hub可扩展节点间的最大距离。</li>
<li>通过hub，不能将10BaseT和100BaseT的网路连接到一起。</li>
</ul></li>
<li>交换机
<ul>
<li>链路层设备</li>
<li>透明</li>
<li>即插即用，自学习</li>
</ul></li>
</ol>
]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title>《计算机网络：自顶向下方法》第四章笔记整理</title>
    <url>/2021/03/30/%E3%80%8A%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%87%AA%E9%A1%B6%E5%90%91%E4%B8%8B%E7%BD%91%E7%BB%9C%E3%80%8B%E7%AC%AC%E5%9B%9B%E7%AB%A0%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>对大黑书《计算机网络：自顶向下方法》第四章进行笔记整理...... <span id="more"></span></p>
<h2 id="导论">导论</h2>
<h3 id="网络层服务">网络层服务</h3>
<ol type="1">
<li>在发送主机和接收主机之间传送段（segment）。<br></li>
<li>在发送端将段封装到数据报中，在接收端将段上交给传输层实体。<br></li>
<li>网络层协议存在于每一个主机和路由器。<br></li>
<li>路由器检查每一个经过它的IP数据报的头部。</li>
</ol>
<h3 id="网络层的关键功能">网络层的关键功能</h3>
<ol type="1">
<li>转发：将分组从路由器的输入接口转发到合适的输出接口。<br></li>
<li>路由：使用路由算法来决定分组从发送主机到目标接收主机的路径。</li>
</ol>
<h3 id="数据平面和控制平面">数据平面和控制平面</h3>
<h4 id="数据平面">数据平面</h4>
<ol type="1">
<li>本地，每个路由器功能<br></li>
<li>决定从路由器输入端口到达的分组如何转发到输出端口<br></li>
<li>转发功能：传统方式是基于目标地址和转发表；SDN方式是基于多个字段和转发表</li>
</ol>
<h4 id="控制平面">控制平面</h4>
<ol type="1">
<li>网络范围内的逻辑<br></li>
<li>决定数据报如何在路由器之间路由，决定数据报从源到目标主机之间的端到端路径<br></li>
<li>两个控制平面方法：传统的路由算法在路由器中被实现；SDN在远程的服务器中实现</li>
</ol>
<h3 id="传统方式">传统方式</h3>
<ol type="1">
<li><p>在每一个路由器中的单独路由器算法元件，在控制平面进行交互<br></p></li>
<li><p>路由和转发的相互作用：</p>
<ul>
<li>控制平面：路由算法决定端到端路径<br></li>
<li>数据平面：IP协议根据转发表决定了IP数据报在此路由器上的局部转发</li>
</ul></li>
</ol>
<h3 id="sdn方式逻辑集中的控制平面">SDN方式：逻辑集中的控制平面</h3>
<ol type="1">
<li>一个不同的（通常是远程的）控制器与本地控制代理（CAs）交互</li>
</ol>
<h3 id="网络服务模型">网络服务模型</h3>
<p>从发送方主机到接收方主机传输数据报的通道，<br> 对于单个数据报的服务：可靠传送；延迟保证<br> 对于数据报流的服务：保序数据报传送；保证流的最小带宽；分组之间的延迟差</p>
<h3 id="连接建立">连接建立</h3>
<p>在分组传输之前，在两个主机之间，在通过一些路由器所构成的路径上建立一个网络层连接（涉及到路由器）</p>
<h2 id="路由器组成">路由器组成</h2>
<h3 id="路由器结构概况">路由器结构概况</h3>
<p>高层面通用路由器体系架构<br> 1. 路由：运行路由选择算法/协议（RIP，OSPF，BGP）——生成路由表<br> 2. 转发：从输入到输出链路交换数据报——根据路由表进行分组的转发 <img src="https://img-blog.csdnimg.cn/20210330184838153.png" alt="在这里插入图片描述"></p>
<h3 id="输入端口功能">输入端口功能</h3>
<ol type="1">
<li><p>物理层：Bit级的接收<br></p></li>
<li><p>数据链路层：链路层协议动作、解封装<br></p></li>
<li><p>网络层：分布式交换<br></p>
<ul>
<li>根据数据报头部的信息如：目的地址，在输入端口内存中的转发表中查找合适的输出端口</li>
<li>基于目标的转发：仅仅依赖于IP数据报的目标IP地址（最长前缀匹配）</li>
<li>通用转发：基于头部字段的任意集合进行转发</li>
</ul></li>
</ol>
<h3 id="输入端口缓存">输入端口缓存</h3>
<ol type="1">
<li>当交换机构的速率小于输入端口的汇聚速率时，在输入端口可能要排队（排队延迟以及由于输入缓存溢出造成丢失）<br></li>
<li>HOL blocking：排在队头的数据报阻止了队列中其他数据报向前移动 <img src="https://img-blog.csdnimg.cn/20210330185758172.png" alt="在这里插入图片描述"></li>
</ol>
<h3 id="交换结构">交换结构</h3>
<ol type="1">
<li>将分组从输入缓存区传输到合适的输出端口<br></li>
<li>交换速率：分组可以按照该速率从输入传输到输出<br></li>
<li>三种典型的交换机构</li>
</ol>
<h4 id="通过内存交换">通过内存交换</h4>
<ol type="1">
<li>在CPU直接控制下的交换，采用传统的计算机<br></li>
<li>分组被拷贝到系统内存，CPU从分组头部提取出目标地址，查找转发表，找到对应的输出端口，拷贝到输出端口<br></li>
<li>转发速率被内存的带宽限制（数据报通过BUS两遍）<br></li>
<li>一次只能转发一个分组 <img src="https://img-blog.csdnimg.cn/20210330190501677.png" alt="在这里插入图片描述"></li>
</ol>
<h4 id="通过总线交换">通过总线交换</h4>
<ol type="1">
<li>数据报通过共享总线，从输入端口转发到输出端口<br></li>
<li>总线竞争：交换速度受限于总线带宽<br></li>
<li>一次处理一个分组 <img src="https://img-blog.csdnimg.cn/20210330191022409.png" alt="在这里插入图片描述"></li>
</ol>
<h4 id="通过互联网络crossbar等的交换">通过互联网络（crossbar等）的交换</h4>
<ol type="1">
<li>同时并发转发多个分组，克服总线带宽限制<br></li>
<li>榕树网络，纵横网络和其他的互联网络被开发，将多个处理器连接成多处理器<br></li>
<li>高级设计：将数据报分片为固定长度的信元，通过交换网络交换 <img src="https://img-blog.csdnimg.cn/20210330191343346.png" alt="在这里插入图片描述"></li>
</ol>
<h3 id="输出端口">输出端口</h3>
<ol type="1">
<li>当数据报从交换机构的到达速度比传输速率快，就需要输出端口缓存（数据报可能会被丢弃，由于拥塞，缓冲区没有空间）<br></li>
<li>由调度规则选择排队的数据报进行传输（不一定先到先传）</li>
</ol>
<h4 id="调度机制">调度机制</h4>
<ol type="1">
<li>调度：选择下一个要通过链路传输的分组<br></li>
<li>FIFO scheduling：按照分组到来的次序发送<br></li>
<li>丢弃策略：丢弃刚到达的分组、根据优先权丢失/移除分组，随机地丢弃/移除<br></li>
<li>调度策略</li>
</ol>
<h5 id="调度策略优先权">调度策略：优先权</h5>
<p>发送最高优先权的分组</p>
<h2 id="ipinternet协议">IP：Internet协议</h2>
<h3 id="互联网的网络层">互联网的网络层</h3>
<figure>
<img src="https://img-blog.csdnimg.cn/20210330192430417.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3JhaW55X3VuaXZlcnNl,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><figcaption aria-hidden="true">在这里插入图片描述</figcaption>
</figure>
<h3 id="ip数据报格式">IP数据报格式</h3>
<figure>
<img src="https://img-blog.csdnimg.cn/20210330192548897.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3JhaW55X3VuaXZlcnNl,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><figcaption aria-hidden="true">在这里插入图片描述</figcaption>
</figure>
<h3 id="ip分片和重组">IP分片和重组</h3>
<ol type="1">
<li><p>网络链路有MTU（最大传输单元）——链路层帧所携带的最大数据长度（不同的链路类型有不同的MTU）<br></p></li>
<li><p>大的IP数据报在网络上被分片</p>
<ul>
<li>一个数据报被分割成若干个小的数据报（有相同的ID，不同的偏移量，最后一个分片标记为0）</li>
<li>重组分片只在最终的目标主机进行</li>
<li>IP头部的信息被用于标识，排序相关分片</li>
</ul></li>
<li><p>例子 <img src="https://img-blog.csdnimg.cn/20210330195240725.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3JhaW55X3VuaXZlcnNl,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p></li>
</ol>
<h3 id="ip编址">IP编址</h3>
<h4 id="引论">引论</h4>
<ol type="1">
<li><p>IP地址：32位标示，对主机或者路由器的接口编址<br></p></li>
<li><p>接口：主机/路由器和物理链路的连接处</p>
<ul>
<li>路由器通常拥有多个接口</li>
<li>主机也有可能有多个接口</li>
<li>IP地址和每一个接口关联</li>
</ul></li>
</ol>
<p><strong>3. 一个IP地址和一个接口相关联</strong></p>
<h4 id="子网subnets">子网（subnets）</h4>
<ol type="1">
<li>IP地址：</li>
</ol>
<ul>
<li>子网部分（高位bits）</li>
<li>主机部分（低位bits）</li>
</ul>
<ol start="2" type="1">
<li>一个子网内的节点（主机或者路由器）它们的IP地址的高位部分相同，这些节点构成的网络的一部分叫做子网<br></li>
<li>无需路由器介入，子网内各主机可以在物理上相互直接到达<br></li>
<li>判断子网的方法（将每一个接口从主机或者路由器上分开）</li>
</ol>
<h4 id="ip地址分类">IP地址分类</h4>
<figure>
<img src="https://img-blog.csdnimg.cn/20210330202049353.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3JhaW55X3VuaXZlcnNl,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><figcaption aria-hidden="true">在这里插入图片描述</figcaption>
</figure>
<ol type="1">
<li><p>特殊IP地址：</p>
<ul>
<li>子网部分全为0，本网络</li>
<li>主机部分全为0，本主机</li>
<li>主机部分全为1，广播地址，这个网络的所有主机</li>
</ul></li>
<li><p>内网（专用）IP地址</p>
<ul>
<li>专用地址：地址空间的一部分供专用地址使用</li>
<li>永远不会被当做公用地址来分配，不会与公用地址重复（只在局部网络中意义，区分不同的设备）</li>
<li>路由器不对目标地址是专用地址的分组进行转发</li>
</ul></li>
</ol>
<h4 id="cidr-无类域间路由">CIDR 无类域间路由</h4>
<ol type="1">
<li>子网部分可以在任意的位置<br></li>
<li>地址格式：a.b.c.d/x，其中x是地址中子网号的长度，例如，200.23.16.0/23</li>
</ol>
<h4 id="子网掩码">子网掩码</h4>
<ol type="1">
<li>32bits，1代表子网部分，0代表主机部分</li>
</ol>
<h6 id="转发表和转发算法">转发表和转发算法</h6>
<p>获取IP数据报的目标地址，对于转发表中的每一个表项，如果如 (IP Des addr) &amp; (mask)==destination, 则按照表项对应的接口转发该数据报；如果都没有找到，则使用默认表项转发数据报</p>
<h4 id="获取ip地址的过程">获取IP地址的过程</h4>
<ol type="1">
<li><p>主机获得IP地址</p>
<ul>
<li><p>系统管理员将地址配置在一个文件中</p></li>
<li><p><strong>DHCP</strong>：从服务器中动态获得一个IP地址（即插即用）</p>
<ul>
<li><p>允许主机在加入网络的时候，动态地从服务器那里获得IP地址</p></li>
<li><p>DHCP工作概况：</p>
<ul>
<li>主机广播“DHCP discover”报文</li>
<li>DHCP服务器用“DHCP offer”提供报文响应</li>
<li>主机请求IP地址：发送“DHCP request”报文</li>
<li>DHCP服务器发送地址：“DHCP ack”报文</li>
</ul></li>
</ul></li>
</ul></li>
<li><p>获取一个网络的子网部分</p>
<ul>
<li><p>从ISP获得地址块中分配一个小地址块</p></li>
<li><p><strong>层次编址</strong> <img src="https://img-blog.csdnimg.cn/20210330212245799.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3JhaW55X3VuaXZlcnNl,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p></li>
</ul></li>
</ol>
<h4 id="nat网络地址转换">NAT：网络地址转换</h4>
<ol type="1">
<li><p>所有离开本地网络的数据报具有一个相同的源地址NAT IP address，但是具有不同的端口号<br></p></li>
<li><p>本地网络只有一个有效IP地址</p>
<ul>
<li>不需要从ISP分配一块地址，可用一个IP地址用于所有的（局域网）设备</li>
<li>可以在局域网改变设备的地址情况下而无需通知外界</li>
<li>可以改变ISP（地址变化）而不需改变内部的设备地址</li>
<li>局域网内部的设备没有明确的地址，对外是不可见的</li>
</ul></li>
<li><p>实现</p>
<ul>
<li>外出数据包：替换源地址和端口号为NAT IP地址和新的端口号，目标IP和端口不变</li>
<li>记住每个转换替换对（在NAT转换表中）</li>
<li>进入数据包：替换目标IP地址和端口号，采用存储在NAT表中的mapping表项</li>
</ul></li>
</ol>
<h4 id="ipv6">IPv6</h4>
<ol type="1">
<li>固定的40字节头部，数据报传输过程中，不允许分片<br></li>
<li>IPv4到IPv6的平移</li>
</ol>
]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title>多模态情感分析综述</title>
    <url>/2022/02/11/%E5%A4%9A%E6%A8%A1%E6%80%81%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>多模态情感分析是一个越来越受欢迎的研究领域，它将传统的基于文本的情感分析任务扩展到多模态的情景下，多模态包括文本、视频和语音模态，以往的情感分析任务通常聚焦于单个模态，但在某些情况下，仅仅通过文本模态来分析说话人的情感是不够的，如以下语句 <span id="more"></span></p>
<blockquote>
<p>The movie is sick.</p>
</blockquote>
<p>这句话本身所要表达的意思是模棱两可的，但如果说话者在说这句话的同时也在微笑，那么它就会被认为是积极的（positive），反过来说，如果说话者在说这句话的同时皱着眉头，则这句话被认为是消极的（negative），以上例子说明了双模态信息之间的互动，其中微笑和皱着眉头等信息能够在视频模态中体现出来。然后我们可以继续加入语音模态从而考虑三模态的情况，当说话声音较大时，这句话的积极情感会进一步增强，但加入使用fair替换掉sick这个单词，虽然同样是大声说话的情景，但考虑到fair这个词的强烈影响，这句话的情感并没有因为大声说话而产生巨大变化。以上例子说明了模态间信息交互的复杂性，但不可置疑的是，视频和语音模态中可能包含文本模态中的互补信息，因此，在多模态场景下进行情感分析任务更具有可信性。</p>
<center>
<img src="/2022/02/11/%E5%A4%9A%E6%A8%A1%E6%80%81%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/001.png" width="400px">
<p style="font-family:KaiTi">
图1：模态之间信息的互动
</p>
</center>
<p>现阶段的多模态情感分析任务，大多站在如何有效地将多模态的特征信息进行融合这一角度考虑问题，目的是排除与情感分析任务无关的噪声数据，最大化利用与情感分析任务相关的多模态数据，包括单模态内的数据交互与模态间的数据交互，最终达到分析情感极性的目标。本文主要就多模态数据集和多模态特征的融合方法进行介绍。</p>
<h3 id="数据集">数据集</h3>
<h3 id="多模态特征融合方法">多模态特征融合方法</h3>
<h4 id="早期融合">早期融合</h4>
<p>早期融合，也被称为特征级别的融合，直接将来自不同模式的特征表示连接成一个单一的表示，这是一种最直观的方法。下图展示了早期融合的具体操作。</p>
<center>
<img src="/2022/02/11/%E5%A4%9A%E6%A8%A1%E6%80%81%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/002.png" width="200px">
<p style="font-family:KaiTi">
图2：早期融合具体操作
</p>
</center>
<p>但是，由于来自不同模态的特征表示可能存在巨大差异，我们必须考虑时间同步问题，即不同模态的特征表示必须在时间层面上进行对齐，以便在融合前将这些特征表示转化为相同的格式，所以在某一个模态或某几个模态有信息缺失的情况下，早期融合的方法不能对模态间的互动进行有效建模。 另外，因为早期融合直接将不同模态的特征表示进行连接，可能会造成融合特征表示较为复杂的情况，最终会导致过拟合的发生。</p>
<h4 id="后期融合">后期融合</h4>
<p>后期融合，也被称为决策级别的融合，通常是整合来自每一个单一模态的预测结果，即首先利用每一个模态的数据信息进行预测，再将每一个模态的预测结果进行整合，流行的整合方法包括平均法、投票法和信号差异法。</p>
<p>后期融合的优点包括：</p>
<ol type="1">
<li>灵活性，可以为每个模态的数据选择最佳的分类器。</li>
<li>鲁棒性，当某些模态的数据有缺失时，后期融合的方法依然可以工作。</li>
</ol>
<p>但是后期融合的方法也有相当大的缺点，即不能够有效学习模态间的信息交互，不同模态数据之间的相关性和差异性被忽略。</p>
<h4 id="基于张量的融合">基于张量的融合</h4>
<p>基于张量的融合方法建立了一个融合层，具体融合方法为在三维向量场中将每种模态的向量表示进行外积操作得到融合向量表示，操作方法如图所示，</p>
<center>
<img src="/2022/02/11/%E5%A4%9A%E6%A8%A1%E6%80%81%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/003.png" width="350px">
<p style="font-family:KaiTi">
图3：基于张量的融合方法
</p>
</center>
<p>需要注意的是，不同模态的特征在进行融合时还需要保留原模态的特征，这样可以更好地利用单模态内的信息，基于张量的融合方法也考虑到这一点，在每一个模态的向量末尾连接1，这样在进行外积的同时还能够保留原有模态的特征，下图以视频模态和文本模态的特征融合为例说明了这一点，</p>
<center>
<img src="/2022/02/11/%E5%A4%9A%E6%A8%A1%E6%80%81%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/004.png" width="300px">
<p style="font-family:KaiTi">
图4：视频模态和文本模态的特征融合
</p>
</center>
<p>可以使用如下公式来表示文本模态、语音模态和视频模态的融合过程，其中<span class="math inline">\(\mathbf{z}^{m}\)</span>表示融合特征向量，<span class="math inline">\(\mathbf{z}^{l}\)</span>表示文本模态特征向量，<span class="math inline">\(\mathbf{z}^{v}\)</span>表示视频模态特征向量，<span class="math inline">\(\mathbf{z}^{a}\)</span>表示语音模态特征向量，可以结合图4来理解以下公式，其中<span class="math inline">\(\mathbf{z}^{l} \otimes \mathbf{z}^{v}\)</span>、<span class="math inline">\(\mathbf{z}^{a} \otimes \mathbf{z}^{v}\)</span>和<span class="math inline">\(\mathbf{z}^{l} \otimes \mathbf{z}^{a}\)</span>表示双模态之间的信息互动，<span class="math inline">\(\mathbf{z}^{l} \otimes \mathbf{z}^{v} \otimes \mathbf{z}^{a}\)</span>表示三模态之间的信息互动。</p>
<p><span class="math display">\[
\mathbf{z}^{m}=\left[\begin{array}{c}
\mathbf{z}^{l} \\
1
\end{array}\right] \otimes\left[\begin{array}{c}
\mathbf{z}^{v} \\
1
\end{array}\right] \otimes\left[\begin{array}{c}
\mathbf{z}^{a} \\
1
\end{array}\right]
\]</span></p>
<p>基于张量的融合方法显式地建模了单模态、双模态、三模态之间的信息互动，并且虽然融合特征向量是高维向量，但由于单模态特征之间是通过外积操作来进行融合的，没有可学习的参数，所以不用考虑过拟合的问题。</p>
<h4 id="低秩多模态融合方法">低秩多模态融合方法</h4>
<p>以上我们已经介绍了基于张量的融合方法，但这种方法往往会因为将单模态向量转化为融合张量而导致维度和计算复杂性的指数级增长（外积操作所致），为了解决基于张量的多模态融合方法计算效率差的问题，提出了一种低秩多模态融合的方法，主要是利用了将张量和权重并行分解的思想。</p>
<p>首先我们需要明确输出<span class="math inline">\(h\)</span>的计算方法，如以下公式所示，</p>
<p><span class="math display">\[
h=g(\mathcal{Z} ; \mathcal{W}, b)=\mathcal{W} \cdot \mathcal{Z}+b, h, b \in \mathbb{R}^{d_{y}}
\]</span></p>
<p>其中<span class="math inline">\(\mathcal{Z}\)</span>为融合张量，低秩多模态融合方法首先将权重<span class="math inline">\(\mathcal{W}\)</span>进行分解为如下形式,</p>
<p><span class="math display">\[
\mathcal{W}=\sum_{i=1}^{r} \bigotimes_{m=1}^{M} \mathbf{w}_{m}^{(i)}
\]</span></p>
<p>可以首先尝试理解一下作者这样分解的目的，即为了利用类似于分配律的原理让外积操作变为对应元素的乘积，最终达到每种模态数据乘以对应权重，然后再进行element wise（对应元素乘积）的操作的目标，以下我们可以尝试证明一下，</p>
<p><span class="math display">\[
\begin{aligned}
h &amp;=\left(\sum_{i=1}^{r} \bigotimes_{m=1}^{M} \mathbf{w}_{m}^{(i)}\right) \cdot \mathcal{Z} \\
&amp;=\sum_{i=1}^{r}\left(\bigotimes_{m=1}^{M} \mathbf{w}_{m}^{(i)} \cdot \mathcal{Z}\right) \\
&amp;=\sum_{i=1}^{r}\left(\bigotimes_{m=1}^{M} \mathbf{w}_{m}^{(i)} \cdot \bigotimes_{m=1}^{M} z_{m}\right) \\
&amp;=\Lambda_{m=1}^{M}\left[\sum_{i=1}^{r} \mathbf{w}_{m}^{(i)} \cdot z_{m}\right]
\end{aligned}
\]</span></p>
<p>其中公式中的<span class="math inline">\(\Lambda\)</span>符号表示对应元素乘积的操作，特别地，有以下记法：<span class="math inline">\(\Lambda_{t=1}^{3} x_{t}=x_{1} \circ x_{2} \circ x_{3}\)</span>，最终可以使用下图来表示低秩多模态融合方法，</p>
<center>
<img src="/2022/02/11/%E5%A4%9A%E6%A8%A1%E6%80%81%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/005.png" width="550px">
<p style="font-family:KaiTi">
图5：低秩多模态融合方法
</p>
</center>
<h4 id="基于神经网络的融合方法">基于神经网络的融合方法</h4>
<p>基于神经网络的融合采用了一种直接和直观的策略，通过神经网络融合不同模态的特征表示。基于注意力的融合使用注意力机制来获得一组具有标量权重的特征表示的加权和，这些标量权重是由一个注意力模块动态学习的。还有一些融合方法参考了当前比较流行的Transformer和BERT结构的思想，这些使用深度模型的融合方法能够以端到端的方式从大量的数据中学习，具有良好的性能，但也存在可解释性低的问题。</p>
<h5 id="基于注意力机制的融合方法">基于注意力机制的融合方法</h5>
<p>注意力机制被用于多模态融合是很容易想到的操作，注意力机制天然地能够发掘源模态和目标模态的联系，从而生成注意力权重，通过乘积进一步加强目标模态的特征表示，注意力机制很适用于发掘模态之间的共同特征，但对于发掘模态的特有特征就显得有些力不从心。这里我们以记忆融合网络（MFN）为例来探索注意力机制在多模态融合方面的应用。</p>
<p>注意力机制通常不会单独应用在多模态特征融合中，可能还会结合其他结构，如MFN由三个主要部分组成，分别是LSTM系统、Delta-Memory注意力网络、多模态门控记忆，其中LSTM系统有多个长短期记忆网络（LSTM）组成，每个模态对应一个LSTM网络，LSTM对模态内的信息的交互进行建模，或者说对每个时间步的模态信息进行记忆；Delta-Memory注意力网络是一种特殊的注意力机制，旨在发现LSTM系统中不同模态记忆间的横向交互关系；多模态门控记忆结构是一种统一的记忆存储，存储随着时间步推移的跨模态信息交互，总结下来，LSTM结构致力于模态内信息的建模，Delta-Memory注意力网络和多模态门控记忆结构致力于模态间信息的建模，模型总体结构如下所示，</p>
<center>
<img src="/2022/02/11/%E5%A4%9A%E6%A8%A1%E6%80%81%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/008.png" width="500px">
<p style="font-family:KaiTi">
图6：记忆融合网络（MFN）的整体架构
</p>
</center>
<p>Delta-Memory注意力网络截取<span class="math inline">\(t-1\)</span>和<span class="math inline">\(t\)</span>两个时间步的LSTM系统输出信息，目的是为了防止重复的跨模态交互信息被赋高权重的情况，如果有了两个时间步的模态信息之后，就可以将两个时间步的信息进行比较从而将高系数分配给那些将要变化的模态信息，文中指出，理想情况下，在LSTM系统的记忆状态发生变化之前，每个跨模态交互只分配一次高系数。Delta-Memory注意力网络中的计算公式如下所示，首先计算出注意力权重系数，再将系数乘以原模态特征表示。</p>
<p><span class="math display">\[
\begin{gather*}
a^{[t-1, t]}=\mathcal{D}_{a}\left(c^{[t-1, t]}\right) \\
\hat{c}^{[t-1, t]}=c^{[t-1, t]} \odot a^{[t-1, t]}
\end{gather*}
\]</span></p>
<p>多模态门控记忆结构存储了一段时间内跨模态信息交互的历史，包括两个门，分别是保持门和更新门，其中保持门分配上一个时间步的记忆，更新门根据更新建议（或称为当前时间步状态）来进行更新，计算公式如下所示，</p>
<p><span class="math display">\[
\begin{gather*}
\hat{u}^{t}=\mathcal{D}_{u}\left(\hat{c}^{[t-1, t]}\right) \\
\gamma_{1}^{t}=\mathcal{D}_{\gamma_{1}}\left(\hat{c}^{[t-1, t]}\right), \gamma_{2}^{t}=\mathcal{D}_{\gamma_{2}}\left(\hat{c}^{[t-1, t]}\right) \\
u^{t}=\gamma_{1}^{t} \odot u^{t-1}+\gamma_{2}^{t} \odot \tanh \left(\hat{u}^{t}\right)
\end{gather*}
\]</span></p>
<h5 id="多模态transformer融合方法">多模态Transformer融合方法</h5>
<p>这里介绍的多模态Transformer融合方法出自<em>Multimodal Transformer for Unaligned Multimodal Language Sequences</em>一文，文章提出了MulT结构，主要应用于非对齐多模态语言序列场景。相关研究表明，在对多模态的语言时间序列数据进行建模时主要存在两个挑战：</p>
<ol type="1">
<li>由于每种模态序列的采样率不同，导致数据没有在时间层面上对齐。</li>
<li>不同模态的元素之间存在长距离依赖的现象。</li>
</ol>
<p>文章中提出的MulT结构以端到端的方法解决了上述问题。MulT结构以跨模态的Transformer作为主要结构来建模模态与模态（双模态）之间的关系，具体来说，每个跨模态Transformer通过学习两个模态之间的注意力，用另一个源模态的低级特征反复强化目标模态，以视频模态和文本模态为例，第一种情况下，以视频模态为源模态，以文本模态为目标模态，在每一个跨模态的注意力机制块中，将源模态特征向量作为注意力机制的<span class="math inline">\(Q\)</span>，将目标模态特征向量作为注意力机制的<span class="math inline">\(K\)</span>、<span class="math inline">\(V\)</span>，经过注意力机制计算得到注意力权重，将权重乘以目标模态特征向量得到强化后的目标模态特征向量，计算公式如下所示，</p>
<p><span class="math display">\[
\text { Attention }(\mathrm{Q}, \mathrm{K}, \mathrm{V})=\operatorname{softmax}\left(\frac{\mathrm{QK}^{\mathrm{T}}}{\sqrt{\mathrm{d}_{\mathrm{k}}}}\right) \mathrm{V}
\]</span></p>
<p>实际情况下采用的多为多头注意力机制；第二种情况下，以文本模态为源模态，以视频模态为目标模态，其他计算过程相同。跨模态的注意力机制块的堆叠组成了跨模态的Transformer结构，另外，这里只着重强调了跨模态的注意力机制块中的跨模态的注意力机制计算细节，跨模态的注意力机制块除了交叉注意力结构，还包括前馈神经网络等结构。跨模态的注意力机制块结构如下所示，</p>
<center>
<img src="/2022/02/11/%E5%A4%9A%E6%A8%A1%E6%80%81%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/006.png" width="300px">
<p style="font-family:KaiTi">
图7：跨模态注意力机制块结构
</p>
</center>
<p>MulT的整体架构如下图所示，在这里我们只关注模态信息融合的部分，即架构的中间部分，可以从图中看到，在完成特征向量的处理之后，将两个模态的特征向量分别作为源模态和目标模态输入到跨模态的Transformer结构中，两个Transformer的输出进行连接，再输入到Transformer结构中，不同的是，跨模态的Transformer中进行的主要是交叉注意力计算，而上部的Transformer中进行的主要是自注意计算。架构的底部和顶部在这里不作主要关注，只需要明确架构的底部为特征向量的处理，架构的顶部为下游任务的适应。</p>
<center>
<img src="/2022/02/11/%E5%A4%9A%E6%A8%A1%E6%80%81%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/007.png" width="350px">
<p style="font-family:KaiTi">
图8：MulT的整体架构
</p>
</center>
<h5 id="基于神经网络的融合方法在可解释性方面的改进">基于神经网络的融合方法在可解释性方面的改进</h5>
<p>一般的基于神经网络的融合方法在可解释性方面都表现较差，所以出现了一些提高可解释性的尝试，现有大多数的尝试是在挖掘共同和独特信息方面的改进。一些框架可以从多模态特征表示中提取公共信息，同时获取一些独特信息，这里的独特信息通常是指特定于模态的信息，最后再通过融合层将共同信息和独特信息进行整合。</p>
<p>将DeepCU结构作为示例进行说明，DeepCU包括两个子网络，包括可以获取模态特性的独特子网络，以及由深度卷积张量网络组成的共同子网络。DeepCU可以通过两个子网络获取到互补的信息，进而通过融合层进行信息整合。DeepCU框架示意图如下所示，</p>
<center>
<img src="/2022/02/11/%E5%A4%9A%E6%A8%A1%E6%80%81%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/009.png" width="500px">
<p style="font-family:KaiTi">
图9：DeepCU的整体架构
</p>
</center>
<p>框架示意图左边是TFN和LMF架构的示意图，DeepCU相较于TFN和LMF更具表现力，因为它在共同子网络中捕获了非线性的多模态交互关系，在独特子网络中不会了分解的非线性特征关系，缓解了缺失值场景，增强了模型的泛化能力。</p>
<p>首先介绍独特子网络获得特定于模态信息的方法，从三个模态提取潜在特征之后，为了能对单模态中信息与信息的交互进行建模，这里采取的是分解机的方法，采用分解机的另一个好处是可以缓解缺失值场景，分解机公式如下所示，</p>
<p><span class="math display">\[
\hat{y}_{F M(\boldsymbol{x})}=w_{0}+\sum_{i=1}^{n} \boldsymbol{w}_{i} \boldsymbol{x}_{i}+\sum_{i=1}^{n} \sum_{j=i+1}^{n} \boldsymbol{v}_{i}^{T} \boldsymbol{v}_{j} \cdot \boldsymbol{x}_{i} \boldsymbol{x}_{j}
\]</span></p>
<p>然后我们对共同子网络进行介绍。共同子网络首先将三个模态的潜在特征进行外积，得到联合表示张量，类似于TFN的做法，其中张量的每个元素代表融合模态元素之间的相互作用强度，然后我们在这些张量上运用卷积操作，因为卷积核是非线性的特征提取器，所以比前馈层的泛化效果更好，并且能够有效降低复杂度，最后再将卷积操作输出输入到全连接层中。融合层的操作较为简单，只采取了简单的连接和转置操作。</p>
<p>上文提到过尝试挖掘共同和独特信息方面的改进，介绍了在挖掘模态共同和独特信息方面的方法，但是挖掘独特信息并不一定指模态独特信息，Deep-HOSeq架构的独特子网络挖掘的是时间步上的独特信息，旨在发现时间与语义观点上的协作，以下是Deep-HOSeq整体架构示意图，</p>
<center>
<img src="/2022/02/11/%E5%A4%9A%E6%A8%A1%E6%80%81%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E7%BB%BC%E8%BF%B0/010.png" width="500px">
<p style="font-family:KaiTi">
图10：Deep-HOSeq的整体架构
</p>
</center>
<p>Deep-HOSeq处理独特子网络的方法是，将每个时间步的各个模态的向量进行外积操作，再进行卷积操作，得到每个时间步的独特信息，经过sum-pooling进行相加，得到独特向量。Deep-HOSeq处理共同子网络和DeepCU处理共同子网络的方法类似。</p>
]]></content>
      <categories>
        <category>深度学习论文阅读</category>
      </categories>
      <tags>
        <tag>多模态情感分析</tag>
      </tags>
  </entry>
</search>
